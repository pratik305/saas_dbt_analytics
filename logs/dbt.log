[0m19:51:36.421070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207AB21EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207AD5303A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207AD530190>]}


============================== 19:51:36.454209 | 21be7286-f5a3-42e1-b31d-0e3215a8bab4 ==============================
[0m19:51:36.454209 [info ] [MainThread]: Running with dbt=1.11.2
[0m19:51:36.454209 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt init saas_dbt_analytics', 'use_experimental_parser': 'False', 'log_path': 'logs'}
[0m19:51:36.503890 [info ] [MainThread]: Creating dbt configuration folder at C:\Users\HP\.dbt
[0m19:51:36.510371 [debug] [MainThread]: Starter project path: C:\Users\HP\anaconda3\envs\saas\lib\site-packages\dbt\include\starter_project
[0m19:51:36.924390 [info ] [MainThread]: 
Your new dbt project "saas_dbt_analytics" was created!

For more information on how to configure the profiles.yml file,
please consult the dbt documentation here:

  https://docs.getdbt.com/docs/configure-your-profile

One more thing:

Need help? Don't hesitate to reach out to us via GitHub issues or on Slack:

  https://community.getdbt.com/

Happy modeling!

[0m19:51:36.924390 [info ] [MainThread]: Setting up your profile.
[0m19:51:46.408359 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:51:46.411894 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:51:46.411894 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:59:23.208011 [info ] [MainThread]: Profile saas_dbt_analytics written to C:\Users\HP\.dbt\profiles.yml using target's profile_template.yml and your supplied values. Run 'dbt debug' to validate the connection.
[0m19:59:23.213381 [debug] [MainThread]: Command `dbt init` succeeded at 19:59:23.212528 after 466.98 seconds
[0m19:59:23.216651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207AB21EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207AD52D2A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207AD531240>]}
[0m19:59:23.217410 [debug] [MainThread]: Flushing usage events
[0m19:59:25.213883 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:59:53.620130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F2393EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F24C98400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F24C98190>]}


============================== 19:59:53.620130 | f73c97a0-05df-427f-bdef-3c42547f405b ==============================
[0m19:59:53.620130 [info ] [MainThread]: Running with dbt=1.11.2
[0m19:59:53.636212 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'logs'}
[0m19:59:53.674659 [info ] [MainThread]: dbt version: 1.11.2
[0m19:59:53.674659 [info ] [MainThread]: python version: 3.10.19
[0m19:59:53.687834 [info ] [MainThread]: python path: C:\Users\HP\anaconda3\envs\saas\python.exe
[0m19:59:53.687834 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m19:59:55.901349 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:59:55.901349 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:59:55.903356 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:59:57.463567 [info ] [MainThread]: Using profiles dir at C:\Users\HP\.dbt
[0m19:59:57.463567 [info ] [MainThread]: Using profiles.yml file at C:\Users\HP\.dbt\profiles.yml
[0m19:59:57.463567 [info ] [MainThread]: Using dbt_project.yml file at D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml
[0m19:59:57.463567 [info ] [MainThread]: adapter type: databricks
[0m19:59:57.463567 [info ] [MainThread]: adapter version: 1.11.4
[0m19:59:57.479338 [info ] [MainThread]: Configuration:
[0m19:59:57.481206 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m19:59:57.484183 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m19:59:57.485257 [info ] [MainThread]: Required dependencies:
[0m19:59:57.487708 [debug] [MainThread]: Executing "git --help"
[0m19:59:57.592396 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:59:57.603495 [debug] [MainThread]: STDERR: "b''"
[0m19:59:57.604508 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m19:59:57.606620 [info ] [MainThread]: Connection:
[0m19:59:57.608652 [info ] [MainThread]:   host: dbc-61bb1ae2-dcbc.cloud.databricks.com
[0m19:59:57.609660 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/f7938fbc9bec1e4a
[0m19:59:57.610665 [info ] [MainThread]:   catalog: workspace
[0m19:59:57.613090 [info ] [MainThread]:   schema: analytics
[0m19:59:57.614097 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m19:59:58.619384 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:59:58.636145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'f73c97a0-05df-427f-bdef-3c42547f405b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F246360E0>]}
[0m19:59:58.636145 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m19:59:58.636145 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m19:59:58.636145 [debug] [MainThread]: Using databricks connection "debug"
[0m19:59:58.636145 [debug] [MainThread]: On debug: select 1 as id
[0m19:59:58.636145 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:00:01.869935 [error] [MainThread]: databricks-sql-connector adapter: ThriftBackend.attempt_request: Exception: %s
[0m20:00:02.613448 [error] [MainThread]: Databricks adapter: Connection(session-id=Unknown) - Exception while trying to create connection: Error during request to server: : Invalid access token.. 
Error properties: attempt=1/30, bounded-retry-delay=None, elapsed-seconds=1.6760308742523193/900.0, error-message=: Invalid access token., http-code=403, method=OpenSession, no-retry-reason=non-retryable error, original-exception=, query-id=None, session-id=None
[0m20:00:02.615456 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
select 1 as id
: Database Error
  Error during request to server: : Invalid access token.. 
[0m20:00:02.617463 [debug] [MainThread]: On debug: No close available on handle
[0m20:00:02.618743 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m20:00:02.618743 [info ] [MainThread]: [31m2 checks failed:[0m
[0m20:00:02.622136 [info ] [MainThread]: Project loading failed for the following reason:
 project path <D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml> not found

[0m20:00:02.623140 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  Database Error
    Error during request to server: : Invalid access token.. 

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m20:00:02.626564 [debug] [MainThread]: Command `dbt debug` failed at 20:00:02.625551 after 9.25 seconds
[0m20:00:02.626564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F2393EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F24636590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F2466A4D0>]}
[0m20:00:02.626564 [debug] [MainThread]: Flushing usage events
[0m20:00:04.251424 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:03:13.602756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEB861EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEB99603A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEB99601C0>]}


============================== 20:03:13.610784 | 3268c31f-7d38-41ba-a094-4a9131f36441 ==============================
[0m20:03:13.610784 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:03:13.612530 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'logs'}
[0m20:03:13.660542 [info ] [MainThread]: dbt version: 1.11.2
[0m20:03:13.661760 [info ] [MainThread]: python version: 3.10.19
[0m20:03:13.663107 [info ] [MainThread]: python path: C:\Users\HP\anaconda3\envs\saas\python.exe
[0m20:03:13.664434 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m20:03:16.317659 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:03:16.319667 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:03:16.321675 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:03:17.972658 [info ] [MainThread]: Using profiles dir at C:\Users\HP\.dbt
[0m20:03:17.972658 [info ] [MainThread]: Using profiles.yml file at C:\Users\HP\.dbt\profiles.yml
[0m20:03:17.972658 [info ] [MainThread]: Using dbt_project.yml file at D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml
[0m20:03:17.972658 [info ] [MainThread]: adapter type: databricks
[0m20:03:17.972658 [info ] [MainThread]: adapter version: 1.11.4
[0m20:03:17.988698 [info ] [MainThread]: Configuration:
[0m20:03:17.990784 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:03:17.992203 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m20:03:17.995211 [info ] [MainThread]: Required dependencies:
[0m20:03:17.997401 [debug] [MainThread]: Executing "git --help"
[0m20:03:18.110752 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:03:18.110752 [debug] [MainThread]: STDERR: "b''"
[0m20:03:18.110752 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:03:18.116292 [info ] [MainThread]: Connection:
[0m20:03:18.119312 [info ] [MainThread]:   host: dbc-61bb1ae2-dcbc.cloud.databricks.com
[0m20:03:18.121701 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/f7938fbc9bec1e4a
[0m20:03:18.121701 [info ] [MainThread]:   catalog: workspace
[0m20:03:18.126064 [info ] [MainThread]:   schema: analytics
[0m20:03:18.129106 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:03:19.207762 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:03:19.207762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '3268c31f-7d38-41ba-a094-4a9131f36441', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEBC3D94E0>]}
[0m20:03:19.207762 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m20:03:19.207762 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:03:19.207762 [debug] [MainThread]: Using databricks connection "debug"
[0m20:03:19.216075 [debug] [MainThread]: On debug: select 1 as id
[0m20:03:19.218082 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:03:22.690048 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0d-a01a-1f34-9f54-b142d554d556) - Created
[0m20:03:36.658314 [debug] [MainThread]: SQL status: OK in 17.440 seconds
[0m20:03:36.660320 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f10c0d-a01a-1f34-9f54-b142d554d556, command-id=01f10c0d-a054-12e7-afdc-b7a74d9a850a) - Closing
[0m20:03:37.002603 [debug] [MainThread]: On debug: Close
[0m20:03:37.015847 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0d-a01a-1f34-9f54-b142d554d556) - Closing
[0m20:03:37.282567 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m20:03:37.284576 [info ] [MainThread]: [31m1 check failed:[0m
[0m20:03:37.287626 [info ] [MainThread]: Project loading failed for the following reason:
 project path <D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml> not found

[0m20:03:37.291396 [debug] [MainThread]: Command `dbt debug` failed at 20:03:37.291396 after 23.94 seconds
[0m20:03:37.293732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEB861EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEB930F130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEB930CC70>]}
[0m20:03:37.295107 [debug] [MainThread]: Flushing usage events
[0m20:03:38.979522 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:05:49.652567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B130DEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B144283A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B144281C0>]}


============================== 20:05:49.667134 | 7c5de5a1-2c51-4eb4-b151-68e7539d9d95 ==============================
[0m20:05:49.667134 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:05:49.669193 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt debug', 'use_experimental_parser': 'False', 'log_path': 'logs'}
[0m20:05:49.719033 [info ] [MainThread]: dbt version: 1.11.2
[0m20:05:49.720531 [info ] [MainThread]: python version: 3.10.19
[0m20:05:49.722595 [info ] [MainThread]: python path: C:\Users\HP\anaconda3\envs\saas\python.exe
[0m20:05:49.723615 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m20:05:51.877235 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:05:51.879242 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:05:51.879242 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:05:53.447098 [info ] [MainThread]: Using profiles dir at C:\Users\HP\.dbt
[0m20:05:53.447098 [info ] [MainThread]: Using profiles.yml file at C:\Users\HP\.dbt\profiles.yml
[0m20:05:53.447098 [info ] [MainThread]: Using dbt_project.yml file at D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml
[0m20:05:53.447098 [info ] [MainThread]: adapter type: databricks
[0m20:05:53.447098 [info ] [MainThread]: adapter version: 1.11.4
[0m20:05:53.456287 [info ] [MainThread]: Configuration:
[0m20:05:53.457786 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:05:53.459868 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m20:05:53.462523 [info ] [MainThread]: Required dependencies:
[0m20:05:53.464765 [debug] [MainThread]: Executing "git --help"
[0m20:05:53.574439 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:05:53.574439 [debug] [MainThread]: STDERR: "b''"
[0m20:05:53.574439 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:05:53.574439 [info ] [MainThread]: Connection:
[0m20:05:53.574439 [info ] [MainThread]:   host: dbc-61bb1ae2-dcbc.cloud.databricks.com
[0m20:05:53.574439 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/f7938fbc9bec1e4a
[0m20:05:53.574439 [info ] [MainThread]:   catalog: workspace
[0m20:05:53.587368 [info ] [MainThread]:   schema: analytics
[0m20:05:53.589554 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:05:54.656013 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:05:54.656013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7c5de5a1-2c51-4eb4-b151-68e7539d9d95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B16E98B50>]}
[0m20:05:54.656013 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m20:05:54.656013 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:05:54.656013 [debug] [MainThread]: Using databricks connection "debug"
[0m20:05:54.656013 [debug] [MainThread]: On debug: select 1 as id
[0m20:05:54.656013 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:05:57.827827 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0d-fca4-15d2-962b-9a6860b02b3a) - Created
[0m20:05:58.414847 [debug] [MainThread]: SQL status: OK in 3.760 seconds
[0m20:05:58.430455 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f10c0d-fca4-15d2-962b-9a6860b02b3a, command-id=01f10c0d-fccc-104d-a110-c4e06d02e42d) - Closing
[0m20:05:58.430455 [debug] [MainThread]: On debug: Close
[0m20:05:58.430455 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0d-fca4-15d2-962b-9a6860b02b3a) - Closing
[0m20:05:58.696696 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m20:05:58.696696 [info ] [MainThread]: [31m1 check failed:[0m
[0m20:05:58.696696 [info ] [MainThread]: Project loading failed for the following reason:
 project path <D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml> not found

[0m20:05:58.696696 [debug] [MainThread]: Command `dbt debug` failed at 20:05:58.696696 after 9.26 seconds
[0m20:05:58.696696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B130DEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B13DCEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B13DCF1F0>]}
[0m20:05:58.696696 [debug] [MainThread]: Flushing usage events
[0m20:06:00.294973 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:07:06.082256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002320AB4EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002320CE6C3D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002320CE6C1F0>]}


============================== 20:07:06.091335 | 99a163f3-ff17-460f-be63-6aa7b3419cb4 ==============================
[0m20:07:06.091335 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:07:06.091335 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'logs'}
[0m20:07:06.144967 [info ] [MainThread]: dbt version: 1.11.2
[0m20:07:06.146983 [info ] [MainThread]: python version: 3.10.19
[0m20:07:06.147993 [info ] [MainThread]: python path: C:\Users\HP\anaconda3\envs\saas\python.exe
[0m20:07:06.150009 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m20:07:08.240784 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:07:08.240784 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:07:08.240784 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:07:09.805008 [info ] [MainThread]: Using profiles dir at C:\Users\HP\.dbt
[0m20:07:09.805008 [info ] [MainThread]: Using profiles.yml file at C:\Users\HP\.dbt\profiles.yml
[0m20:07:09.805008 [info ] [MainThread]: Using dbt_project.yml file at D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml
[0m20:07:09.805008 [info ] [MainThread]: adapter type: databricks
[0m20:07:09.820804 [info ] [MainThread]: adapter version: 1.11.4
[0m20:07:09.824571 [info ] [MainThread]: Configuration:
[0m20:07:09.826605 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:07:09.827110 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m20:07:09.829670 [info ] [MainThread]: Required dependencies:
[0m20:07:09.831918 [debug] [MainThread]: Executing "git --help"
[0m20:07:09.943270 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:07:09.943270 [debug] [MainThread]: STDERR: "b''"
[0m20:07:09.943270 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:07:09.943270 [info ] [MainThread]: Connection:
[0m20:07:09.943270 [info ] [MainThread]:   host: dbc-61bb1ae2-dcbc.cloud.databricks.com
[0m20:07:09.943270 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/f7938fbc9bec1e4a
[0m20:07:09.943270 [info ] [MainThread]:   catalog: workspace
[0m20:07:09.956920 [info ] [MainThread]:   schema: analytics
[0m20:07:09.960571 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:07:11.032954 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:07:11.034297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '99a163f3-ff17-460f-be63-6aa7b3419cb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002320C844C40>]}
[0m20:07:11.037614 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m20:07:11.038616 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:07:11.041221 [debug] [MainThread]: Using databricks connection "debug"
[0m20:07:11.042242 [debug] [MainThread]: On debug: select 1 as id
[0m20:07:11.044542 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:07:14.288961 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0e-2a37-1768-9b1d-37edf864d1c8) - Created
[0m20:07:14.751037 [debug] [MainThread]: SQL status: OK in 3.710 seconds
[0m20:07:14.755054 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f10c0e-2a37-1768-9b1d-37edf864d1c8, command-id=01f10c0e-2a5f-1ea9-b266-3790ea7938e3) - Closing
[0m20:07:14.757063 [debug] [MainThread]: On debug: Close
[0m20:07:14.757063 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0e-2a37-1768-9b1d-37edf864d1c8) - Closing
[0m20:07:15.032499 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m20:07:15.032499 [info ] [MainThread]: [31m1 check failed:[0m
[0m20:07:15.032499 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  dbt_project.yml does not parse to a dictionary


[0m20:07:15.032499 [debug] [MainThread]: Command `dbt debug` failed at 20:07:15.032499 after 9.16 seconds
[0m20:07:15.032499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002320AB4EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002322002A380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002320C7ECD30>]}
[0m20:07:15.043462 [debug] [MainThread]: Flushing usage events
[0m20:07:17.587339 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:09:24.899896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A7937DEB60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A795AFC370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A795AFC190>]}


============================== 20:09:24.915662 | 51c41592-e577-43de-8bd9-22e5c8d7659a ==============================
[0m20:09:24.915662 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:09:24.919789 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:09:24.968324 [info ] [MainThread]: dbt version: 1.11.2
[0m20:09:24.969637 [info ] [MainThread]: python version: 3.10.19
[0m20:09:24.970643 [info ] [MainThread]: python path: C:\Users\HP\anaconda3\envs\saas\python.exe
[0m20:09:24.971796 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m20:09:27.152646 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:09:27.152646 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:09:27.168738 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:09:28.849697 [info ] [MainThread]: Using profiles dir at C:\Users\HP\.dbt
[0m20:09:28.849697 [info ] [MainThread]: Using profiles.yml file at C:\Users\HP\.dbt\profiles.yml
[0m20:09:28.849697 [info ] [MainThread]: Using dbt_project.yml file at D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml
[0m20:09:28.849697 [info ] [MainThread]: adapter type: databricks
[0m20:09:28.849697 [info ] [MainThread]: adapter version: 1.11.4
[0m20:09:29.089657 [info ] [MainThread]: Configuration:
[0m20:09:29.091714 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:09:29.092749 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:09:29.094825 [info ] [MainThread]: Required dependencies:
[0m20:09:29.096568 [debug] [MainThread]: Executing "git --help"
[0m20:09:29.196018 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:09:29.198425 [debug] [MainThread]: STDERR: "b''"
[0m20:09:29.199425 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:09:29.200596 [info ] [MainThread]: Connection:
[0m20:09:29.201659 [info ] [MainThread]:   host: dbc-61bb1ae2-dcbc.cloud.databricks.com
[0m20:09:29.202867 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/f7938fbc9bec1e4a
[0m20:09:29.204285 [info ] [MainThread]:   catalog: workspace
[0m20:09:29.206481 [info ] [MainThread]:   schema: analytics
[0m20:09:29.208878 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:09:30.258403 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:09:30.258403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '51c41592-e577-43de-8bd9-22e5c8d7659a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A795544760>]}
[0m20:09:30.258403 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m20:09:30.258403 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:09:30.258403 [debug] [MainThread]: Using databricks connection "debug"
[0m20:09:30.258403 [debug] [MainThread]: On debug: select 1 as id
[0m20:09:30.272990 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:09:33.421070 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0e-7d27-1155-9b4a-ed63d230c2c9) - Created
[0m20:09:33.804280 [debug] [MainThread]: SQL status: OK in 3.530 seconds
[0m20:09:33.804280 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f10c0e-7d27-1155-9b4a-ed63d230c2c9, command-id=01f10c0e-7d4c-1580-bbf2-2f89002c53f7) - Closing
[0m20:09:33.820359 [debug] [MainThread]: On debug: Close
[0m20:09:33.822371 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0e-7d27-1155-9b4a-ed63d230c2c9) - Closing
[0m20:09:34.068711 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m20:09:34.084777 [info ] [MainThread]: [32mAll checks passed![0m
[0m20:09:34.095722 [debug] [MainThread]: Command `dbt debug` succeeded at 20:09:34.095722 after 9.43 seconds
[0m20:09:34.097767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A7937DEB60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A794B4D6F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A795ABE410>]}
[0m20:09:34.098769 [debug] [MainThread]: Flushing usage events
[0m20:09:36.050463 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:10:52.869218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC18AEBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC2BF0370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC2BF0190>]}


============================== 20:10:52.876147 | 15cd9480-c59e-4877-b303-a0362fad9010 ==============================
[0m20:10:52.876147 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:10:52.880123 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'invocation_command': 'dbt run --select stg_accounts', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:10:55.032591 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:10:55.032591 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:10:55.032591 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:10:56.956353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC5514160>]}
[0m20:10:57.088913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC55149A0>]}
[0m20:10:57.088913 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:10:58.152952 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:10:58.152952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC2727A90>]}
[0m20:10:58.195102 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:10:58.199501 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m20:10:58.200508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC26B3430>]}
[0m20:11:02.185958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AD6B600D0>]}
[0m20:11:02.386434 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:11:02.386434 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:11:02.425343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AD6A85E40>]}
[0m20:11:02.425343 [info ] [MainThread]: Found 1 model, 731 macros
[0m20:11:02.433696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AD6B9CD00>]}
[0m20:11:02.437713 [info ] [MainThread]: 
[0m20:11:02.439841 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:11:02.443211 [info ] [MainThread]: 
[0m20:11:02.444805 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:11:02.444805 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:11:02.451482 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:11:02.451482 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:11:02.487243 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:11:02.487243 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:11:02.493530 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:11:05.702964 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0e-b423-1538-b5f5-18586c52cdb9) - Created
[0m20:11:06.715161 [debug] [ThreadPool]: SQL status: OK in 4.220 seconds
[0m20:11:06.715161 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c0e-b423-1538-b5f5-18586c52cdb9, command-id=01f10c0e-b44d-1faf-8b0f-686f968e3f62) - Closing
[0m20:11:06.728912 [debug] [ThreadPool]: On list_workspace: Close
[0m20:11:06.730922 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0e-b423-1538-b5f5-18586c52cdb9) - Closing
[0m20:11:07.011679 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:11:07.012682 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:11:07.026302 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:11:07.026302 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:11:07.029056 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:11:10.132814 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0e-b6cb-1a9e-9d5c-b87f6a2ec2dc) - Created
[0m20:11:11.202029 [debug] [ThreadPool]: SQL status: OK in 4.170 seconds
[0m20:11:11.217837 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c0e-b6cb-1a9e-9d5c-b87f6a2ec2dc, command-id=01f10c0e-b6f2-1c3a-b797-702248e40961) - Closing
[0m20:11:11.217837 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:11:11.217837 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0e-b6cb-1a9e-9d5c-b87f6a2ec2dc) - Closing
[0m20:11:11.512045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AD6A0BFD0>]}
[0m20:11:11.537462 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_accounts
[0m20:11:11.537462 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_accounts ............................. [RUN]
[0m20:11:11.537462 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_accounts) - Creating connection
[0m20:11:11.543855 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_accounts'
[0m20:11:11.545862 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_accounts
[0m20:11:11.577143 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_accounts"
[0m20:11:11.577143 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_accounts
[0m20:11:11.624761 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:11:11.642854 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:11:11.642854 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AD6B63430>]}
[0m20:11:11.677173 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_accounts`
[0m20:11:11.703795 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_accounts"
[0m20:11:11.710269 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_accounts"
[0m20:11:11.711269 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_accounts: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_accounts"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_accounts`
  
  as (
    select *
from workspace.default.bronze_accounts
  )

[0m20:11:11.713733 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:11:14.982261 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10c0e-b9a9-100a-a7cf-f4daef3a7503) - Created
[0m20:11:17.655128 [debug] [Thread-3 (]: SQL status: OK in 5.940 seconds
[0m20:11:17.655128 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10c0e-b9a9-100a-a7cf-f4daef3a7503, command-id=01f10c0e-b9d6-1c80-96ec-d17fcaa6249d) - Closing
[0m20:11:17.687017 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:11:17.687017 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_accounts: Close
[0m20:11:17.687017 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10c0e-b9a9-100a-a7cf-f4daef3a7503) - Closing
[0m20:11:18.003403 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AD6BD3700>]}
[0m20:11:18.003403 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_accounts ........................ [[32mOK[0m in 6.47s]
[0m20:11:18.003403 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_accounts
[0m20:11:18.003403 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:11:18.019398 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:11:18.019398 [info ] [MainThread]: 
[0m20:11:18.019398 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 15.57 seconds (15.57s).
[0m20:11:18.025822 [debug] [MainThread]: Command end result
[0m20:11:18.101982 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:11:18.110091 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:11:18.126668 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:11:18.127674 [info ] [MainThread]: 
[0m20:11:18.128678 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:11:18.130867 [info ] [MainThread]: 
[0m20:11:18.131874 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:11:18.135396 [debug] [MainThread]: Command `dbt run` succeeded at 20:11:18.135396 after 25.54 seconds
[0m20:11:18.137854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC18AEBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC5514130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC1C4EA40>]}
[0m20:11:18.139292 [debug] [MainThread]: Flushing usage events
[0m20:11:19.811181 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:17:01.591093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8DECEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8F2103A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8F2101C0>]}


============================== 20:17:01.591093 | 4e4abcc2-3db5-4957-8ef8-245b9836fd6a ==============================
[0m20:17:01.591093 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:17:01.591093 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select stg_accounts', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:17:03.817851 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:17:03.817851 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:17:03.817851 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:17:05.918463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD91831660>]}
[0m20:17:06.081421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8F1DCD60>]}
[0m20:17:06.081421 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:17:07.112268 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:17:07.112268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8ED06B90>]}
[0m20:17:07.144897 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:17:07.677583 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m20:17:07.677583 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\silver\schema.yml
[0m20:17:07.677583 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\stg_accounts.sql
[0m20:17:08.664017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA3340640>]}
[0m20:17:08.885952 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:17:08.885952 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:17:08.920110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA3343100>]}
[0m20:17:08.923846 [info ] [MainThread]: Found 1 model, 4 data tests, 731 macros
[0m20:17:08.923846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA2EAC790>]}
[0m20:17:08.923846 [info ] [MainThread]: 
[0m20:17:08.923846 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:17:08.933014 [info ] [MainThread]: 
[0m20:17:08.935131 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:17:08.936429 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:17:08.937783 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:17:08.937783 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:17:09.123214 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:17:09.123214 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:17:09.123214 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:17:12.411726 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0f-8ebc-143d-b8dc-08c1f5faeb18) - Created
[0m20:17:13.002931 [debug] [ThreadPool]: SQL status: OK in 3.880 seconds
[0m20:17:13.014735 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c0f-8ebc-143d-b8dc-08c1f5faeb18, command-id=01f10c0f-8ee3-1b00-ad01-518ee67cc1fc) - Closing
[0m20:17:13.016742 [debug] [ThreadPool]: On list_workspace: Close
[0m20:17:13.018750 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0f-8ebc-143d-b8dc-08c1f5faeb18) - Closing
[0m20:17:13.288510 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:17:13.290592 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:17:13.304379 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:17:13.304379 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:17:13.304379 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:17:16.380339 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0f-9118-19c5-8dd7-c075398fcd21) - Created
[0m20:17:17.400780 [debug] [ThreadPool]: SQL status: OK in 4.080 seconds
[0m20:17:17.400780 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c0f-9118-19c5-8dd7-c075398fcd21, command-id=01f10c0f-913f-1d48-a36d-ea0602dd55d9) - Closing
[0m20:17:17.400780 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:17:17.400780 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0f-9118-19c5-8dd7-c075398fcd21) - Closing
[0m20:17:17.680060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD887C61D0>]}
[0m20:17:17.685421 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_accounts
[0m20:17:17.685421 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_accounts ............................. [RUN]
[0m20:17:17.685421 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_accounts) - Creating connection
[0m20:17:17.685421 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_accounts'
[0m20:17:17.685421 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_accounts
[0m20:17:17.717454 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_accounts"
[0m20:17:17.718456 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_accounts
[0m20:17:17.763371 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:17:17.775509 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:17:17.775509 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA2F19C30>]}
[0m20:17:17.809278 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_accounts`
[0m20:17:17.836326 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_accounts"
[0m20:17:17.838350 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_accounts"
[0m20:17:17.839357 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_accounts: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_accounts"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_accounts`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_accounts

),

cleaned as (

    select
        account_id,
        account_name,
        industry,
        country,
        signup_date,
        referral_source,
        plan_tier,
        seats,
        is_trial,
        churn_flag,
        ingestion_ts

    from source

    where account_id is not null

)

select *
from cleaned
  )

[0m20:17:17.841654 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:17:21.004615 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10c0f-93da-1c42-85ee-997d2b0b4799) - Created
[0m20:17:22.093786 [debug] [Thread-3 (]: SQL status: OK in 4.250 seconds
[0m20:17:22.095794 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10c0f-93da-1c42-85ee-997d2b0b4799, command-id=01f10c0f-9403-1535-af2f-a8884efad486) - Closing
[0m20:17:22.118142 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:17:22.118142 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_accounts: Close
[0m20:17:22.118142 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10c0f-93da-1c42-85ee-997d2b0b4799) - Closing
[0m20:17:22.415662 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8B30AF20>]}
[0m20:17:22.415662 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_accounts ........................ [[32mOK[0m in 4.73s]
[0m20:17:22.415662 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_accounts
[0m20:17:22.415662 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:17:22.415662 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:17:22.415662 [info ] [MainThread]: 
[0m20:17:22.428287 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 13.48 seconds (13.48s).
[0m20:17:22.431388 [debug] [MainThread]: Command end result
[0m20:17:22.510892 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:17:22.517962 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:17:22.525293 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:17:22.525293 [info ] [MainThread]: 
[0m20:17:22.525293 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:17:22.525293 [info ] [MainThread]: 
[0m20:17:22.525293 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:17:22.541159 [debug] [MainThread]: Command `dbt run` succeeded at 20:17:22.540128 after 21.17 seconds
[0m20:17:22.542169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8DECEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA30C79A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA30C67A0>]}
[0m20:17:22.574947 [debug] [MainThread]: Flushing usage events
[0m20:17:24.983783 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:17:45.797008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224491AEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244B4C0370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244B4C0190>]}


============================== 20:17:45.806483 | 011ab80f-3c63-46d8-9689-8607f603d904 ==============================
[0m20:17:45.806483 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:17:45.807984 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt test --select stg_accounts', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:17:48.305834 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:17:48.307858 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:17:48.308859 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:17:50.624650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244DAE16F0>]}
[0m20:17:50.786763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244B4953F0>]}
[0m20:17:50.789044 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:17:51.881751 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:17:51.884757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244AFF3340>]}
[0m20:17:51.924925 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:17:52.564663 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:17:52.565674 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:17:52.566700 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:17:52.697222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245F1BE050>]}
[0m20:17:52.977513 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:17:52.985342 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:17:53.048660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245F034100>]}
[0m20:17:53.051131 [info ] [MainThread]: Found 1 model, 4 data tests, 731 macros
[0m20:17:53.053159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245F082830>]}
[0m20:17:53.058360 [info ] [MainThread]: 
[0m20:17:53.060896 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:17:53.062910 [info ] [MainThread]: 
[0m20:17:53.065941 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:17:53.066960 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:17:53.072827 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:17:53.074844 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:17:53.114475 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:17:53.116912 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:17:53.117933 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:17:56.619721 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0f-a916-17b1-b39e-220953ede289) - Created
[0m20:17:57.198480 [debug] [ThreadPool]: SQL status: OK in 4.080 seconds
[0m20:17:57.214867 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c0f-a916-17b1-b39e-220953ede289, command-id=01f10c0f-a93c-1e6e-a3df-2a441dd34f9a) - Closing
[0m20:17:57.214867 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:17:57.214867 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0f-a916-17b1-b39e-220953ede289) - Closing
[0m20:17:57.493300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245F035690>]}
[0m20:17:57.509511 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:17:57.509511 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_accounts_account_id ............................. [RUN]
[0m20:17:57.513940 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m20:17:57.513940 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108'
[0m20:17:57.515947 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:17:57.553374 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:17:57.561144 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:17:57.603796 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:17:57.603796 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:17:57.603796 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:17:57.612263 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:18:00.900625 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-aba4-113e-b3a4-2119701a66b3) - Created
[0m20:18:02.261558 [debug] [Thread-2 (]: SQL status: OK in 4.650 seconds
[0m20:18:02.267090 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c0f-aba4-113e-b3a4-2119701a66b3, command-id=01f10c0f-abc8-1d92-ac20-e4d7261366f9) - Closing
[0m20:18:02.275437 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: Close
[0m20:18:02.275437 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-aba4-113e-b3a4-2119701a66b3) - Closing
[0m20:18:02.551903 [info ] [Thread-2 (]: 1 of 4 PASS not_null_stg_accounts_account_id ................................... [[32mPASS[0m in 5.04s]
[0m20:18:02.553908 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:18:02.553908 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:18:02.558721 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_accounts_seats .................................. [RUN]
[0m20:18:02.561964 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39) - Creating connection
[0m20:18:02.563974 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39'
[0m20:18:02.565986 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:18:02.585985 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:18:02.591270 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:18:02.604228 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:18:02.608243 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:18:02.610252 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select seats
from `workspace`.`analytics`.`stg_accounts`
where seats is null



  
  
      
    ) dbt_internal_test
[0m20:18:02.610252 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:18:06.234728 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-aed1-103e-9d9c-5a96587b330c) - Created
[0m20:18:07.116602 [debug] [Thread-2 (]: SQL status: OK in 4.510 seconds
[0m20:18:07.118060 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c0f-aed1-103e-9d9c-5a96587b330c, command-id=01f10c0f-aef8-1ac2-9e41-86f0f2477ce1) - Closing
[0m20:18:07.118060 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: Close
[0m20:18:07.118060 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-aed1-103e-9d9c-5a96587b330c) - Closing
[0m20:18:07.406011 [info ] [Thread-2 (]: 2 of 4 PASS not_null_stg_accounts_seats ........................................ [[32mPASS[0m in 4.84s]
[0m20:18:07.408020 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:18:07.410029 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:18:07.412038 [info ] [Thread-2 (]: 3 of 4 START test not_null_stg_accounts_signup_date ............................ [RUN]
[0m20:18:07.414114 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a) - Creating connection
[0m20:18:07.416118 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a'
[0m20:18:07.418789 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:18:07.436484 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:18:07.438947 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:18:07.438947 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:18:07.438947 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:18:07.451234 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signup_date
from `workspace`.`analytics`.`stg_accounts`
where signup_date is null



  
  
      
    ) dbt_internal_test
[0m20:18:07.451786 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:18:10.896743 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-b195-15d0-8ae7-771496aafc1f) - Created
[0m20:18:11.586684 [debug] [Thread-2 (]: SQL status: OK in 4.130 seconds
[0m20:18:11.588695 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c0f-b195-15d0-8ae7-771496aafc1f, command-id=01f10c0f-b1be-13cf-a8a7-570a4bd07a87) - Closing
[0m20:18:11.592256 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: Close
[0m20:18:11.594265 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-b195-15d0-8ae7-771496aafc1f) - Closing
[0m20:18:11.860820 [info ] [Thread-2 (]: 3 of 4 PASS not_null_stg_accounts_signup_date .................................. [[32mPASS[0m in 4.45s]
[0m20:18:11.873782 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:18:11.873782 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:18:11.878053 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_accounts_account_id ............................... [RUN]
[0m20:18:11.880577 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m20:18:11.881600 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79'
[0m20:18:11.882831 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:18:11.885577 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:18:11.904773 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:18:11.908305 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:18:11.908305 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:18:11.917309 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:18:11.919543 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:18:15.083606 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-b414-1081-bd85-183224d84f16) - Created
[0m20:18:17.828628 [debug] [Thread-2 (]: SQL status: OK in 5.910 seconds
[0m20:18:17.828628 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c0f-b414-1081-bd85-183224d84f16, command-id=01f10c0f-b43f-1be7-ab18-34b5683db3a6) - Closing
[0m20:18:17.828628 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: Close
[0m20:18:17.844643 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-b414-1081-bd85-183224d84f16) - Closing
[0m20:18:18.146478 [info ] [Thread-2 (]: 4 of 4 PASS unique_stg_accounts_account_id ..................................... [[32mPASS[0m in 6.27s]
[0m20:18:18.146478 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:18:18.156391 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:18:18.158399 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:18:18.160406 [info ] [MainThread]: 
[0m20:18:18.163434 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 25.10 seconds (25.10s).
[0m20:18:18.168100 [debug] [MainThread]: Command end result
[0m20:18:18.260018 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:18:18.267419 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:18:18.286629 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:18:18.287960 [info ] [MainThread]: 
[0m20:18:18.289974 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:18:18.291987 [info ] [MainThread]: 
[0m20:18:18.294094 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m20:18:18.297182 [debug] [MainThread]: Command `dbt test` succeeded at 20:18:18.297182 after 32.74 seconds
[0m20:18:18.299840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224491AEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244B4953F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244A519390>]}
[0m20:18:18.301623 [debug] [MainThread]: Flushing usage events
[0m20:18:20.012827 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:32:58.137611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEDEC9EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEE0FBC3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEE0FBC1C0>]}


============================== 20:32:58.153536 | a7675eb7-9980-408b-a476-0c7fcf903ad2 ==============================
[0m20:32:58.153536 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:32:58.163299 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_subscriptions', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:33:02.492230 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:33:02.492230 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:33:02.492230 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:33:06.549413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEDFFBCDC0>]}
[0m20:33:06.825205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEE06C1C60>]}
[0m20:33:06.829233 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:33:08.908516 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:33:08.912544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEE0AE3E80>]}
[0m20:33:08.964871 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:33:10.033353 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m20:33:10.038393 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\silver\stg_subscriptions.sql
[0m20:33:10.040406 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\schema.yml
[0m20:33:11.381785 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_subscriptions'
in package 'saas_dbt_analytics' (models\silver\schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m20:33:11.381785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF4FCE800>]}
[0m20:33:11.726169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF510C130>]}
[0m20:33:12.147497 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:33:12.161945 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:33:12.225101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF511FAC0>]}
[0m20:33:12.225101 [info ] [MainThread]: Found 2 models, 8 data tests, 731 macros
[0m20:33:12.225101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF511F130>]}
[0m20:33:12.225101 [info ] [MainThread]: 
[0m20:33:12.243001 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:33:12.250774 [info ] [MainThread]: 
[0m20:33:12.254504 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:33:12.256524 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:33:12.267790 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:33:12.272748 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:33:12.323523 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:33:12.326128 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:33:12.326128 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:33:19.213563 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c11-ceea-1c8f-add2-5749a64bb795) - Created
[0m20:33:33.245862 [debug] [ThreadPool]: SQL status: OK in 20.920 seconds
[0m20:33:33.269104 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c11-ceea-1c8f-add2-5749a64bb795, command-id=01f10c11-cf27-1da2-a5a9-47c45eccdf59) - Closing
[0m20:33:33.651432 [debug] [ThreadPool]: On list_workspace: Close
[0m20:33:33.655436 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c11-ceea-1c8f-add2-5749a64bb795) - Closing
[0m20:33:34.001824 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:33:34.004914 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:33:34.031272 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:33:34.034400 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:33:34.037485 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:33:40.313568 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c11-db7b-132c-b18b-45103b25b060) - Created
[0m20:33:43.094544 [debug] [ThreadPool]: SQL status: OK in 9.060 seconds
[0m20:33:43.106146 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c11-db7b-132c-b18b-45103b25b060, command-id=01f10c11-dbec-13e1-8cab-04c60288f490) - Closing
[0m20:33:43.112278 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:33:43.115407 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c11-db7b-132c-b18b-45103b25b060) - Closing
[0m20:33:43.431790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF510D900>]}
[0m20:33:43.451230 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_subscriptions
[0m20:33:43.455480 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_subscriptions ........................ [RUN]
[0m20:33:43.461776 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_subscriptions) - Creating connection
[0m20:33:43.464902 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_subscriptions'
[0m20:33:43.468066 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_subscriptions
[0m20:33:43.546019 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_subscriptions"
[0m20:33:43.551281 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_subscriptions
[0m20:33:43.995279 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:33:44.006291 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:33:44.012675 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF4C9A680>]}
[0m20:33:44.113356 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_subscriptions`
[0m20:33:44.150770 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_subscriptions"
[0m20:33:44.158610 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_subscriptions"
[0m20:33:44.162874 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_subscriptions"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_subscriptions`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_subscriptions

),

cleaned as (

    select
        subscription_id,
        account_id,
        start_date,
        end_date,
        plan_tier,
        seats,
        mrr_amount,
        arr_amount,
        is_trial,
        upgrade_flag,
        downgrade_flag,
        churn_flag,
        billing_frequency,
        auto_renew_flag,
        ingestion_ts

    from source

    where subscription_id is not null
      and account_id is not null

)

select *
from cleaned
  )

[0m20:33:44.165893 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:33:50.140991 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10c11-e167-1176-898e-2b94ab8a5d53) - Created
[0m20:33:54.278153 [debug] [Thread-3 (]: SQL status: OK in 10.110 seconds
[0m20:33:54.282149 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10c11-e167-1176-898e-2b94ab8a5d53, command-id=01f10c11-e22c-16db-a620-6d8d6a6a9c08) - Closing
[0m20:33:54.327915 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:33:54.337326 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_subscriptions: Close
[0m20:33:54.340824 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10c11-e167-1176-898e-2b94ab8a5d53) - Closing
[0m20:33:55.278338 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF51EF850>]}
[0m20:33:55.282340 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_subscriptions ................... [[32mOK[0m in 11.81s]
[0m20:33:55.287375 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_subscriptions
[0m20:33:55.291635 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:33:55.294715 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:33:55.298112 [info ] [MainThread]: 
[0m20:33:55.302135 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 43.05 seconds (43.05s).
[0m20:33:55.308139 [debug] [MainThread]: Command end result
[0m20:33:55.471671 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:33:55.486646 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:33:55.514675 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:33:55.517685 [info ] [MainThread]: 
[0m20:33:55.521890 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:33:55.525751 [info ] [MainThread]: 
[0m20:33:55.530377 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:33:55.536249 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m20:33:55.544802 [debug] [MainThread]: Command `dbt run` succeeded at 20:33:55.543794 after 57.96 seconds
[0m20:33:55.546802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEDEC9EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEE000F910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEDCE99F60>]}
[0m20:33:55.550178 [debug] [MainThread]: Flushing usage events
[0m20:33:59.356220 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:34:25.416958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024222A8EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024224DBC370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024224DBC190>]}


============================== 20:34:25.432202 | ed075333-bf7c-4de4-874c-22fdc38b1145 ==============================
[0m20:34:25.432202 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:34:25.435360 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt test --select stg_subscriptions', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:34:30.663584 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:34:30.669187 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:34:30.673220 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:34:36.317869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024220E9FE50>]}
[0m20:34:36.707054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024224DBEA70>]}
[0m20:34:36.715116 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:34:39.890891 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:34:39.892901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242248DB7F0>]}
[0m20:34:39.998145 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:34:41.293399 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:34:41.295409 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:34:41.298431 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:34:41.504969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024238AF4130>]}
[0m20:34:41.929381 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:34:41.949719 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:34:42.050332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024238A5C730>]}
[0m20:34:42.053680 [info ] [MainThread]: Found 2 models, 8 data tests, 731 macros
[0m20:34:42.059583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024238A5CA30>]}
[0m20:34:42.069009 [info ] [MainThread]: 
[0m20:34:42.076064 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:34:42.081110 [info ] [MainThread]: 
[0m20:34:42.088122 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:34:42.088122 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:34:42.142330 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:34:42.145357 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:34:42.207839 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:34:42.209850 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:34:42.214945 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:34:47.724454 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c12-03c3-12e4-b92b-bb7eb9d05954) - Created
[0m20:34:48.468427 [debug] [ThreadPool]: SQL status: OK in 6.250 seconds
[0m20:34:48.490031 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c12-03c3-12e4-b92b-bb7eb9d05954, command-id=01f10c12-03ea-198c-bc7b-dd5808228461) - Closing
[0m20:34:48.495092 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:34:48.497108 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c12-03c3-12e4-b92b-bb7eb9d05954) - Closing
[0m20:34:48.785600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024238A53D90>]}
[0m20:34:48.804844 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:34:48.807392 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_subscriptions_account_id ........................ [RUN]
[0m20:34:48.807392 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9) - Creating connection
[0m20:34:48.814407 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9'
[0m20:34:48.818930 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:34:48.893431 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:34:48.904885 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:34:48.988617 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:34:48.995644 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:34:49.002992 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_subscriptions`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:34:49.005032 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:34:56.705455 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-08a8-1b75-9339-1b184874d489) - Created
[0m20:34:58.555759 [debug] [Thread-2 (]: SQL status: OK in 9.550 seconds
[0m20:34:58.575070 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c12-08a8-1b75-9339-1b184874d489, command-id=01f10c12-0945-13c8-997f-38a34beb7c35) - Closing
[0m20:34:58.587453 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: Close
[0m20:34:58.600709 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-08a8-1b75-9339-1b184874d489) - Closing
[0m20:34:59.566366 [info ] [Thread-2 (]: 1 of 4 PASS not_null_stg_subscriptions_account_id .............................. [[32mPASS[0m in 10.76s]
[0m20:34:59.576413 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:34:59.580626 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:34:59.587138 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_subscriptions_subscription_id ................... [RUN]
[0m20:34:59.594229 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91) - Creating connection
[0m20:34:59.596246 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m20:34:59.603334 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:34:59.641122 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:34:59.644322 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:34:59.665949 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:34:59.680532 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:34:59.680532 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:34:59.687912 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:06.630012 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-0f01-19b8-bb36-fcc068a53738) - Created
[0m20:35:07.878697 [debug] [Thread-2 (]: SQL status: OK in 8.200 seconds
[0m20:35:07.898435 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c12-0f01-19b8-bb36-fcc068a53738, command-id=01f10c12-0f30-14cc-bb77-904e8f10fcd1) - Closing
[0m20:35:07.902922 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m20:35:07.905960 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-0f01-19b8-bb36-fcc068a53738) - Closing
[0m20:35:08.188912 [info ] [Thread-2 (]: 2 of 4 PASS not_null_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 8.60s]
[0m20:35:08.195965 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:35:08.195965 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:35:08.203334 [info ] [Thread-2 (]: 3 of 4 START test relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:35:08.203334 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90) - Creating connection
[0m20:35:08.210631 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90'
[0m20:35:08.210631 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:35:08.262527 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:35:08.270012 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:35:08.285809 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:35:08.295189 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:35:08.298510 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_subscriptions`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:35:08.302701 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:14.511294 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-13b2-15f9-8079-e8ad53f5bb18) - Created
[0m20:35:18.939841 [debug] [Thread-2 (]: SQL status: OK in 10.640 seconds
[0m20:35:18.958122 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c12-13b2-15f9-8079-e8ad53f5bb18, command-id=01f10c12-13e1-1d54-bd53-680159649629) - Closing
[0m20:35:18.965589 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: Close
[0m20:35:18.969623 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-13b2-15f9-8079-e8ad53f5bb18) - Closing
[0m20:35:19.331449 [info ] [Thread-2 (]: 3 of 4 PASS relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 11.13s]
[0m20:35:19.340753 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:35:19.340753 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:35:19.340753 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_subscriptions_subscription_id ..................... [RUN]
[0m20:35:19.361169 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e) - Creating connection
[0m20:35:19.368223 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m20:35:19.380185 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:35:19.458788 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:35:19.465041 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:35:19.481401 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:35:19.484501 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:35:19.492705 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:35:19.492705 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:25.886259 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-1a7a-10f8-bf68-f3ee7224ac59) - Created
[0m20:35:27.855709 [debug] [Thread-2 (]: SQL status: OK in 8.360 seconds
[0m20:35:27.864871 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c12-1a7a-10f8-bf68-f3ee7224ac59, command-id=01f10c12-1aa9-1199-a5c7-e342e52c11cd) - Closing
[0m20:35:27.874034 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m20:35:27.878681 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-1a7a-10f8-bf68-f3ee7224ac59) - Closing
[0m20:35:28.167972 [info ] [Thread-2 (]: 4 of 4 PASS unique_stg_subscriptions_subscription_id ........................... [[32mPASS[0m in 8.81s]
[0m20:35:28.175761 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:35:28.192211 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:35:28.195566 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:35:28.205153 [info ] [MainThread]: 
[0m20:35:28.211108 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 46.12 seconds (46.12s).
[0m20:35:28.221114 [debug] [MainThread]: Command end result
[0m20:35:28.392279 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:35:28.410813 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:35:28.438658 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:35:28.447790 [info ] [MainThread]: 
[0m20:35:28.447790 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:35:28.447790 [info ] [MainThread]: 
[0m20:35:28.458982 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m20:35:28.466200 [debug] [MainThread]: Command `dbt test` succeeded at 20:35:28.466200 after 63.43 seconds
[0m20:35:28.466200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024222A8EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024222DD1960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242228BFA30>]}
[0m20:35:28.466200 [debug] [MainThread]: Flushing usage events
[0m20:35:32.222554 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:52:10.773661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEDBBEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEFECC370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEFECC1C0>]}


============================== 19:52:10.802205 | c38ba085-fd5f-4df6-a40a-da2e8fc6536e ==============================
[0m19:52:10.802205 [info ] [MainThread]: Running with dbt=1.11.2
[0m19:52:10.809561 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt deps', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m19:52:11.506968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c38ba085-fd5f-4df6-a40a-da2e8fc6536e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEFE964A0>]}
[0m19:52:11.636792 [debug] [MainThread]: Set downloads directory='C:\Users\HP\AppData\Local\Temp\dbt-downloads-y442wvki'
[0m19:52:11.640323 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m19:52:13.986777 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m19:52:13.996762 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m19:52:16.125851 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m19:52:16.193606 [info ] [MainThread]: Updating lock file in file path: D:\DataScience\saas-databricks-dbt-analytics/package-lock.yml
[0m19:52:16.227584 [debug] [MainThread]: Set downloads directory='C:\Users\HP\AppData\Local\Temp\dbt-downloads-hmgk8ca2'
[0m19:52:16.247474 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m19:52:18.977488 [info ] [MainThread]: Installed from version 1.1.1
[0m19:52:18.978988 [info ] [MainThread]: Updated version available: 1.3.3
[0m19:52:18.981021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'c38ba085-fd5f-4df6-a40a-da2e8fc6536e', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEEF07730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DF015F910>]}
[0m19:52:18.983091 [info ] [MainThread]: 
[0m19:52:18.985235 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m19:52:19.006219 [debug] [MainThread]: Command `dbt deps` succeeded at 19:52:18.992774 after 8.80 seconds
[0m19:52:19.009222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEDBBEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEEF07730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DF00420B0>]}
[0m19:52:19.012608 [debug] [MainThread]: Flushing usage events
[0m19:52:21.752218 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:56:24.878154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8351AEB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8374D0370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8374D01C0>]}


============================== 19:56:24.892771 | 8cad52e2-978f-4050-88df-c6c0eefc613c ==============================
[0m19:56:24.892771 [info ] [MainThread]: Running with dbt=1.11.2
[0m19:56:24.897097 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_feature_usage', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m19:56:29.146658 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:56:29.146658 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:56:29.162544 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:56:32.769784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B836E4BF70>]}
[0m19:56:33.003280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B839EC4AC0>]}
[0m19:56:33.007299 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m19:56:34.527047 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:56:34.527047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B836FDEA70>]}
[0m19:56:34.631265 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m19:56:35.056909 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m19:56:35.058654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B836BF7310>]}
[0m19:56:42.928299 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_subscriptions'
in package 'saas_dbt_analytics' (models\silver\schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m19:56:42.928299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B1D0FA0>]}
[0m19:56:43.430457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B420130>]}
[0m19:56:43.794840 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m19:56:43.829785 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m19:56:43.884614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B41F880>]}
[0m19:56:43.886621 [info ] [MainThread]: Found 3 models, 11 data tests, 845 macros
[0m19:56:43.892384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B423310>]}
[0m19:56:43.900418 [info ] [MainThread]: 
[0m19:56:43.902426 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:56:43.904969 [info ] [MainThread]: 
[0m19:56:43.908545 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:56:43.913028 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:56:43.921147 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m19:56:43.922890 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m19:56:43.966438 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m19:56:43.966438 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m19:56:43.966438 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:56:48.823352 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd5-e02b-1a67-a9a9-43ea8fdb2f24) - Created
[0m19:57:05.419925 [debug] [ThreadPool]: SQL status: OK in 21.450 seconds
[0m19:57:05.447240 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd5-e02b-1a67-a9a9-43ea8fdb2f24, command-id=01f10cd5-e06c-15b4-87dd-4a5b8960d9b8) - Closing
[0m19:57:05.919594 [debug] [ThreadPool]: On list_workspace: Close
[0m19:57:05.919594 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd5-e02b-1a67-a9a9-43ea8fdb2f24) - Closing
[0m19:57:07.597221 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m19:57:07.597221 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m19:57:07.623866 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m19:57:07.625875 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m19:57:07.627882 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:57:13.818219 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd5-ef20-126d-b358-352d29695b7c) - Created
[0m19:57:16.808190 [debug] [ThreadPool]: SQL status: OK in 9.180 seconds
[0m19:57:16.816454 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd5-ef20-126d-b358-352d29695b7c, command-id=01f10cd5-ef50-1757-bfd0-73fdf73ae23d) - Closing
[0m19:57:16.821043 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m19:57:16.827057 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd5-ef20-126d-b358-352d29695b7c) - Closing
[0m19:57:17.810085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B41F280>]}
[0m19:57:17.849939 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_feature_usage
[0m19:57:17.857609 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_feature_usage ........................ [RUN]
[0m19:57:17.857609 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_feature_usage) - Creating connection
[0m19:57:17.857609 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_feature_usage'
[0m19:57:17.857609 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_feature_usage
[0m19:57:17.927464 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_feature_usage"
[0m19:57:17.932060 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_feature_usage
[0m19:57:18.030257 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:57:18.038140 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:57:18.046232 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B657310>]}
[0m19:57:18.130280 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_feature_usage`
[0m19:57:18.178565 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_feature_usage"
[0m19:57:18.178565 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_feature_usage"
[0m19:57:18.184504 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_feature_usage: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_feature_usage"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_feature_usage`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_feature_usage
    where subscription_id is not null

),

aggregated as (

    select
        subscription_id,
        usage_date,
        feature_name,

        sum(usage_count)              as total_usage_count,
        sum(usage_duration_secs)      as total_usage_duration_secs,
        sum(error_count)              as total_error_count,

        max(is_beta_feature)          as is_beta_feature

    from source
    group by
        subscription_id,
        usage_date,
        feature_name

)

select *
from aggregated
  )

[0m19:57:18.184504 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:57:23.127614 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd5-f4ae-1366-9098-3bd08a52c948) - Created
[0m19:57:27.026920 [debug] [Thread-3 (]: SQL status: OK in 8.840 seconds
[0m19:57:27.026920 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10cd5-f4ae-1366-9098-3bd08a52c948, command-id=01f10cd5-f4dc-1fc3-bf67-7fb6d7ced3d3) - Closing
[0m19:57:27.069125 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:57:27.076765 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_feature_usage: Close
[0m19:57:27.078771 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd5-f4ae-1366-9098-3bd08a52c948) - Closing
[0m19:57:27.361541 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8335BAE90>]}
[0m19:57:27.363554 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_feature_usage ................... [[32mOK[0m in 9.49s]
[0m19:57:27.369899 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_feature_usage
[0m19:57:27.377558 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:57:27.377558 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:57:27.381932 [info ] [MainThread]: 
[0m19:57:27.388340 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 43.47 seconds (43.47s).
[0m19:57:27.394254 [debug] [MainThread]: Command end result
[0m19:57:27.585644 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m19:57:27.585644 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m19:57:27.617360 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m19:57:27.617360 [info ] [MainThread]: 
[0m19:57:27.617360 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:57:27.617360 [info ] [MainThread]: 
[0m19:57:27.617360 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m19:57:27.633661 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 3 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m19:57:27.637640 [debug] [MainThread]: Command `dbt run` succeeded at 19:57:27.637640 after 63.14 seconds
[0m19:57:27.642120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8351AEB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B1CA500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8364C2A40>]}
[0m19:57:27.645080 [debug] [MainThread]: Flushing usage events
[0m19:57:32.754198 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:00:54.974578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204FF9BEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020481E803A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020481E801F0>]}


============================== 20:00:54.992347 | 5cd8634b-16fc-46fc-96e1-3a8c6d143e48 ==============================
[0m20:00:54.992347 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:00:54.992347 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt test --select stg_feature_usage', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:00:58.221334 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:00:58.237335 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:00:58.237335 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:01:00.914153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020481E80460>]}
[0m20:01:01.141623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048470FE80>]}
[0m20:01:01.145697 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:01:02.712283 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:01:02.712283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048189BD60>]}
[0m20:01:02.784099 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:01:03.621996 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:01:03.621996 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:01:03.621996 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:01:03.805114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020495C20130>]}
[0m20:01:04.151249 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:01:04.160481 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:01:04.224594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020495A43DF0>]}
[0m20:01:04.226340 [info ] [MainThread]: Found 3 models, 11 data tests, 845 macros
[0m20:01:04.229016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020495A43880>]}
[0m20:01:04.231025 [info ] [MainThread]: 
[0m20:01:04.231025 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:01:04.237580 [info ] [MainThread]: 
[0m20:01:04.237580 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:01:04.243298 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:01:04.280937 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:01:04.282946 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:01:04.322114 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:01:04.322114 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:01:04.328559 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:09.747072 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd6-7bc2-13a0-b594-425e2bf90c54) - Created
[0m20:01:11.602687 [debug] [ThreadPool]: SQL status: OK in 7.260 seconds
[0m20:01:11.622085 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd6-7bc2-13a0-b594-425e2bf90c54, command-id=01f10cd6-7bf1-16dc-b432-0f44325941d3) - Closing
[0m20:01:11.622085 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:01:11.622085 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd6-7bc2-13a0-b594-425e2bf90c54) - Closing
[0m20:01:12.099899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020495A41BD0>]}
[0m20:01:12.115734 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:01:12.124139 [info ] [Thread-2 (]: 1 of 3 START test dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [RUN]
[0m20:01:12.131740 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77) - Creating connection
[0m20:01:12.131740 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77'
[0m20:01:12.131740 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:01:12.212189 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:01:12.218772 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:01:12.274331 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:01:12.290455 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:01:12.290455 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  





with validation_errors as (

    select
        subscription_id, usage_date, feature_name
    from `workspace`.`analytics`.`stg_feature_usage`
    group by subscription_id, usage_date, feature_name
    having count(*) > 1

)

select *
from validation_errors



  
  
      
    ) dbt_internal_test
[0m20:01:12.290455 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:01:17.523896 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd6-7fe6-12f1-89a8-96b12cbfb0ca) - Created
[0m20:01:20.730311 [debug] [Thread-2 (]: SQL status: OK in 8.440 seconds
[0m20:01:20.741079 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd6-7fe6-12f1-89a8-96b12cbfb0ca, command-id=01f10cd6-8092-15f7-b3da-730f2b9d431b) - Closing
[0m20:01:20.744941 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: Close
[0m20:01:20.756982 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd6-7fe6-12f1-89a8-96b12cbfb0ca) - Closing
[0m20:01:21.217363 [info ] [Thread-2 (]: 1 of 3 PASS dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [[32mPASS[0m in 9.09s]
[0m20:01:21.217363 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:01:21.217363 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:01:21.217363 [info ] [Thread-2 (]: 2 of 3 START test not_null_stg_feature_usage_subscription_id ................... [RUN]
[0m20:01:21.231337 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3) - Creating connection
[0m20:01:21.235424 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3'
[0m20:01:21.238808 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:01:21.292476 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:01:21.293201 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:01:21.303846 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:01:21.309493 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:01:21.312292 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_feature_usage`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:01:21.312292 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:01:25.924265 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd6-8565-156b-a4f3-ddb0f0fce13d) - Created
[0m20:01:27.056003 [debug] [Thread-2 (]: SQL status: OK in 5.740 seconds
[0m20:01:27.061909 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd6-8565-156b-a4f3-ddb0f0fce13d, command-id=01f10cd6-8594-19b3-a1f9-7a172c6b6a5d) - Closing
[0m20:01:27.063920 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: Close
[0m20:01:27.065930 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd6-8565-156b-a4f3-ddb0f0fce13d) - Closing
[0m20:01:27.467184 [info ] [Thread-2 (]: 2 of 3 PASS not_null_stg_feature_usage_subscription_id ......................... [[32mPASS[0m in 6.24s]
[0m20:01:27.473201 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:01:27.476732 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:01:27.480983 [info ] [Thread-2 (]: 3 of 3 START test relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [RUN]
[0m20:01:27.484129 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850) - Creating connection
[0m20:01:27.487194 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850'
[0m20:01:27.489202 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:01:27.525662 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:01:27.525662 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:01:27.547673 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:01:27.549679 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:01:27.552392 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select subscription_id as from_field
    from `workspace`.`analytics`.`stg_feature_usage`
    where subscription_id is not null
),

parent as (
    select subscription_id as to_field
    from `workspace`.`analytics`.`stg_subscriptions`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:01:27.552392 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:01:32.170052 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd6-891f-171a-8af3-356cb43c13c7) - Created
[0m20:01:37.706369 [debug] [Thread-2 (]: SQL status: OK in 10.150 seconds
[0m20:01:37.710395 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd6-891f-171a-8af3-356cb43c13c7, command-id=01f10cd6-894d-1e3f-98c1-f5ac7d4b2e19) - Closing
[0m20:01:37.714773 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: Close
[0m20:01:37.718876 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd6-891f-171a-8af3-356cb43c13c7) - Closing
[0m20:01:38.056090 [info ] [Thread-2 (]: 3 of 3 PASS relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [[32mPASS[0m in 10.57s]
[0m20:01:38.060197 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:01:38.064679 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:01:38.068761 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:01:38.068761 [info ] [MainThread]: 
[0m20:01:38.074926 [info ] [MainThread]: Finished running 3 data tests in 0 hours 0 minutes and 33.83 seconds (33.83s).
[0m20:01:38.080956 [debug] [MainThread]: Command end result
[0m20:01:38.259220 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:01:38.269060 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:01:38.294905 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:01:38.294905 [info ] [MainThread]: 
[0m20:01:38.299092 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:01:38.299092 [info ] [MainThread]: 
[0m20:01:38.303346 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m20:01:38.307431 [debug] [MainThread]: Command `dbt test` succeeded at 20:01:38.307431 after 43.62 seconds
[0m20:01:38.307431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204FF9BEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048470FE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020495C04D30>]}
[0m20:01:38.311469 [debug] [MainThread]: Flushing usage events
[0m20:01:40.770587 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:09:48.817527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BB96AEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBB9C03A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBB9C01F0>]}


============================== 20:09:48.817527 | af6587a2-21f4-4203-8626-69ccc8a8f436 ==============================
[0m20:09:48.817527 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:09:48.817527 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_churn_events', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:09:51.984785 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:09:51.984785 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:09:51.984785 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:09:54.771857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBB9C0460>]}
[0m20:09:55.000479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BCEC2B730>]}
[0m20:09:55.000479 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:09:56.608509 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:09:56.608509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBB4DC700>]}
[0m20:09:56.704233 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:09:57.614024 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m20:09:57.614024 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\silver\stg_churn_events.sql
[0m20:09:57.618050 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\schema.yml
[0m20:09:58.839153 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_churn_events'
in package 'saas_dbt_analytics' (models\silver\schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m20:09:58.839153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BCFABEFE0>]}
[0m20:09:59.237970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BCFB14130>]}
[0m20:09:59.799040 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:09:59.804292 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:09:59.854442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BCF99D510>]}
[0m20:09:59.854442 [info ] [MainThread]: Found 4 models, 15 data tests, 845 macros
[0m20:09:59.854442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BCF902F80>]}
[0m20:09:59.863814 [info ] [MainThread]: 
[0m20:09:59.865337 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:09:59.865337 [info ] [MainThread]: 
[0m20:09:59.865337 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:09:59.871978 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:09:59.878819 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:09:59.880207 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:09:59.922152 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:09:59.922152 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:09:59.922152 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:10:04.451653 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd7-ba82-114b-819a-0e4fc2ad35c6) - Created
[0m20:10:05.090597 [debug] [ThreadPool]: SQL status: OK in 5.170 seconds
[0m20:10:05.114843 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd7-ba82-114b-819a-0e4fc2ad35c6, command-id=01f10cd7-baa8-16c7-8e44-7fe951c123a8) - Closing
[0m20:10:05.121114 [debug] [ThreadPool]: On list_workspace: Close
[0m20:10:05.122122 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd7-ba82-114b-819a-0e4fc2ad35c6) - Closing
[0m20:10:05.433920 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:10:05.437731 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:10:05.461559 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:10:05.463568 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:10:05.463568 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:10:09.793562 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd7-bda5-1015-a859-0b5b49fc2bb0) - Created
[0m20:10:10.549727 [debug] [ThreadPool]: SQL status: OK in 5.080 seconds
[0m20:10:10.561794 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd7-bda5-1015-a859-0b5b49fc2bb0, command-id=01f10cd7-bdd6-1cb7-8bc2-f325a4979d3d) - Closing
[0m20:10:10.565556 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:10:10.567567 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd7-bda5-1015-a859-0b5b49fc2bb0) - Closing
[0m20:10:10.852397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BCF841330>]}
[0m20:10:10.858658 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_churn_events
[0m20:10:10.858658 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_churn_events ......................... [RUN]
[0m20:10:10.876583 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_churn_events) - Creating connection
[0m20:10:10.876583 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_churn_events'
[0m20:10:10.876583 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_churn_events
[0m20:10:10.919410 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_churn_events"
[0m20:10:10.930464 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_churn_events
[0m20:10:10.986661 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:10:10.986661 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:10:10.994391 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBE3BB070>]}
[0m20:10:11.036618 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_churn_events`
[0m20:10:11.071280 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_churn_events"
[0m20:10:11.073288 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_churn_events"
[0m20:10:11.075297 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_churn_events: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_churn_events"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_churn_events`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_churn_events

),

cleaned as (

    select
        churn_event_id,
        account_id,
        churn_date,
        reason_code,
        refund_amount_usd,
        preceding_upgrade_flag,
        preceding_downgrade_flag,
        is_reactivation,
        feedback_text,
        ingestion_ts

    from source

    where churn_event_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:10:11.076812 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:10:15.443785 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd7-c106-18e2-8775-fa3f33f3375b) - Created
[0m20:10:16.884337 [debug] [Thread-3 (]: SQL status: OK in 5.810 seconds
[0m20:10:16.890371 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10cd7-c106-18e2-8775-fa3f33f3375b, command-id=01f10cd7-c135-160b-8da8-c0d68afedbf5) - Closing
[0m20:10:16.954255 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:10:16.964025 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_churn_events: Close
[0m20:10:16.968037 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd7-c106-18e2-8775-fa3f33f3375b) - Closing
[0m20:10:17.297706 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BB7ABAF20>]}
[0m20:10:17.297706 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_churn_events .................... [[32mOK[0m in 6.41s]
[0m20:10:17.307750 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_churn_events
[0m20:10:17.312587 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:10:17.318669 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:10:17.320771 [info ] [MainThread]: 
[0m20:10:17.324075 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 17.46 seconds (17.46s).
[0m20:10:17.334831 [debug] [MainThread]: Command end result
[0m20:10:17.484939 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:10:17.501106 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:10:17.516210 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:10:17.524315 [info ] [MainThread]: 
[0m20:10:17.524315 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:10:17.524315 [info ] [MainThread]: 
[0m20:10:17.531418 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:10:17.535290 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m20:10:17.541344 [debug] [MainThread]: Command `dbt run` succeeded at 20:10:17.540743 after 28.98 seconds
[0m20:10:17.544525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BB96AEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBDFAFD00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBDFAFD60>]}
[0m20:10:17.544525 [debug] [MainThread]: Flushing usage events
[0m20:10:20.356697 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:10:52.351933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB83BCABF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB85EEC3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB85EEC1F0>]}


============================== 20:10:52.364071 | 735c705a-0114-48f7-b9e6-15fbf044a604 ==============================
[0m20:10:52.364071 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:10:52.364071 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt test --select stg_churn_events', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:10:55.900432 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:10:55.900432 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:10:55.900432 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:10:59.285679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB85EEC460>]}
[0m20:10:59.550915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB85EEE0B0>]}
[0m20:10:59.550915 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:11:01.273010 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:11:01.273010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB859F9120>]}
[0m20:11:01.354454 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:11:02.395643 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:11:02.395643 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:11:02.395643 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:11:02.630400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB99C850C0>]}
[0m20:11:03.048602 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:11:03.057657 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:11:03.139237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB99C87220>]}
[0m20:11:03.139237 [info ] [MainThread]: Found 4 models, 15 data tests, 845 macros
[0m20:11:03.139237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB99C86F80>]}
[0m20:11:03.151221 [info ] [MainThread]: 
[0m20:11:03.151221 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:11:03.159519 [info ] [MainThread]: 
[0m20:11:03.163622 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:11:03.165630 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:11:03.202319 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:11:03.206914 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:11:03.266656 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:11:03.269743 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:11:03.270795 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:11:08.385994 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd7-e093-1b48-8306-29c519b57bdc) - Created
[0m20:11:10.042059 [debug] [ThreadPool]: SQL status: OK in 6.770 seconds
[0m20:11:10.063658 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd7-e093-1b48-8306-29c519b57bdc, command-id=01f10cd7-e0c2-1b69-948f-362489930d03) - Closing
[0m20:11:10.063658 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:11:10.071823 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd7-e093-1b48-8306-29c519b57bdc) - Closing
[0m20:11:10.376762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB99C65FF0>]}
[0m20:11:10.397297 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:11:10.401324 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_churn_events_account_id ......................... [RUN]
[0m20:11:10.407389 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d) - Creating connection
[0m20:11:10.407389 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d'
[0m20:11:10.407389 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:11:10.478825 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:11:10.489315 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:11:10.577832 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:11:10.585052 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:11:10.585052 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_churn_events`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:11:10.585052 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:11:15.353445 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-e4ba-1660-abd0-ecaf46d2c4e1) - Created
[0m20:11:16.426860 [debug] [Thread-2 (]: SQL status: OK in 5.840 seconds
[0m20:11:16.437250 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd7-e4ba-1660-abd0-ecaf46d2c4e1, command-id=01f10cd7-e4eb-12c9-8266-6221bd2dc5d0) - Closing
[0m20:11:16.457593 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: Close
[0m20:11:16.457593 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-e4ba-1660-abd0-ecaf46d2c4e1) - Closing
[0m20:11:16.720032 [info ] [Thread-2 (]: 1 of 4 PASS not_null_stg_churn_events_account_id ............................... [[32mPASS[0m in 6.31s]
[0m20:11:16.728385 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:11:16.728385 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:11:16.738588 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_churn_events_churn_event_id ..................... [RUN]
[0m20:11:16.744404 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc) - Creating connection
[0m20:11:16.750978 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc'
[0m20:11:16.761260 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:11:16.781454 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:11:16.789745 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:11:16.800251 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:11:16.800251 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:11:16.800251 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select churn_event_id
from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is null



  
  
      
    ) dbt_internal_test
[0m20:11:16.800251 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:11:21.658145 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-e87f-1a82-a5d5-fd5db0d8deaa) - Created
[0m20:11:22.709562 [debug] [Thread-2 (]: SQL status: OK in 5.910 seconds
[0m20:11:22.709562 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd7-e87f-1a82-a5d5-fd5db0d8deaa, command-id=01f10cd7-e8ac-1ec8-b0bf-70e0d3496901) - Closing
[0m20:11:22.709562 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: Close
[0m20:11:22.721470 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-e87f-1a82-a5d5-fd5db0d8deaa) - Closing
[0m20:11:23.029269 [info ] [Thread-2 (]: 2 of 4 PASS not_null_stg_churn_events_churn_event_id ........................... [[32mPASS[0m in 6.28s]
[0m20:11:23.029269 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:11:23.045501 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:11:23.045501 [info ] [Thread-2 (]: 3 of 4 START test relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:11:23.045501 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b) - Creating connection
[0m20:11:23.061524 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b'
[0m20:11:23.061524 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:11:23.126590 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:11:23.126590 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:11:23.143328 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:11:23.155988 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:11:23.158777 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_churn_events`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:11:23.160131 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:11:28.050356 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-ec4b-1830-b4cb-e660aae1b46b) - Created
[0m20:11:30.653932 [debug] [Thread-2 (]: SQL status: OK in 7.490 seconds
[0m20:11:30.675662 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd7-ec4b-1830-b4cb-e660aae1b46b, command-id=01f10cd7-ec7a-1fa5-b0db-374fdbae777a) - Closing
[0m20:11:30.679896 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: Close
[0m20:11:30.679896 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-ec4b-1830-b4cb-e660aae1b46b) - Closing
[0m20:11:30.956605 [info ] [Thread-2 (]: 3 of 4 PASS relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 7.91s]
[0m20:11:30.956605 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:11:30.956605 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:11:30.972320 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_churn_events_churn_event_id ....................... [RUN]
[0m20:11:30.974226 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947) - Creating connection
[0m20:11:30.974226 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947'
[0m20:11:30.980526 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:11:31.031912 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:11:31.035928 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:11:31.041670 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:11:31.041670 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:11:31.055091 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    churn_event_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is not null
group by churn_event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:11:31.058278 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:11:36.256578 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-f139-1d6b-bd6b-6149963b91ea) - Created
[0m20:11:37.258986 [debug] [Thread-2 (]: SQL status: OK in 6.200 seconds
[0m20:11:37.274984 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd7-f139-1d6b-bd6b-6149963b91ea, command-id=01f10cd7-f15f-1b9c-8998-8ee1f5099238) - Closing
[0m20:11:37.274984 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: Close
[0m20:11:37.274984 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-f139-1d6b-bd6b-6149963b91ea) - Closing
[0m20:11:37.562249 [info ] [Thread-2 (]: 4 of 4 PASS unique_stg_churn_events_churn_event_id ............................. [[32mPASS[0m in 6.59s]
[0m20:11:37.562249 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:11:37.578341 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:11:37.580694 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:11:37.582702 [info ] [MainThread]: 
[0m20:11:37.587271 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 34.42 seconds (34.42s).
[0m20:11:37.589279 [debug] [MainThread]: Command end result
[0m20:11:37.802391 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:11:37.802391 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:11:37.843196 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:11:37.843196 [info ] [MainThread]: 
[0m20:11:37.843196 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:11:37.850237 [info ] [MainThread]: 
[0m20:11:37.854251 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m20:11:37.854251 [debug] [MainThread]: Command `dbt test` succeeded at 20:11:37.854251 after 45.82 seconds
[0m20:11:37.859886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB83BCABF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB85EEE0B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB99903DC0>]}
[0m20:11:37.861896 [debug] [MainThread]: Flushing usage events
[0m20:11:41.051089 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:15:28.097352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B23BEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B46DC3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B46DC1F0>]}


============================== 20:15:28.107491 | 6d379967-da85-49c2-9669-b8a7b2d067d5 ==============================
[0m20:15:28.107491 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:15:28.113232 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select stg_support_tickets', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:15:31.316805 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:15:31.316805 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:15:31.324710 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:15:34.023659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B46DC460>]}
[0m20:15:34.232381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B6F57EB0>]}
[0m20:15:34.232381 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:15:35.732125 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:15:35.732125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B4164D00>]}
[0m20:15:35.798322 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:15:36.640154 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m20:15:36.649683 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\silver\stg_support_tickets.sql
[0m20:15:36.649683 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\schema.yml
[0m20:15:37.758588 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on
'stg_support_tickets' in package 'saas_dbt_analytics'
(models\silver\schema.yml). Arguments to generic tests should be nested under
the `arguments` property.
[0m20:15:37.758588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9C87EAE60>]}
[0m20:15:38.059713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9C8854E50>]}
[0m20:15:38.561324 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:15:38.561324 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:15:38.611647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9C85EC340>]}
[0m20:15:38.611647 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:15:38.615764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9C85EE4A0>]}
[0m20:15:38.621505 [info ] [MainThread]: 
[0m20:15:38.621505 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:15:38.625954 [info ] [MainThread]: 
[0m20:15:38.626975 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:15:38.626975 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:15:38.630675 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:15:38.630675 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:15:38.680026 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:15:38.682034 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:15:38.682034 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:15:43.441701 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-8487-19d1-9021-71af68136ee3) - Created
[0m20:15:44.201940 [debug] [ThreadPool]: SQL status: OK in 5.520 seconds
[0m20:15:44.221549 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd8-8487-19d1-9021-71af68136ee3, command-id=01f10cd8-84b7-14b7-ba5c-ab3d124405a1) - Closing
[0m20:15:44.233592 [debug] [ThreadPool]: On list_workspace: Close
[0m20:15:44.233592 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-8487-19d1-9021-71af68136ee3) - Closing
[0m20:15:44.613532 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:15:44.613532 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:15:44.633848 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:15:44.635857 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:15:44.637867 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:15:49.306871 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-880d-1bd5-b227-6e90a759100a) - Created
[0m20:15:51.062032 [debug] [ThreadPool]: SQL status: OK in 6.420 seconds
[0m20:15:51.089756 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd8-880d-1bd5-b227-6e90a759100a, command-id=01f10cd8-8833-1a17-8350-4f142a8b6944) - Closing
[0m20:15:51.095525 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:15:51.099549 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-880d-1bd5-b227-6e90a759100a) - Closing
[0m20:15:51.430679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9C860ED40>]}
[0m20:15:51.437456 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:15:51.448163 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_support_tickets ...................... [RUN]
[0m20:15:51.457230 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:15:51.457230 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:15:51.457230 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:15:51.506885 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:15:51.506885 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:15:51.577729 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:15:51.593391 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:15:51.593391 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B4637E80>]}
[0m20:15:51.649033 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:15:51.673752 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:15:51.693527 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:15:51.695536 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_churn_events

),

cleaned as (

    select
        ticket_id
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:15:51.695536 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:15:56.851131 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd8-8c84-15de-8088-ec7b6b712268) - Created
[0m20:15:58.687165 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_churn_events

),

cleaned as (

    select
        ticket_id
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `ingestion_ts`, `churn_date`, `churn_event_id`, `reason_code`]. SQLSTATE: 42703; line 31 pos 10
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `ingestion_ts`, `churn_date`, `churn_event_id`, `reason_code`]. SQLSTATE: 42703; line 31 pos 10
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `ingestion_ts`, `churn_date`, `churn_event_id`, `reason_code`]. SQLSTATE: 42703; line 31 pos 10
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd8-8cb3-1f53-ab11-a9e770809385
[0m20:15:58.687165 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:15:58.700508 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd8-8c84-15de-8088-ec7b6b712268) - Closing
[0m20:15:59.031094 [debug] [Thread-3 (]: Database Error in model stg_support_tickets (models\silver\stg_support_tickets.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `ingestion_ts`, `churn_date`, `churn_event_id`, `reason_code`]. SQLSTATE: 42703; line 31 pos 10
  compiled code at target\run\saas_dbt_analytics\models\silver\stg_support_tickets.sql
[0m20:15:59.046924 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B3DC5DE0>]}
[0m20:15:59.046924 [error] [Thread-3 (]: 1 of 1 ERROR creating sql view model analytics.stg_support_tickets ............. [[31mERROR[0m in 7.58s]
[0m20:15:59.046924 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:15:59.046924 [debug] [Thread-6 (]: Marking all children of 'model.saas_dbt_analytics.stg_support_tickets' to be skipped because of status 'error'.  Reason: Database Error in model stg_support_tickets (models\silver\stg_support_tickets.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `ingestion_ts`, `churn_date`, `churn_event_id`, `reason_code`]. SQLSTATE: 42703; line 31 pos 10
  compiled code at target\run\saas_dbt_analytics\models\silver\stg_support_tickets.sql.
[0m20:15:59.064836 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:15:59.073105 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:15:59.073105 [info ] [MainThread]: 
[0m20:15:59.073105 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 20.45 seconds (20.45s).
[0m20:15:59.073105 [debug] [MainThread]: Command end result
[0m20:15:59.240379 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:15:59.250685 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:15:59.272328 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:15:59.274337 [info ] [MainThread]: 
[0m20:15:59.276346 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m20:15:59.276346 [info ] [MainThread]: 
[0m20:15:59.280618 [error] [MainThread]: [31mFailure in model stg_support_tickets (models\silver\stg_support_tickets.sql)[0m
[0m20:15:59.286069 [error] [MainThread]:   Database Error in model stg_support_tickets (models\silver\stg_support_tickets.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `ingestion_ts`, `churn_date`, `churn_event_id`, `reason_code`]. SQLSTATE: 42703; line 31 pos 10
  compiled code at target\run\saas_dbt_analytics\models\silver\stg_support_tickets.sql
[0m20:15:59.290084 [info ] [MainThread]: 
[0m20:15:59.292094 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\stg_support_tickets.sql
[0m20:15:59.294103 [info ] [MainThread]: 
[0m20:15:59.298122 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m20:15:59.300132 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m20:15:59.306231 [debug] [MainThread]: Command `dbt run` failed at 20:15:59.304152 after 31.48 seconds
[0m20:15:59.308242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B23BEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9C80D4250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B46A5F30>]}
[0m20:15:59.310488 [debug] [MainThread]: Flushing usage events
[0m20:16:02.584470 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:16:25.120227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE6688EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE68BA0370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE68BA01C0>]}


============================== 20:16:25.135430 | 36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc ==============================
[0m20:16:25.135430 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:16:25.135430 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt test --select stg_support_tickets', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:16:28.300756 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:16:28.300756 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:16:28.300756 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:16:31.083178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE68BA0430>]}
[0m20:16:31.298926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE6B59CC10>]}
[0m20:16:31.300931 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:16:32.818109 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:16:32.821221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE6852B4F0>]}
[0m20:16:32.885398 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:16:33.823032 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:16:33.825804 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:16:33.825804 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:16:34.016275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE7C986410>]}
[0m20:16:34.397560 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:16:34.407606 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:16:34.482004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE7C905630>]}
[0m20:16:34.484013 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:16:34.486019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE7C7CA920>]}
[0m20:16:34.488949 [info ] [MainThread]: 
[0m20:16:34.495717 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:16:34.499037 [info ] [MainThread]: 
[0m20:16:34.502713 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:16:34.504722 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:16:34.538086 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:16:34.538086 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:16:34.592125 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:16:34.594362 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:16:34.596370 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:16:40.067165 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-a636-1667-bc53-ede6e3357308) - Created
[0m20:16:40.808429 [debug] [ThreadPool]: SQL status: OK in 6.210 seconds
[0m20:16:40.826699 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd8-a636-1667-bc53-ede6e3357308, command-id=01f10cd8-a675-1012-9dfa-edc8fc66e1b7) - Closing
[0m20:16:40.831168 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:16:40.833006 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-a636-1667-bc53-ede6e3357308) - Closing
[0m20:16:41.098883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE7C8D3490>]}
[0m20:16:41.127857 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:16:41.131997 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_support_tickets_account_id ...................... [RUN]
[0m20:16:41.131997 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:16:41.131997 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:16:41.143765 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:16:41.267767 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:16:41.267767 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:16:41.365416 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:16:41.365416 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:16:41.381230 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:16:41.383673 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:16:46.236944 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-a9fc-136f-b720-bb7532ae63a0) - Created
[0m20:16:47.037495 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd8-aa22-15b6-9cff-4679c2a0e796
[0m20:16:47.046413 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:16:47.051449 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-a9fc-136f-b720-bb7532ae63a0) - Closing
[0m20:16:47.492482 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql
[0m20:16:47.496555 [error] [Thread-2 (]: 1 of 4 ERROR not_null_stg_support_tickets_account_id ........................... [[31mERROR[0m in 6.36s]
[0m20:16:47.500764 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:16:47.505286 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:16:47.509777 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql.
[0m20:16:47.514226 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:16:47.524299 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:16:47.528326 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:16:47.534379 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:16:47.584262 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:16:47.588286 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:16:47.600954 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:16:47.605173 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:16:47.605173 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:16:47.609278 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:16:52.154223 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-ad7a-150e-9faf-5286b99066ba) - Created
[0m20:16:52.969893 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd8-ada9-1c90-aabd-4571963cf4b4
[0m20:16:52.977979 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:16:52.977979 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-ad7a-150e-9faf-5286b99066ba) - Closing
[0m20:16:53.284861 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:16:53.288900 [error] [Thread-2 (]: 2 of 4 ERROR not_null_stg_support_tickets_ticket_id ............................ [[31mERROR[0m in 5.76s]
[0m20:16:53.294931 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:16:53.301202 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:16:53.303215 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql.
[0m20:16:53.305237 [info ] [Thread-2 (]: 3 of 4 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:16:53.313608 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:16:53.317962 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:16:53.322242 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:16:53.392553 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:16:53.398745 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:16:53.419378 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:16:53.425473 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:16:53.429493 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:16:53.429493 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:16:58.285010 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-b124-124a-b4b3-ef8e287ee06e) - Created
[0m20:16:59.020905 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd8-b151-1753-9a23-5c13ae42c010
[0m20:16:59.026902 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:16:59.030906 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-b124-124a-b4b3-ef8e287ee06e) - Closing
[0m20:16:59.325199 [debug] [Thread-2 (]: Database Error in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:16:59.328203 [error] [Thread-2 (]: 3 of 4 ERROR relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[31mERROR[0m in 6.01s]
[0m20:16:59.334434 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:16:59.341872 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:16:59.345962 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e' to be skipped because of status 'error'.  Reason: Database Error in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql.
[0m20:16:59.350216 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_support_tickets_ticket_id ......................... [RUN]
[0m20:16:59.363488 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:16:59.367953 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:16:59.372953 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:16:59.421228 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:16:59.426626 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:16:59.441942 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:16:59.446749 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:16:59.449940 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:16:59.452108 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:17:04.511579 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-b4e2-1982-b5b6-b8ea3717a723) - Created
[0m20:17:05.264270 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd8-b508-19a5-b25c-b0bb05371ddd
[0m20:17:05.269270 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:17:05.272333 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-b4e2-1982-b5b6-b8ea3717a723) - Closing
[0m20:17:06.186238 [debug] [Thread-2 (]: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:17:06.189241 [error] [Thread-2 (]: 4 of 4 ERROR unique_stg_support_tickets_ticket_id .............................. [[31mERROR[0m in 6.83s]
[0m20:17:06.192594 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:17:06.194814 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a' to be skipped because of status 'error'.  Reason: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql.
[0m20:17:06.199061 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:17:06.201061 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:17:06.204063 [info ] [MainThread]: 
[0m20:17:06.206220 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 31.70 seconds (31.70s).
[0m20:17:06.211485 [debug] [MainThread]: Command end result
[0m20:17:06.419416 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:17:06.430356 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:17:06.466849 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:17:06.468850 [info ] [MainThread]: 
[0m20:17:06.471853 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m20:17:06.475181 [info ] [MainThread]: 
[0m20:17:06.478962 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)[0m
[0m20:17:06.482074 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql
[0m20:17:06.486033 [info ] [MainThread]: 
[0m20:17:06.489756 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql
[0m20:17:06.493230 [info ] [MainThread]: 
[0m20:17:06.495611 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:17:06.499164 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:17:06.502172 [info ] [MainThread]: 
[0m20:17:06.504616 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:17:06.507746 [info ] [MainThread]: 
[0m20:17:06.509857 [error] [MainThread]: [31mFailure in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)[0m
[0m20:17:06.513065 [error] [MainThread]:   Database Error in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:17:06.515065 [info ] [MainThread]: 
[0m20:17:06.518789 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:17:06.521441 [info ] [MainThread]: 
[0m20:17:06.524539 [error] [MainThread]: [31mFailure in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:17:06.526541 [error] [MainThread]:   Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:17:06.528538 [info ] [MainThread]: 
[0m20:17:06.532099 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:17:06.534098 [info ] [MainThread]: 
[0m20:17:06.537687 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=4 SKIP=0 NO-OP=0 TOTAL=4
[0m20:17:06.541738 [debug] [MainThread]: Command `dbt test` failed at 20:17:06.541738 after 41.70 seconds
[0m20:17:06.543738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE6688EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE67BFB910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE7CBBA9E0>]}
[0m20:17:06.545875 [debug] [MainThread]: Flushing usage events
[0m20:17:10.478393 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:18:49.168347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017063DBEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000170660DC3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000170660DC1F0>]}


============================== 20:18:49.175059 | cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c ==============================
[0m20:18:49.175059 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:18:49.175059 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt test', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:18:52.368143 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:18:52.368143 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:18:52.368143 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:18:55.145978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000170660DC460>]}
[0m20:18:55.367978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001706896FF70>]}
[0m20:18:55.367978 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:18:57.018964 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:18:57.018964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017065BEB340>]}
[0m20:18:57.101301 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:18:58.029787 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:18:58.030353 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\stg_feature_usage.sql
[0m20:18:58.035798 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\schema.yml
[0m20:18:59.824284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001707A2DC130>]}
[0m20:19:00.428504 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:19:00.433094 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:19:00.513740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001707A0CD900>]}
[0m20:19:00.519114 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:19:00.521757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001707A0CDB70>]}
[0m20:19:00.528512 [info ] [MainThread]: 
[0m20:19:00.528512 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:19:00.533210 [info ] [MainThread]: 
[0m20:19:00.538305 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:19:00.542719 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:19:00.569178 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:19:00.577545 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:19:00.618783 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:19:00.618783 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:19:00.618783 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:19:05.579303 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-fd04-1a09-b823-3f451d0c759f) - Created
[0m20:19:07.536002 [debug] [ThreadPool]: SQL status: OK in 6.920 seconds
[0m20:19:07.547189 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd8-fd04-1a09-b823-3f451d0c759f, command-id=01f10cd8-fd33-1580-ad88-3297b3d07c16) - Closing
[0m20:19:07.547189 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:19:07.547189 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-fd04-1a09-b823-3f451d0c759f) - Closing
[0m20:19:07.840543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001707A033C10>]}
[0m20:19:07.864691 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:19:07.866549 [info ] [Thread-2 (]: 1 of 19 START test dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [RUN]
[0m20:19:07.869882 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77) - Creating connection
[0m20:19:07.871889 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77'
[0m20:19:07.873902 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:19:07.901658 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:19:07.912333 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:19:07.978476 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:19:07.978476 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:19:07.978476 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  





with validation_errors as (

    select
        subscription_id, usage_date, feature_name
    from `workspace`.`analytics`.`stg_feature_usage`
    group by subscription_id, usage_date, feature_name
    having count(*) > 1

)

select *
from validation_errors



  
  
      
    ) dbt_internal_test
[0m20:19:07.978476 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:13.157657 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-0187-1248-b4f4-86dbfabea05f) - Created
[0m20:19:13.781774 [debug] [Thread-2 (]: SQL status: OK in 5.800 seconds
[0m20:19:13.793581 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-0187-1248-b4f4-86dbfabea05f, command-id=01f10cd9-01b6-17a2-b714-a5297392cf04) - Closing
[0m20:19:13.807372 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: Close
[0m20:19:13.807372 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-0187-1248-b4f4-86dbfabea05f) - Closing
[0m20:19:14.111045 [info ] [Thread-2 (]: 1 of 19 PASS dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [[32mPASS[0m in 6.24s]
[0m20:19:14.117507 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:19:14.120528 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:19:14.126170 [info ] [Thread-2 (]: 2 of 19 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m20:19:14.141265 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m20:19:14.146236 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108'
[0m20:19:14.149801 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:19:14.191446 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:19:14.197584 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:19:14.212327 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:19:14.226611 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:19:14.226611 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:14.226611 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:18.480012 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-04b4-15f3-8461-bb0c9f2bfc57) - Created
[0m20:19:19.410389 [debug] [Thread-2 (]: SQL status: OK in 5.180 seconds
[0m20:19:19.423811 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-04b4-15f3-8461-bb0c9f2bfc57, command-id=01f10cd9-04e4-1226-9b8d-a4dfef5ffbc0) - Closing
[0m20:19:19.427107 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: Close
[0m20:19:19.427107 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-04b4-15f3-8461-bb0c9f2bfc57) - Closing
[0m20:19:19.735531 [info ] [Thread-2 (]: 2 of 19 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 5.60s]
[0m20:19:19.746259 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:19:19.746259 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:19:19.746259 [info ] [Thread-2 (]: 3 of 19 START test not_null_stg_accounts_seats ................................. [RUN]
[0m20:19:19.758125 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39) - Creating connection
[0m20:19:19.760523 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39'
[0m20:19:19.765462 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:19:19.805196 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:19:19.810605 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:19:19.821068 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:19:19.825725 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:19:19.827829 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select seats
from `workspace`.`analytics`.`stg_accounts`
where seats is null



  
  
      
    ) dbt_internal_test
[0m20:19:19.827829 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:24.421586 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-083f-10a4-be2f-341dbc3d78ea) - Created
[0m20:19:25.253266 [debug] [Thread-2 (]: SQL status: OK in 5.430 seconds
[0m20:19:25.257296 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-083f-10a4-be2f-341dbc3d78ea, command-id=01f10cd9-086d-1b36-a4f3-38f50d720a26) - Closing
[0m20:19:25.265161 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: Close
[0m20:19:25.265161 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-083f-10a4-be2f-341dbc3d78ea) - Closing
[0m20:19:25.554432 [info ] [Thread-2 (]: 3 of 19 PASS not_null_stg_accounts_seats ....................................... [[32mPASS[0m in 5.81s]
[0m20:19:25.560295 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:19:25.562305 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:19:25.566325 [info ] [Thread-2 (]: 4 of 19 START test not_null_stg_accounts_signup_date ........................... [RUN]
[0m20:19:25.571002 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a) - Creating connection
[0m20:19:25.575022 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a'
[0m20:19:25.577089 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:19:25.616377 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:19:25.620399 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:19:25.638708 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:19:25.640716 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:19:25.646957 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signup_date
from `workspace`.`analytics`.`stg_accounts`
where signup_date is null



  
  
      
    ) dbt_internal_test
[0m20:19:25.646957 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:30.259140 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-0bb9-1295-aad8-c00553781d7d) - Created
[0m20:19:31.074158 [debug] [Thread-2 (]: SQL status: OK in 5.430 seconds
[0m20:19:31.089428 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-0bb9-1295-aad8-c00553781d7d, command-id=01f10cd9-0be7-1c51-b0e2-fe61fde08660) - Closing
[0m20:19:31.093264 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: Close
[0m20:19:31.095430 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-0bb9-1295-aad8-c00553781d7d) - Closing
[0m20:19:31.380193 [info ] [Thread-2 (]: 4 of 19 PASS not_null_stg_accounts_signup_date ................................. [[32mPASS[0m in 5.81s]
[0m20:19:31.390756 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:19:31.392849 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:19:31.395694 [info ] [Thread-2 (]: 5 of 19 START test not_null_stg_churn_events_account_id ........................ [RUN]
[0m20:19:31.400224 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d) - Creating connection
[0m20:19:31.401637 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d'
[0m20:19:31.401637 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:19:31.426488 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:19:31.434376 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:19:31.449089 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:19:31.453104 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:19:31.455112 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_churn_events`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:31.457471 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:36.097125 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-0f34-16f1-ab30-6dc7ff3fca3d) - Created
[0m20:19:36.613379 [debug] [Thread-2 (]: SQL status: OK in 5.160 seconds
[0m20:19:36.620479 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-0f34-16f1-ab30-6dc7ff3fca3d, command-id=01f10cd9-0f64-1559-97ff-4b2a596924b0) - Closing
[0m20:19:36.623404 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: Close
[0m20:19:36.626706 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-0f34-16f1-ab30-6dc7ff3fca3d) - Closing
[0m20:19:36.931255 [info ] [Thread-2 (]: 5 of 19 PASS not_null_stg_churn_events_account_id .............................. [[32mPASS[0m in 5.53s]
[0m20:19:36.932765 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:19:36.934773 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:19:36.934773 [info ] [Thread-2 (]: 6 of 19 START test not_null_stg_churn_events_churn_event_id .................... [RUN]
[0m20:19:36.941378 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc) - Creating connection
[0m20:19:36.942943 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc'
[0m20:19:36.946281 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:19:36.984593 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:19:36.986601 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:19:36.991584 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:19:37.005636 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:19:37.007698 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select churn_event_id
from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:37.010889 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:41.384642 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-1260-12f9-addc-7cd60a11d92b) - Created
[0m20:19:41.839508 [debug] [Thread-2 (]: SQL status: OK in 4.830 seconds
[0m20:19:41.847885 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-1260-12f9-addc-7cd60a11d92b, command-id=01f10cd9-1288-1e6f-b7c6-2eac9b0f6c4b) - Closing
[0m20:19:41.856212 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: Close
[0m20:19:41.856212 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-1260-12f9-addc-7cd60a11d92b) - Closing
[0m20:19:42.147371 [info ] [Thread-2 (]: 6 of 19 PASS not_null_stg_churn_events_churn_event_id .......................... [[32mPASS[0m in 5.21s]
[0m20:19:42.155993 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:19:42.158000 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:19:42.162017 [info ] [Thread-2 (]: 7 of 19 START test not_null_stg_feature_usage_subscription_id .................. [RUN]
[0m20:19:42.200000 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3) - Creating connection
[0m20:19:42.203760 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3'
[0m20:19:42.205958 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:19:42.225866 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:19:42.231922 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:19:42.242377 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:19:42.252034 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:19:42.252034 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_feature_usage`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:42.255924 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:46.676522 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-158a-100b-8470-5c3f519bf171) - Created
[0m20:19:47.366812 [debug] [Thread-2 (]: SQL status: OK in 5.110 seconds
[0m20:19:47.377769 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-158a-100b-8470-5c3f519bf171, command-id=01f10cd9-15b0-1fcb-b47d-edf06fb03482) - Closing
[0m20:19:47.382789 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: Close
[0m20:19:47.382789 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-158a-100b-8470-5c3f519bf171) - Closing
[0m20:19:47.675815 [info ] [Thread-2 (]: 7 of 19 PASS not_null_stg_feature_usage_subscription_id ........................ [[32mPASS[0m in 5.48s]
[0m20:19:47.677823 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:19:47.677823 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:19:47.686960 [info ] [Thread-2 (]: 8 of 19 START test not_null_stg_subscriptions_account_id ....................... [RUN]
[0m20:19:47.691351 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9) - Creating connection
[0m20:19:47.691351 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9'
[0m20:19:47.691351 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:19:47.758182 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:19:47.758182 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:19:47.782624 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:19:47.790590 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:19:47.790590 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_subscriptions`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:47.790590 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:52.173194 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-18c8-1c59-9070-de0daa125ab2) - Created
[0m20:19:53.306003 [debug] [Thread-2 (]: SQL status: OK in 5.520 seconds
[0m20:19:53.306003 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-18c8-1c59-9070-de0daa125ab2, command-id=01f10cd9-18f7-1e14-9ace-eb486593c72a) - Closing
[0m20:19:53.322159 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: Close
[0m20:19:53.322159 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-18c8-1c59-9070-de0daa125ab2) - Closing
[0m20:19:53.640357 [info ] [Thread-2 (]: 8 of 19 PASS not_null_stg_subscriptions_account_id ............................. [[32mPASS[0m in 5.95s]
[0m20:19:53.640357 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:19:53.640357 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:19:53.640357 [info ] [Thread-2 (]: 9 of 19 START test not_null_stg_subscriptions_subscription_id .................. [RUN]
[0m20:19:53.640357 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91) - Creating connection
[0m20:19:53.651181 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m20:19:53.651181 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:19:53.680142 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:19:53.680142 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:19:53.689730 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:19:53.689730 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:19:53.703276 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:53.705076 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:58.100053 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-1c53-1427-b7be-6a85cb3ae095) - Created
[0m20:19:59.038069 [debug] [Thread-2 (]: SQL status: OK in 5.330 seconds
[0m20:19:59.042085 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-1c53-1427-b7be-6a85cb3ae095, command-id=01f10cd9-1c82-11a7-8935-24b31e4b58be) - Closing
[0m20:19:59.046104 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m20:19:59.046104 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-1c53-1427-b7be-6a85cb3ae095) - Closing
[0m20:19:59.344431 [info ] [Thread-2 (]: 9 of 19 PASS not_null_stg_subscriptions_subscription_id ........................ [[32mPASS[0m in 5.70s]
[0m20:19:59.347697 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:19:59.347697 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:19:59.347697 [info ] [Thread-2 (]: 10 of 19 START test not_null_stg_support_tickets_account_id .................... [RUN]
[0m20:19:59.357505 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:19:59.360290 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:19:59.361830 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:19:59.406367 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:19:59.406367 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:19:59.420476 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:19:59.423074 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:19:59.423074 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:59.423074 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:03.835644 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-1fbd-10f0-baf9-a4a04cb968e2) - Created
[0m20:20:04.660171 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd9-1fec-1415-bf9e-84d9c99ecefc
[0m20:20:04.660171 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:20:04.660171 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-1fbd-10f0-baf9-a4a04cb968e2) - Closing
[0m20:20:05.706411 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql
[0m20:20:05.706411 [error] [Thread-2 (]: 10 of 19 ERROR not_null_stg_support_tickets_account_id ......................... [[31mERROR[0m in 6.36s]
[0m20:20:05.706411 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:20:05.706411 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:20:05.706411 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql.
[0m20:20:05.722152 [info ] [Thread-2 (]: 11 of 19 START test not_null_stg_support_tickets_ticket_id ..................... [RUN]
[0m20:20:05.727655 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:20:05.730551 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:20:05.730551 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:20:05.749503 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:20:05.755754 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:20:05.769629 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:20:05.771519 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:20:05.771519 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:20:05.771519 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:10.299741 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-2398-1a86-83af-133431112140) - Created
[0m20:20:11.042476 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd9-23c6-1b29-8d33-6707542b1537
[0m20:20:11.042476 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:20:11.042476 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-2398-1a86-83af-133431112140) - Closing
[0m20:20:11.363878 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:20:11.364886 [error] [Thread-2 (]: 11 of 19 ERROR not_null_stg_support_tickets_ticket_id .......................... [[31mERROR[0m in 5.64s]
[0m20:20:11.364886 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:20:11.364886 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:20:11.364886 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql.
[0m20:20:11.378156 [info ] [Thread-2 (]: 12 of 19 START test relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:20:11.389359 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b) - Creating connection
[0m20:20:11.391371 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b'
[0m20:20:11.399570 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:20:11.453851 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:20:11.453851 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:20:11.477261 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:20:11.477261 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:20:11.477261 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_churn_events`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:20:11.485298 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:16.851319 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-277f-1fe9-a04f-454b97c12f3f) - Created
[0m20:20:17.373074 [debug] [Thread-2 (]: SQL status: OK in 5.890 seconds
[0m20:20:17.384865 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-277f-1fe9-a04f-454b97c12f3f, command-id=01f10cd9-27ae-1c3d-b257-d12700200b05) - Closing
[0m20:20:17.389082 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: Close
[0m20:20:17.393098 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-277f-1fe9-a04f-454b97c12f3f) - Closing
[0m20:20:17.676323 [info ] [Thread-2 (]: 12 of 19 PASS relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 6.29s]
[0m20:20:17.678331 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:20:17.682517 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:20:17.682517 [info ] [Thread-2 (]: 13 of 19 START test relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [RUN]
[0m20:20:17.691791 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850) - Creating connection
[0m20:20:17.696654 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850'
[0m20:20:17.696654 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:20:17.739153 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:20:17.739153 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:20:17.757516 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:20:17.768220 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:20:17.768220 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select subscription_id as from_field
    from `workspace`.`analytics`.`stg_feature_usage`
    where subscription_id is not null
),

parent as (
    select subscription_id as to_field
    from `workspace`.`analytics`.`stg_subscriptions`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:20:17.773824 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:22.072636 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-2a9c-18be-832a-da2d8b931eb7) - Created
[0m20:20:22.683479 [debug] [Thread-2 (]: SQL status: OK in 4.910 seconds
[0m20:20:22.699373 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-2a9c-18be-832a-da2d8b931eb7, command-id=01f10cd9-2ac9-1848-9c87-7a47fefcf551) - Closing
[0m20:20:22.699373 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: Close
[0m20:20:22.699373 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-2a9c-18be-832a-da2d8b931eb7) - Closing
[0m20:20:23.000889 [info ] [Thread-2 (]: 13 of 19 PASS relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [[32mPASS[0m in 5.31s]
[0m20:20:23.002897 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:20:23.010093 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:20:23.012102 [info ] [Thread-2 (]: 14 of 19 START test relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:20:23.017376 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90) - Creating connection
[0m20:20:23.021401 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90'
[0m20:20:23.025429 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:20:23.090002 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:20:23.090002 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:20:23.117795 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:20:23.117795 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:20:23.123988 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_subscriptions`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:20:23.123988 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:27.501852 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-2dd8-1638-98d2-1829145a71d5) - Created
[0m20:20:28.734033 [debug] [Thread-2 (]: SQL status: OK in 5.610 seconds
[0m20:20:28.740059 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-2dd8-1638-98d2-1829145a71d5, command-id=01f10cd9-2e07-18b9-8387-ea5b69af6232) - Closing
[0m20:20:28.742339 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: Close
[0m20:20:28.744349 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-2dd8-1638-98d2-1829145a71d5) - Closing
[0m20:20:29.050364 [info ] [Thread-2 (]: 14 of 19 PASS relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 6.03s]
[0m20:20:29.053664 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:20:29.053664 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:20:29.053664 [info ] [Thread-2 (]: 15 of 19 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:20:29.062145 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:20:29.065160 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:20:29.067252 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:20:29.086471 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:20:29.086471 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:20:29.100778 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:20:29.100778 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:20:29.110574 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:20:29.110574 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:34.569183 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-320f-112c-a238-c8a2322f439c) - Created
[0m20:20:35.482660 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd9-323e-1d46-871d-b9ae9f5d5ef0
[0m20:20:35.482660 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:20:35.482660 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-320f-112c-a238-c8a2322f439c) - Closing
[0m20:20:36.434988 [debug] [Thread-2 (]: Database Error in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:20:36.434988 [error] [Thread-2 (]: 15 of 19 ERROR relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[31mERROR[0m in 7.38s]
[0m20:20:36.442845 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:20:36.442845 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:20:36.442845 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e' to be skipped because of status 'error'.  Reason: Database Error in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql.
[0m20:20:36.450202 [info ] [Thread-2 (]: 16 of 19 START test unique_stg_accounts_account_id ............................. [RUN]
[0m20:20:36.456092 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m20:20:36.460287 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79'
[0m20:20:36.461097 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:20:36.521625 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:20:36.529670 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:20:36.546545 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:20:36.546545 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:20:36.554837 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:20:36.554837 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:40.916443 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-35d6-19e9-9216-547acc51a52f) - Created
[0m20:20:41.732798 [debug] [Thread-2 (]: SQL status: OK in 5.180 seconds
[0m20:20:41.754696 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-35d6-19e9-9216-547acc51a52f, command-id=01f10cd9-3604-1b8f-b8af-ef3c98de47e6) - Closing
[0m20:20:41.760724 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: Close
[0m20:20:41.764481 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-35d6-19e9-9216-547acc51a52f) - Closing
[0m20:20:42.045805 [info ] [Thread-2 (]: 16 of 19 PASS unique_stg_accounts_account_id ................................... [[32mPASS[0m in 5.59s]
[0m20:20:42.047813 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:20:42.047813 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:20:42.047813 [info ] [Thread-2 (]: 17 of 19 START test unique_stg_churn_events_churn_event_id ..................... [RUN]
[0m20:20:42.062752 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947) - Creating connection
[0m20:20:42.067288 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947'
[0m20:20:42.072686 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:20:42.100754 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:20:42.100754 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:20:42.121983 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:20:42.129996 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:20:42.134526 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    churn_event_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is not null
group by churn_event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:20:42.138045 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:46.548670 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-3930-1d36-92aa-5373ea66b94f) - Created
[0m20:20:47.085929 [debug] [Thread-2 (]: SQL status: OK in 4.950 seconds
[0m20:20:47.097384 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-3930-1d36-92aa-5373ea66b94f, command-id=01f10cd9-3960-1208-9e09-065b53557aee) - Closing
[0m20:20:47.101957 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: Close
[0m20:20:47.101957 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-3930-1d36-92aa-5373ea66b94f) - Closing
[0m20:20:47.505567 [info ] [Thread-2 (]: 17 of 19 PASS unique_stg_churn_events_churn_event_id ........................... [[32mPASS[0m in 5.44s]
[0m20:20:47.510567 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:20:47.514238 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:20:47.520526 [info ] [Thread-2 (]: 18 of 19 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m20:20:47.526052 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e) - Creating connection
[0m20:20:47.526052 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m20:20:47.530154 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:20:47.562496 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:20:47.566552 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:20:47.580156 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:20:47.581337 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:20:47.581337 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:20:47.581337 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:51.975899 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-3c6d-1603-be3a-1c1be93d17bb) - Created
[0m20:20:53.028091 [debug] [Thread-2 (]: SQL status: OK in 5.450 seconds
[0m20:20:53.039941 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-3c6d-1603-be3a-1c1be93d17bb, command-id=01f10cd9-3c9c-167e-9c5c-dcb8118ea334) - Closing
[0m20:20:53.042016 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m20:20:53.044099 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-3c6d-1603-be3a-1c1be93d17bb) - Closing
[0m20:20:53.332577 [info ] [Thread-2 (]: 18 of 19 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 5.81s]
[0m20:20:53.332577 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:20:53.332577 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:20:53.340636 [info ] [Thread-2 (]: 19 of 19 START test unique_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:20:53.340636 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:20:53.348774 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:20:53.354359 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:20:53.383604 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:20:53.383604 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:20:53.398445 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:20:53.413895 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:20:53.413895 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:20:53.420068 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:57.813971 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-3fe9-1852-8391-db0a5ef1a39c) - Created
[0m20:20:58.531773 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd9-4017-14dd-85a7-f90749a778d4
[0m20:20:58.535792 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:20:58.537799 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-3fe9-1852-8391-db0a5ef1a39c) - Closing
[0m20:20:59.569424 [debug] [Thread-2 (]: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:20:59.571430 [error] [Thread-2 (]: 19 of 19 ERROR unique_stg_support_tickets_ticket_id ............................ [[31mERROR[0m in 6.23s]
[0m20:20:59.575183 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:20:59.575183 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a' to be skipped because of status 'error'.  Reason: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql.
[0m20:20:59.575183 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:20:59.575183 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:20:59.575183 [info ] [MainThread]: 
[0m20:20:59.575183 [info ] [MainThread]: Finished running 19 data tests in 0 hours 1 minutes and 59.04 seconds (119.04s).
[0m20:20:59.602796 [debug] [MainThread]: Command end result
[0m20:20:59.754614 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:20:59.763194 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:20:59.789109 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:20:59.789109 [info ] [MainThread]: 
[0m20:20:59.789109 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m20:20:59.789109 [info ] [MainThread]: 
[0m20:20:59.789109 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)[0m
[0m20:20:59.800113 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql
[0m20:20:59.803449 [info ] [MainThread]: 
[0m20:20:59.805725 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql
[0m20:20:59.808037 [info ] [MainThread]: 
[0m20:20:59.811565 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:20:59.814724 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:20:59.816734 [info ] [MainThread]: 
[0m20:20:59.819006 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:20:59.821256 [info ] [MainThread]: 
[0m20:20:59.825543 [error] [MainThread]: [31mFailure in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)[0m
[0m20:20:59.826940 [error] [MainThread]:   Database Error in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:20:59.826940 [info ] [MainThread]: 
[0m20:20:59.834739 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:20:59.834739 [info ] [MainThread]: 
[0m20:20:59.840045 [error] [MainThread]: [31mFailure in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:20:59.840391 [error] [MainThread]:   Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:20:59.842917 [info ] [MainThread]: 
[0m20:20:59.842917 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:20:59.850817 [info ] [MainThread]: 
[0m20:20:59.852825 [info ] [MainThread]: Done. PASS=15 WARN=0 ERROR=4 SKIP=0 NO-OP=0 TOTAL=19
[0m20:20:59.855930 [debug] [MainThread]: Command `dbt test` failed at 20:20:59.855930 after 130.96 seconds
[0m20:20:59.859030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017063DBEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001707A0CCA30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017079D9EC80>]}
[0m20:20:59.859030 [debug] [MainThread]: Flushing usage events
[0m20:21:03.130713 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:21:31.918683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DE934EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEB6683A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEB6681F0>]}


============================== 20:21:31.936623 | 6683c104-e4dc-4f36-b64f-efbfe9909710 ==============================
[0m20:21:31.936623 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:21:31.936623 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_support_tickets', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:21:35.155826 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:21:35.155826 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:21:35.155826 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:21:38.005231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DE775BE50>]}
[0m20:21:38.259476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEAFB6AD0>]}
[0m20:21:38.259476 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:21:39.781904 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:21:39.783910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEB1809D0>]}
[0m20:21:39.855628 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:21:40.733144 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:21:40.733144 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\stg_support_tickets.sql
[0m20:21:42.105970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DFF880BE0>]}
[0m20:21:42.586846 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:21:42.602790 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:21:42.642618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DFF59E710>]}
[0m20:21:42.644689 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:21:42.644689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DFF583010>]}
[0m20:21:42.650566 [info ] [MainThread]: 
[0m20:21:42.654927 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:21:42.656469 [info ] [MainThread]: 
[0m20:21:42.661997 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:21:42.665974 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:21:42.676226 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:21:42.676226 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:21:42.707387 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:21:42.734743 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:21:42.734743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:21:47.890975 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd9-5dc2-1423-b1bb-b3aa1137ec5a) - Created
[0m20:21:48.377222 [debug] [ThreadPool]: SQL status: OK in 5.640 seconds
[0m20:21:48.538689 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd9-5dc2-1423-b1bb-b3aa1137ec5a, command-id=01f10cd9-5df1-1c70-9779-c64d9e0f14b3) - Closing
[0m20:21:48.547149 [debug] [ThreadPool]: On list_workspace: Close
[0m20:21:48.547149 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd9-5dc2-1423-b1bb-b3aa1137ec5a) - Closing
[0m20:21:48.875346 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:21:48.887246 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:21:48.916480 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:21:48.918489 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:21:48.921271 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:21:53.722213 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd9-613d-18e5-9ab5-f276a5f47876) - Created
[0m20:21:54.651238 [debug] [ThreadPool]: SQL status: OK in 5.730 seconds
[0m20:21:54.660298 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd9-613d-18e5-9ab5-f276a5f47876, command-id=01f10cd9-616c-12b6-b824-f649d77e6ef1) - Closing
[0m20:21:54.662304 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:21:54.664313 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd9-613d-18e5-9ab5-f276a5f47876) - Closing
[0m20:21:55.008298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DFF59F430>]}
[0m20:21:55.016875 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:21:55.021301 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_support_tickets ...................... [RUN]
[0m20:21:55.021301 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:21:55.021301 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:21:55.021301 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:21:55.054419 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:21:55.056229 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:21:55.134682 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:21:55.134682 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:21:55.134682 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEAD53850>]}
[0m20:21:55.195365 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:21:55.219940 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:21:55.219940 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:21:55.227955 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_support_tickets

),

cleaned as (

    select
        ticket_id
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:21:55.227955 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:22:01.507996 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd9-65de-1640-97ed-77fd98d4f66e) - Created
[0m20:22:03.286818 [debug] [Thread-3 (]: SQL status: OK in 8.060 seconds
[0m20:22:03.290572 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10cd9-65de-1640-97ed-77fd98d4f66e, command-id=01f10cd9-660e-1414-9327-3e882a20e0d6) - Closing
[0m20:22:03.321703 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:22:03.321703 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:22:03.337495 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd9-65de-1640-97ed-77fd98d4f66e) - Closing
[0m20:22:03.593370 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DE775AF50>]}
[0m20:22:03.593370 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_support_tickets ................. [[32mOK[0m in 8.57s]
[0m20:22:03.593370 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:22:03.602840 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:22:03.608414 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:22:03.611393 [info ] [MainThread]: 
[0m20:22:03.612033 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 20.95 seconds (20.95s).
[0m20:22:03.615201 [debug] [MainThread]: Command end result
[0m20:22:03.774012 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:22:03.774012 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:22:03.790158 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:22:03.806204 [info ] [MainThread]: 
[0m20:22:03.806204 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:22:03.806204 [info ] [MainThread]: 
[0m20:22:03.814032 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:22:03.818081 [debug] [MainThread]: Command `dbt run` succeeded at 20:22:03.818081 after 32.18 seconds
[0m20:22:03.821188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DE934EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEA6BD390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEB66AF20>]}
[0m20:22:03.821188 [debug] [MainThread]: Flushing usage events
[0m20:22:07.340141 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:22:54.247016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3F9D3ABF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3FB07C370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3FB07C1C0>]}


============================== 20:22:54.263494 | 9123fd91-589d-48f6-96c9-207bcbeea95b ==============================
[0m20:22:54.263494 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:22:54.263494 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt test', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:22:57.807925 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:22:57.809931 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:22:57.809931 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:23:00.563582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3FB07C190>]}
[0m20:23:00.770759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38E3BB730>]}
[0m20:23:00.786765 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:23:02.279430 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:23:02.283444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38EA99CF0>]}
[0m20:23:02.345515 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:23:03.306224 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:23:03.308235 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:23:03.312305 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:23:03.478412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38F106080>]}
[0m20:23:03.875431 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:23:03.881482 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:23:03.945696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38F05D420>]}
[0m20:23:03.945696 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:23:03.945696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38F05CFA0>]}
[0m20:23:03.961753 [info ] [MainThread]: 
[0m20:23:03.977561 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:23:03.977561 [info ] [MainThread]: 
[0m20:23:03.987803 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:23:03.989811 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:23:04.032528 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:23:04.040434 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:23:04.107576 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:23:04.111185 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:23:04.115125 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:23:08.988607 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd9-8e18-134f-9d8e-302140043e55) - Created
[0m20:23:09.713022 [debug] [ThreadPool]: SQL status: OK in 5.600 seconds
[0m20:23:09.713022 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd9-8e18-134f-9d8e-302140043e55, command-id=01f10cd9-8e47-1be3-9d5b-1f52badf4286) - Closing
[0m20:23:09.730060 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:23:09.730060 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd9-8e18-134f-9d8e-302140043e55) - Closing
[0m20:23:10.022601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38F05C190>]}
[0m20:23:10.042336 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:23:10.044343 [info ] [Thread-2 (]: 1 of 19 START test dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [RUN]
[0m20:23:10.046089 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77) - Creating connection
[0m20:23:10.046089 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77'
[0m20:23:10.046089 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:23:10.124270 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:23:10.126279 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:23:10.187650 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:23:10.191667 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:23:10.193676 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  





with validation_errors as (

    select
        subscription_id, usage_date, feature_name
    from `workspace`.`analytics`.`stg_feature_usage`
    group by subscription_id, usage_date, feature_name
    having count(*) > 1

)

select *
from validation_errors



  
  
      
    ) dbt_internal_test
[0m20:23:10.195686 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:14.618703 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-9175-14e8-96e5-7dc424120d0c) - Created
[0m20:23:15.143579 [debug] [Thread-2 (]: SQL status: OK in 4.950 seconds
[0m20:23:15.155888 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-9175-14e8-96e5-7dc424120d0c, command-id=01f10cd9-91a2-1fed-bc37-70a9cf608087) - Closing
[0m20:23:15.167431 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: Close
[0m20:23:15.169438 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-9175-14e8-96e5-7dc424120d0c) - Closing
[0m20:23:15.435535 [info ] [Thread-2 (]: 1 of 19 PASS dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [[32mPASS[0m in 5.39s]
[0m20:23:15.446155 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:23:15.451530 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:23:15.456922 [info ] [Thread-2 (]: 2 of 19 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m20:23:15.462794 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m20:23:15.464804 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108'
[0m20:23:15.464804 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:23:15.509581 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:23:15.511712 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:23:15.523750 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:23:15.526826 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:23:15.526826 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:15.526826 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:20.458947 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-94ed-1ba9-8d73-1c4af1bac831) - Created
[0m20:23:20.969875 [debug] [Thread-2 (]: SQL status: OK in 5.440 seconds
[0m20:23:20.993359 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-94ed-1ba9-8d73-1c4af1bac831, command-id=01f10cd9-951d-1f43-b9f0-5585b1d03507) - Closing
[0m20:23:20.997376 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: Close
[0m20:23:20.999384 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-94ed-1ba9-8d73-1c4af1bac831) - Closing
[0m20:23:21.297180 [info ] [Thread-2 (]: 2 of 19 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 5.84s]
[0m20:23:21.299189 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:23:21.299189 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:23:21.299189 [info ] [Thread-2 (]: 3 of 19 START test not_null_stg_accounts_seats ................................. [RUN]
[0m20:23:21.312727 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39) - Creating connection
[0m20:23:21.316464 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39'
[0m20:23:21.316464 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:23:21.361806 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:23:21.361806 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:23:21.382648 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:23:21.382648 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:23:21.382648 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select seats
from `workspace`.`analytics`.`stg_accounts`
where seats is null



  
  
      
    ) dbt_internal_test
[0m20:23:21.392351 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:25.782703 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-981b-19e0-94cd-8b4c311c1eb9) - Created
[0m20:23:26.305213 [debug] [Thread-2 (]: SQL status: OK in 4.910 seconds
[0m20:23:26.314999 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-981b-19e0-94cd-8b4c311c1eb9, command-id=01f10cd9-984b-14d3-a702-d5535dd52a61) - Closing
[0m20:23:26.321024 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: Close
[0m20:23:26.324780 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-981b-19e0-94cd-8b4c311c1eb9) - Closing
[0m20:23:26.605454 [info ] [Thread-2 (]: 3 of 19 PASS not_null_stg_accounts_seats ....................................... [[32mPASS[0m in 5.31s]
[0m20:23:26.621372 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:23:26.621372 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:23:26.621372 [info ] [Thread-2 (]: 4 of 19 START test not_null_stg_accounts_signup_date ........................... [RUN]
[0m20:23:26.633240 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a) - Creating connection
[0m20:23:26.638430 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a'
[0m20:23:26.641578 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:23:26.660940 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:23:26.660940 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:23:26.676192 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:23:26.691507 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:23:26.694310 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signup_date
from `workspace`.`analytics`.`stg_accounts`
where signup_date is null



  
  
      
    ) dbt_internal_test
[0m20:23:26.694310 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:31.314001 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-9b66-13a9-8e07-a972e2c8c1fa) - Created
[0m20:23:32.037282 [debug] [Thread-2 (]: SQL status: OK in 5.340 seconds
[0m20:23:32.045310 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-9b66-13a9-8e07-a972e2c8c1fa, command-id=01f10cd9-9b95-110e-b702-71000ab46432) - Closing
[0m20:23:32.049064 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: Close
[0m20:23:32.051074 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-9b66-13a9-8e07-a972e2c8c1fa) - Closing
[0m20:23:32.344507 [info ] [Thread-2 (]: 4 of 19 PASS not_null_stg_accounts_signup_date ................................. [[32mPASS[0m in 5.72s]
[0m20:23:32.344507 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:23:32.344507 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:23:32.344507 [info ] [Thread-2 (]: 5 of 19 START test not_null_stg_churn_events_account_id ........................ [RUN]
[0m20:23:32.360801 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d) - Creating connection
[0m20:23:32.364625 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d'
[0m20:23:32.370660 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:23:32.393203 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:23:32.398910 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:23:32.408708 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:23:32.408708 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:23:32.408708 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_churn_events`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:32.408708 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:37.149662 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-9ee2-193f-ae3b-e863c8bf8209) - Created
[0m20:23:37.663910 [debug] [Thread-2 (]: SQL status: OK in 5.260 seconds
[0m20:23:37.671439 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-9ee2-193f-ae3b-e863c8bf8209, command-id=01f10cd9-9f11-1548-a993-9c63538b04ec) - Closing
[0m20:23:37.675459 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: Close
[0m20:23:37.675459 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-9ee2-193f-ae3b-e863c8bf8209) - Closing
[0m20:23:37.968279 [info ] [Thread-2 (]: 5 of 19 PASS not_null_stg_churn_events_account_id .............................. [[32mPASS[0m in 5.62s]
[0m20:23:37.968279 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:23:37.968279 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:23:37.968279 [info ] [Thread-2 (]: 6 of 19 START test not_null_stg_churn_events_churn_event_id .................... [RUN]
[0m20:23:37.981300 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc) - Creating connection
[0m20:23:37.984627 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc'
[0m20:23:37.985760 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:23:38.009435 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:23:38.020466 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:23:38.034271 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:23:38.038023 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:23:38.042093 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select churn_event_id
from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:38.042093 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:42.579223 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-a222-1290-b07f-46702b93ff36) - Created
[0m20:23:43.083423 [debug] [Thread-2 (]: SQL status: OK in 5.040 seconds
[0m20:23:43.099387 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-a222-1290-b07f-46702b93ff36, command-id=01f10cd9-a24e-159a-b0e1-44f2620ca6a3) - Closing
[0m20:23:43.099387 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: Close
[0m20:23:43.099387 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-a222-1290-b07f-46702b93ff36) - Closing
[0m20:23:43.413183 [info ] [Thread-2 (]: 6 of 19 PASS not_null_stg_churn_events_churn_event_id .......................... [[32mPASS[0m in 5.44s]
[0m20:23:43.418274 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:23:43.418274 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:23:43.418274 [info ] [Thread-2 (]: 7 of 19 START test not_null_stg_feature_usage_subscription_id .................. [RUN]
[0m20:23:43.429183 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3) - Creating connection
[0m20:23:43.433531 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3'
[0m20:23:43.439307 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:23:43.468964 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:23:43.472334 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:23:43.484720 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:23:43.484720 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:23:43.489025 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_feature_usage`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:43.489025 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:47.709360 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-a536-1068-ba36-094f9c4a81ab) - Created
[0m20:23:48.218827 [debug] [Thread-2 (]: SQL status: OK in 4.720 seconds
[0m20:23:48.226653 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-a536-1068-ba36-094f9c4a81ab, command-id=01f10cd9-a565-1bdf-8755-e5c60abb22f4) - Closing
[0m20:23:48.229678 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: Close
[0m20:23:48.233699 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-a536-1068-ba36-094f9c4a81ab) - Closing
[0m20:23:48.512027 [info ] [Thread-2 (]: 7 of 19 PASS not_null_stg_feature_usage_subscription_id ........................ [[32mPASS[0m in 5.08s]
[0m20:23:48.523167 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:23:48.527192 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:23:48.531903 [info ] [Thread-2 (]: 8 of 19 START test not_null_stg_subscriptions_account_id ....................... [RUN]
[0m20:23:48.549455 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9) - Creating connection
[0m20:23:48.551196 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9'
[0m20:23:48.557426 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:23:48.581524 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:23:48.585665 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:23:48.594727 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:23:48.594727 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:23:48.602733 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_subscriptions`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:48.605114 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:52.862270 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-a849-1346-bb6a-0818aa844882) - Created
[0m20:23:53.440573 [debug] [Thread-2 (]: SQL status: OK in 4.840 seconds
[0m20:23:53.452639 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-a849-1346-bb6a-0818aa844882, command-id=01f10cd9-a86f-1249-a7d5-17c6fe9c3a95) - Closing
[0m20:23:53.458666 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: Close
[0m20:23:53.462421 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-a849-1346-bb6a-0818aa844882) - Closing
[0m20:23:53.753269 [info ] [Thread-2 (]: 8 of 19 PASS not_null_stg_subscriptions_account_id ............................. [[32mPASS[0m in 5.21s]
[0m20:23:53.758011 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:23:53.758011 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:23:53.758011 [info ] [Thread-2 (]: 9 of 19 START test not_null_stg_subscriptions_subscription_id .................. [RUN]
[0m20:23:53.769064 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91) - Creating connection
[0m20:23:53.774664 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m20:23:53.776674 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:23:53.820573 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:23:53.822289 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:23:53.844660 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:23:53.847193 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:23:53.849541 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:53.849541 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:58.128505 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-ab66-1617-b756-152aa5a40ac6) - Created
[0m20:23:58.757162 [debug] [Thread-2 (]: SQL status: OK in 4.910 seconds
[0m20:23:58.766877 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-ab66-1617-b756-152aa5a40ac6, command-id=01f10cd9-ab94-1083-8620-3b6f5bac7d0d) - Closing
[0m20:23:58.766877 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m20:23:58.766877 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-ab66-1617-b756-152aa5a40ac6) - Closing
[0m20:23:59.068497 [info ] [Thread-2 (]: 9 of 19 PASS not_null_stg_subscriptions_subscription_id ........................ [[32mPASS[0m in 5.30s]
[0m20:23:59.073544 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:23:59.073544 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:23:59.077781 [info ] [Thread-2 (]: 10 of 19 START test not_null_stg_support_tickets_account_id .................... [RUN]
[0m20:23:59.083882 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:23:59.088129 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:23:59.088129 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:23:59.106040 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:23:59.115969 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:23:59.120743 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:23:59.131638 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:23:59.131638 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:59.137075 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:03.673209 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-aeb1-195b-a9b5-02ec5465cda6) - Created
[0m20:24:04.911050 [debug] [Thread-2 (]: SQL status: OK in 5.770 seconds
[0m20:24:04.921009 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-aeb1-195b-a9b5-02ec5465cda6, command-id=01f10cd9-aee1-10b5-87da-505ad60351e5) - Closing
[0m20:24:04.924766 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:24:04.924766 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-aeb1-195b-a9b5-02ec5465cda6) - Closing
[0m20:24:05.223699 [info ] [Thread-2 (]: 10 of 19 PASS not_null_stg_support_tickets_account_id .......................... [[32mPASS[0m in 6.14s]
[0m20:24:05.226713 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:24:05.228721 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:24:05.228721 [info ] [Thread-2 (]: 11 of 19 START test not_null_stg_support_tickets_ticket_id ..................... [RUN]
[0m20:24:05.235370 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:24:05.237380 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:24:05.239448 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:24:05.276118 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:24:05.286971 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:24:05.286971 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:24:05.302729 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:24:05.302729 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:24:05.302729 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:10.532566 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-b24b-1e42-8a2e-fa053a5c416d) - Created
[0m20:24:11.046605 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd9-b2f7-1a52-bc34-75b721befea9
[0m20:24:11.050625 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:24:11.052633 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-b24b-1e42-8a2e-fa053a5c416d) - Closing
[0m20:24:11.378291 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:24:11.378291 [error] [Thread-2 (]: 11 of 19 ERROR not_null_stg_support_tickets_ticket_id .......................... [[31mERROR[0m in 6.15s]
[0m20:24:11.378291 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:24:11.378291 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:24:11.378291 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql.
[0m20:24:11.378291 [info ] [Thread-2 (]: 12 of 19 START test relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:24:11.398894 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b) - Creating connection
[0m20:24:11.398894 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b'
[0m20:24:11.403095 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:24:11.431838 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:24:11.434852 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:24:11.440154 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:24:11.451790 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:24:11.453182 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_churn_events`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:24:11.455776 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:15.960254 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-b605-10c5-b077-4c3ce953d76f) - Created
[0m20:24:16.480739 [debug] [Thread-2 (]: SQL status: OK in 5.020 seconds
[0m20:24:16.492805 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-b605-10c5-b077-4c3ce953d76f, command-id=01f10cd9-b635-10cf-ae52-6bd986d5c05c) - Closing
[0m20:24:16.498574 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: Close
[0m20:24:16.502098 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-b605-10c5-b077-4c3ce953d76f) - Closing
[0m20:24:16.882318 [info ] [Thread-2 (]: 12 of 19 PASS relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 5.48s]
[0m20:24:16.889231 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:24:16.893255 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:24:16.895267 [info ] [Thread-2 (]: 13 of 19 START test relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [RUN]
[0m20:24:16.901922 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850) - Creating connection
[0m20:24:16.906005 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850'
[0m20:24:16.909779 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:24:16.958318 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:24:16.968168 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:24:16.974886 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:24:16.983082 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:24:16.987130 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select subscription_id as from_field
    from `workspace`.`analytics`.`stg_feature_usage`
    where subscription_id is not null
),

parent as (
    select subscription_id as to_field
    from `workspace`.`analytics`.`stg_subscriptions`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:24:16.987130 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:21.683180 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-b96e-1c0b-9f3d-c601749ea503) - Created
[0m20:24:22.212663 [debug] [Thread-2 (]: SQL status: OK in 5.230 seconds
[0m20:24:22.228364 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-b96e-1c0b-9f3d-c601749ea503, command-id=01f10cd9-b99e-124b-bf94-95bfabc13cc0) - Closing
[0m20:24:22.234392 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: Close
[0m20:24:22.238412 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-b96e-1c0b-9f3d-c601749ea503) - Closing
[0m20:24:22.517980 [info ] [Thread-2 (]: 13 of 19 PASS relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [[32mPASS[0m in 5.62s]
[0m20:24:22.517980 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:24:22.517980 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:24:22.532884 [info ] [Thread-2 (]: 14 of 19 START test relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:24:22.533825 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90) - Creating connection
[0m20:24:22.540926 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90'
[0m20:24:22.540926 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:24:22.571586 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:24:22.571586 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:24:22.586217 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:24:22.586217 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:24:22.597011 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_subscriptions`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:24:22.599019 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:27.431432 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-bcda-1d83-bc77-0049c95a3e1f) - Created
[0m20:24:27.945048 [debug] [Thread-2 (]: SQL status: OK in 5.350 seconds
[0m20:24:27.951801 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-bcda-1d83-bc77-0049c95a3e1f, command-id=01f10cd9-bd09-1e5b-a9e4-7cd4edc156a1) - Closing
[0m20:24:27.955819 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: Close
[0m20:24:27.957827 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-bcda-1d83-bc77-0049c95a3e1f) - Closing
[0m20:24:28.267783 [info ] [Thread-2 (]: 14 of 19 PASS relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 5.73s]
[0m20:24:28.267783 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:24:28.279847 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:24:28.279847 [info ] [Thread-2 (]: 15 of 19 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:24:28.285301 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:24:28.288243 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:24:28.290250 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:24:28.319984 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:24:28.332862 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:24:28.335090 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:24:28.347910 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:24:28.350789 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:24:28.350789 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:33.060973 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-c036-192a-94e4-082da57a7ebe) - Created
[0m20:24:34.822690 [debug] [Thread-2 (]: SQL status: OK in 6.470 seconds
[0m20:24:34.828724 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-c036-192a-94e4-082da57a7ebe, command-id=01f10cd9-c065-18f7-acbf-8449f50d6aec) - Closing
[0m20:24:34.831855 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:24:34.833402 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-c036-192a-94e4-082da57a7ebe) - Closing
[0m20:24:35.213188 [error] [Thread-2 (]: 15 of 19 FAIL 2000 relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[31mFAIL 2000[0m in 6.93s]
[0m20:24:35.215807 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:24:35.215807 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:24:35.215807 [info ] [Thread-2 (]: 16 of 19 START test unique_stg_accounts_account_id ............................. [RUN]
[0m20:24:35.224075 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m20:24:35.224075 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79'
[0m20:24:35.227723 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:24:35.252449 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:24:35.261411 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:24:35.271761 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:24:35.279109 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:24:35.283441 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:24:35.283441 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:39.716922 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-c42d-162c-8659-1bede9ee2e2b) - Created
[0m20:24:40.233871 [debug] [Thread-2 (]: SQL status: OK in 4.950 seconds
[0m20:24:40.235876 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-c42d-162c-8659-1bede9ee2e2b, command-id=01f10cd9-c45b-1f54-9abc-dfef111d2a79) - Closing
[0m20:24:40.241253 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: Close
[0m20:24:40.241253 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-c42d-162c-8659-1bede9ee2e2b) - Closing
[0m20:24:40.641298 [info ] [Thread-2 (]: 16 of 19 PASS unique_stg_accounts_account_id ................................... [[32mPASS[0m in 5.42s]
[0m20:24:40.643307 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:24:40.643307 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:24:40.643307 [info ] [Thread-2 (]: 17 of 19 START test unique_stg_churn_events_churn_event_id ..................... [RUN]
[0m20:24:40.652632 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947) - Creating connection
[0m20:24:40.654517 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947'
[0m20:24:40.658302 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:24:40.701767 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:24:40.706844 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:24:40.720456 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:24:40.724037 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:24:40.724037 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    churn_event_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is not null
group by churn_event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:24:40.728602 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:45.249648 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-c778-1c5b-9427-c883e49ab687) - Created
[0m20:24:45.712103 [debug] [Thread-2 (]: SQL status: OK in 4.980 seconds
[0m20:24:45.716580 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-c778-1c5b-9427-c883e49ab687, command-id=01f10cd9-c7a8-1a3c-817b-52f0c0083407) - Closing
[0m20:24:45.722136 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: Close
[0m20:24:45.722136 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-c778-1c5b-9427-c883e49ab687) - Closing
[0m20:24:46.639513 [info ] [Thread-2 (]: 17 of 19 PASS unique_stg_churn_events_churn_event_id ........................... [[32mPASS[0m in 6.00s]
[0m20:24:46.649844 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:24:46.651853 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:24:46.655877 [info ] [Thread-2 (]: 18 of 19 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m20:24:46.661913 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e) - Creating connection
[0m20:24:46.666036 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m20:24:46.673342 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:24:46.688362 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:24:46.696994 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:24:46.704092 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:24:46.713605 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:24:46.715012 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:24:46.717777 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:51.187039 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-cb04-16cb-aece-184d20c53358) - Created
[0m20:24:51.714840 [debug] [Thread-2 (]: SQL status: OK in 5.000 seconds
[0m20:24:51.725248 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-cb04-16cb-aece-184d20c53358, command-id=01f10cd9-cb33-14e0-89ea-51a8ec237de1) - Closing
[0m20:24:51.729265 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m20:24:51.733288 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-cb04-16cb-aece-184d20c53358) - Closing
[0m20:24:52.004606 [info ] [Thread-2 (]: 18 of 19 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 5.34s]
[0m20:24:52.012912 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:24:52.014743 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:24:52.014743 [info ] [Thread-2 (]: 19 of 19 START test unique_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:24:52.024302 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:24:52.024302 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:24:52.024302 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:24:52.216913 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:24:52.216913 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:24:52.229618 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:24:52.229618 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:24:52.236686 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:24:52.236686 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:56.827992 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-ce5e-1c24-a88b-5a1b781ed90b) - Created
[0m20:24:57.334547 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd9-ce8f-1174-9ecf-3f1f40c5a038
[0m20:24:57.338561 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:24:57.340569 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-ce5e-1c24-a88b-5a1b781ed90b) - Closing
[0m20:24:57.642119 [debug] [Thread-2 (]: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:24:57.645232 [error] [Thread-2 (]: 19 of 19 ERROR unique_stg_support_tickets_ticket_id ............................ [[31mERROR[0m in 5.62s]
[0m20:24:57.645232 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:24:57.645232 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a' to be skipped because of status 'error'.  Reason: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql.
[0m20:24:57.654655 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:24:57.658043 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:24:57.659245 [info ] [MainThread]: 
[0m20:24:57.663742 [info ] [MainThread]: Finished running 19 data tests in 0 hours 1 minutes and 53.67 seconds (113.67s).
[0m20:24:57.675536 [debug] [MainThread]: Command end result
[0m20:24:57.818871 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:24:57.827082 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:24:57.850744 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:24:57.869474 [info ] [MainThread]: 
[0m20:24:57.869474 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m20:24:57.869474 [info ] [MainThread]: 
[0m20:24:57.869474 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:24:57.880632 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:24:57.885374 [info ] [MainThread]: 
[0m20:24:57.887490 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:24:57.889000 [info ] [MainThread]: 
[0m20:24:57.892153 [error] [MainThread]: [31mFailure in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)[0m
[0m20:24:57.894162 [error] [MainThread]:   Got 2000 results, configured to fail if != 0
[0m20:24:57.898091 [info ] [MainThread]: 
[0m20:24:57.899460 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:24:57.899460 [info ] [MainThread]: 
[0m20:24:57.903957 [error] [MainThread]: [31mFailure in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:24:57.907640 [error] [MainThread]:   Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:24:57.909856 [info ] [MainThread]: 
[0m20:24:57.911863 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:24:57.914741 [info ] [MainThread]: 
[0m20:24:57.915901 [info ] [MainThread]: Done. PASS=16 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=19
[0m20:24:57.915901 [debug] [MainThread]: Command `dbt test` failed at 20:24:57.915901 after 123.99 seconds
[0m20:24:57.922251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3F9D3ABF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3CF57AE60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3CF57A470>]}
[0m20:24:57.922251 [debug] [MainThread]: Flushing usage events
[0m20:25:00.197009 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:29:03.341970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215753AEC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215766F83A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215766F81F0>]}


============================== 20:29:03.345178 | 7913153b-d8c7-40e0-a51f-c7512868ac90 ==============================
[0m20:29:03.345178 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:29:03.354763 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_support_tickets', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:29:06.992823 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:29:06.995027 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:29:06.997306 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:29:09.979862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021578F8FDC0>]}
[0m20:29:10.237021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021509F93D60>]}
[0m20:29:10.240674 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:29:11.868751 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:29:11.870760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002150A15E2C0>]}
[0m20:29:11.928657 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:29:12.843214 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:29:12.843214 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:29:12.843214 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:29:13.011432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002150A7C9F30>]}
[0m20:29:13.392553 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:29:13.400409 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:29:13.445166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002150A7A7A30>]}
[0m20:29:13.447173 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:29:13.447173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002150A8445E0>]}
[0m20:29:13.451136 [info ] [MainThread]: 
[0m20:29:13.451136 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:29:13.451136 [info ] [MainThread]: 
[0m20:29:13.459382 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:29:13.461341 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:29:13.466997 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:29:13.466997 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:29:13.510847 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:29:13.510847 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:29:13.510847 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:29:18.343548 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-6a36-14b3-80e6-92c3eecf1a37) - Created
[0m20:29:19.239114 [debug] [ThreadPool]: SQL status: OK in 5.730 seconds
[0m20:29:19.269628 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cda-6a36-14b3-80e6-92c3eecf1a37, command-id=01f10cda-6a9f-1de9-a9cd-b4bd17961c16) - Closing
[0m20:29:19.270382 [debug] [ThreadPool]: On list_workspace: Close
[0m20:29:19.277361 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-6a36-14b3-80e6-92c3eecf1a37) - Closing
[0m20:29:19.742568 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:29:19.749911 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:29:19.778101 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:29:19.778101 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:29:19.778101 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:29:25.619522 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-6e97-19fd-b0eb-f6128ac2c8fe) - Created
[0m20:29:27.279433 [debug] [ThreadPool]: SQL status: OK in 7.500 seconds
[0m20:29:27.291821 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cda-6e97-19fd-b0eb-f6128ac2c8fe, command-id=01f10cda-6ec7-1019-a088-b001e52fff16) - Closing
[0m20:29:27.295459 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:29:27.300287 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-6e97-19fd-b0eb-f6128ac2c8fe) - Closing
[0m20:29:27.594967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002150A7BAC50>]}
[0m20:29:27.616892 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:29:27.636415 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_support_tickets ...................... [RUN]
[0m20:29:27.636415 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:29:27.643021 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:29:27.645027 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:29:27.692131 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:29:27.694226 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:29:27.758982 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:29:27.771565 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:29:27.774617 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002150A963100>]}
[0m20:29:27.820288 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:29:27.847563 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:29:27.851577 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:29:27.853585 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_support_tickets

),

cleaned as (

    select
        ticket_id
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:29:27.853585 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:29:33.203591 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cda-72b0-1880-a5e9-ff6756874ddd) - Created
[0m20:29:34.741850 [debug] [Thread-3 (]: SQL status: OK in 6.890 seconds
[0m20:29:34.747656 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10cda-72b0-1880-a5e9-ff6756874ddd, command-id=01f10cda-738b-171e-abe7-0431ec1fb563) - Closing
[0m20:29:34.784687 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:29:34.793070 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:29:34.796797 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cda-72b0-1880-a5e9-ff6756874ddd) - Closing
[0m20:29:35.086734 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215727E6F80>]}
[0m20:29:35.092347 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_support_tickets ................. [[32mOK[0m in 7.44s]
[0m20:29:35.094355 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:29:35.098373 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:29:35.101543 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:29:35.105559 [info ] [MainThread]: 
[0m20:29:35.108746 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 21.64 seconds (21.64s).
[0m20:29:35.115913 [debug] [MainThread]: Command end result
[0m20:29:35.284181 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:29:35.291071 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:29:35.320742 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:29:35.323604 [info ] [MainThread]: 
[0m20:29:35.325482 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:29:35.327554 [info ] [MainThread]: 
[0m20:29:35.329562 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:29:35.333811 [debug] [MainThread]: Command `dbt run` succeeded at 20:29:35.333811 after 32.29 seconds
[0m20:29:35.337829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215753AEC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215756F0CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021509F93D60>]}
[0m20:29:35.339929 [debug] [MainThread]: Flushing usage events
[0m20:29:37.705913 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:30:09.864476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002091069EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209129B03A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209129B01F0>]}


============================== 20:30:09.864476 | 4e62b041-003d-441f-8d15-f8a01a44fac9 ==============================
[0m20:30:09.864476 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:30:09.880671 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt test --select stg_support_tickets', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:30:13.057014 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:30:13.057014 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:30:13.057014 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:30:15.763236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209129B0460>]}
[0m20:30:15.957872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002091523FEB0>]}
[0m20:30:15.957872 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:30:17.401677 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:30:17.401677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209124C9DB0>]}
[0m20:30:17.458735 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:30:18.237397 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:30:18.237397 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:30:18.237397 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:30:18.425415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002092677A290>]}
[0m20:30:18.769319 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:30:18.777558 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:30:18.837739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209266C3730>]}
[0m20:30:18.839746 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:30:18.842656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209266C3C70>]}
[0m20:30:18.842656 [info ] [MainThread]: 
[0m20:30:18.850869 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:30:18.853526 [info ] [MainThread]: 
[0m20:30:18.854322 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:30:18.854322 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:30:18.887744 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:30:18.887744 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:30:18.935902 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:30:18.937908 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:30:18.940922 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:30:23.380796 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-9104-1ea5-8a6d-d0434dfadd63) - Created
[0m20:30:24.114491 [debug] [ThreadPool]: SQL status: OK in 5.170 seconds
[0m20:30:24.117747 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cda-9104-1ea5-8a6d-d0434dfadd63, command-id=01f10cda-9133-1af7-a218-cc9376201dbd) - Closing
[0m20:30:24.117747 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:30:24.117747 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-9104-1ea5-8a6d-d0434dfadd63) - Closing
[0m20:30:24.398072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209266C3160>]}
[0m20:30:24.413827 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:30:24.425741 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_support_tickets_account_id ...................... [RUN]
[0m20:30:24.429723 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:30:24.429723 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:30:24.429723 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:30:24.538424 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:30:24.540435 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:30:24.603528 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:30:24.619271 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:30:24.619271 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:30:24.619271 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:30:28.911580 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-944f-1742-8475-feb4ae657f9b) - Created
[0m20:30:29.526136 [debug] [Thread-2 (]: SQL status: OK in 4.910 seconds
[0m20:30:29.534553 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cda-944f-1742-8475-feb4ae657f9b, command-id=01f10cda-947e-1ee3-99a9-88cdb5ee9150) - Closing
[0m20:30:29.544588 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:30:29.546595 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-944f-1742-8475-feb4ae657f9b) - Closing
[0m20:30:29.832189 [info ] [Thread-2 (]: 1 of 4 PASS not_null_stg_support_tickets_account_id ............................ [[32mPASS[0m in 5.39s]
[0m20:30:29.832189 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:30:29.840212 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:30:29.842220 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:30:29.849102 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:30:29.851471 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:30:29.855529 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:30:29.882986 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:30:29.882986 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:30:29.899718 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:30:29.899718 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:30:29.899718 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:30:29.913963 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:30:35.258473 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-9819-14a2-b6ef-a3b98053bbf1) - Created
[0m20:30:36.080007 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cda-9847-1eab-bc5e-150d3683f892
[0m20:30:36.084021 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:30:36.086028 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-9819-14a2-b6ef-a3b98053bbf1) - Closing
[0m20:30:37.026961 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:30:37.028969 [error] [Thread-2 (]: 2 of 4 ERROR not_null_stg_support_tickets_ticket_id ............................ [[31mERROR[0m in 7.18s]
[0m20:30:37.034992 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:30:37.039007 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:30:37.040752 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql.
[0m20:30:37.042763 [info ] [Thread-2 (]: 3 of 4 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:30:37.057359 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:30:37.059371 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:30:37.061381 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:30:37.114784 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:30:37.119261 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:30:37.131665 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:30:37.135666 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:30:37.135666 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:30:37.139388 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:30:41.708728 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-9bf1-1dc7-ae40-a4a5adfe7409) - Created
[0m20:30:42.247906 [debug] [Thread-2 (]: SQL status: OK in 5.110 seconds
[0m20:30:42.247906 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cda-9bf1-1dc7-ae40-a4a5adfe7409, command-id=01f10cda-9c29-1a28-905a-f474737458e4) - Closing
[0m20:30:42.247906 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:30:42.247906 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-9bf1-1dc7-ae40-a4a5adfe7409) - Closing
[0m20:30:42.532766 [error] [Thread-2 (]: 3 of 4 FAIL 2000 relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[31mFAIL 2000[0m in 5.48s]
[0m20:30:42.537604 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:30:42.541628 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:30:42.543639 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_support_tickets_ticket_id ......................... [RUN]
[0m20:30:42.549082 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:30:42.554606 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:30:42.554606 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:30:42.614182 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:30:42.614182 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:30:42.636891 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:30:42.636891 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:30:42.645146 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:30:42.645146 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:30:50.006064 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-a0e3-1481-a21c-045946788aa2) - Created
[0m20:30:51.050304 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cda-a112-1f66-9b1e-b7f9fa2c2ec0
[0m20:30:51.054323 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:30:51.058342 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-a0e3-1481-a21c-045946788aa2) - Closing
[0m20:30:51.439003 [debug] [Thread-2 (]: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:30:51.439003 [error] [Thread-2 (]: 4 of 4 ERROR unique_stg_support_tickets_ticket_id .............................. [[31mERROR[0m in 8.89s]
[0m20:30:51.439003 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:30:51.454802 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a' to be skipped because of status 'error'.  Reason: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql.
[0m20:30:51.454802 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:30:51.465922 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:30:51.467931 [info ] [MainThread]: 
[0m20:30:51.472239 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 32.61 seconds (32.61s).
[0m20:30:51.485956 [debug] [MainThread]: Command end result
[0m20:30:51.635022 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:30:51.648313 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:30:51.664440 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:30:51.680119 [info ] [MainThread]: 
[0m20:30:51.680119 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m20:30:51.680119 [info ] [MainThread]: 
[0m20:30:51.680119 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:30:51.689793 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:30:51.692439 [info ] [MainThread]: 
[0m20:30:51.696211 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:30:51.701107 [info ] [MainThread]: 
[0m20:30:51.705508 [error] [MainThread]: [31mFailure in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)[0m
[0m20:30:51.705508 [error] [MainThread]:   Got 2000 results, configured to fail if != 0
[0m20:30:51.712932 [info ] [MainThread]: 
[0m20:30:51.712932 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:30:51.721767 [info ] [MainThread]: 
[0m20:30:51.721767 [error] [MainThread]: [31mFailure in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:30:51.728913 [error] [MainThread]:   Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:30:51.729415 [info ] [MainThread]: 
[0m20:30:51.734970 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:30:51.734970 [info ] [MainThread]: 
[0m20:30:51.741306 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=4
[0m20:30:51.750829 [debug] [MainThread]: Command `dbt test` failed at 20:30:51.750829 after 42.16 seconds
[0m20:30:51.753029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002091069EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020911A0AA40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020926980F40>]}
[0m20:30:51.757593 [debug] [MainThread]: Flushing usage events
[0m20:30:55.736668 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:33:04.162744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6CE4EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6F160370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6F1601C0>]}


============================== 20:33:04.171306 | 272b1b18-a860-4285-a52e-87380a6596b6 ==============================
[0m20:33:04.171306 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:33:04.171306 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select stg_support_tickets', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:33:06.371164 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:33:06.371164 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:33:06.373170 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:33:08.251165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B716BB5B0>]}
[0m20:33:08.388232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B71B562F0>]}
[0m20:33:08.388232 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:33:09.339443 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:33:09.341539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B02B8E7D0>]}
[0m20:33:09.377941 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:33:09.956091 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:33:09.963127 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\stg_support_tickets.sql
[0m20:33:10.918330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B03638F40>]}
[0m20:33:11.389582 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:33:11.401133 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:33:11.429556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B0338A740>]}
[0m20:33:11.431564 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:33:11.431564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B03362680>]}
[0m20:33:11.434700 [info ] [MainThread]: 
[0m20:33:11.437137 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:33:11.439426 [info ] [MainThread]: 
[0m20:33:11.440796 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:33:11.441804 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:33:11.445048 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:33:11.447992 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:33:11.477500 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:33:11.480068 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:33:11.480068 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:33:14.630649 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-f71e-12cd-9407-3f2f8e59b9a7) - Created
[0m20:33:15.207827 [debug] [ThreadPool]: SQL status: OK in 3.730 seconds
[0m20:33:15.207827 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cda-f71e-12cd-9407-3f2f8e59b9a7, command-id=01f10cda-f748-14f2-8d5c-b623b4ba6746) - Closing
[0m20:33:15.207827 [debug] [ThreadPool]: On list_workspace: Close
[0m20:33:15.207827 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-f71e-12cd-9407-3f2f8e59b9a7) - Closing
[0m20:33:15.524147 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:33:15.524147 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:33:15.539907 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:33:15.539907 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:33:15.539907 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:33:18.805893 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-f99b-17d4-8b72-1b62dc5281b8) - Created
[0m20:33:19.488251 [debug] [ThreadPool]: SQL status: OK in 3.950 seconds
[0m20:33:19.488251 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cda-f99b-17d4-8b72-1b62dc5281b8, command-id=01f10cda-f9c6-1ba3-adcc-dfcd1cb8d5db) - Closing
[0m20:33:19.488251 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:33:19.488251 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-f99b-17d4-8b72-1b62dc5281b8) - Closing
[0m20:33:19.788790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B0338AC80>]}
[0m20:33:19.804525 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:19.804525 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_support_tickets ...................... [RUN]
[0m20:33:19.804525 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:33:19.804525 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:33:19.804525 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:19.826783 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:33:19.826783 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:19.866968 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:33:19.870971 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:33:19.872992 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6E857850>]}
[0m20:33:19.909725 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:33:19.928054 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:33:19.930059 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:33:19.932083 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    select
    ticket_id,
    account_id,
    submitted_at,
    closed_at,
    resolution_time_hours,
    priority,
    first_response_time_minutes,
    satisfaction_score,
    escalation_flag,
    ingestion_ts
from workspace.default.bronze_support_ticket
where ticket_id is not null
  and account_id is not null
  )

[0m20:33:19.933090 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:33:22.972111 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cda-fc18-1cae-83aa-e5291314d074) - Created
[0m20:33:23.588359 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    select
    ticket_id,
    account_id,
    submitted_at,
    closed_at,
    resolution_time_hours,
    priority,
    first_response_time_minutes,
    satisfaction_score,
    escalation_flag,
    ingestion_ts
from workspace.default.bronze_support_ticket
where ticket_id is not null
  and account_id is not null
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`default`.`bronze_support_ticket` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 19 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`default`.`bronze_support_ticket` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 19 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`default`.`bronze_support_ticket` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 19 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cda-fc40-1940-857f-a6a312df990d
[0m20:33:23.588359 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:33:23.588359 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cda-fc18-1cae-83aa-e5291314d074) - Closing
[0m20:33:23.881926 [debug] [Thread-3 (]: Database Error in model stg_support_tickets (models\silver\stg_support_tickets.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`default`.`bronze_support_ticket` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 19 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\stg_support_tickets.sql
[0m20:33:23.881926 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6CD011B0>]}
[0m20:33:23.881926 [error] [Thread-3 (]: 1 of 1 ERROR creating sql view model analytics.stg_support_tickets ............. [[31mERROR[0m in 4.08s]
[0m20:33:23.881926 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:23.881926 [debug] [Thread-6 (]: Marking all children of 'model.saas_dbt_analytics.stg_support_tickets' to be skipped because of status 'error'.  Reason: Database Error in model stg_support_tickets (models\silver\stg_support_tickets.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`default`.`bronze_support_ticket` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 19 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\stg_support_tickets.sql.
[0m20:33:23.881926 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:33:23.898008 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:33:23.900385 [info ] [MainThread]: 
[0m20:33:23.901885 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 12.46 seconds (12.46s).
[0m20:33:23.904198 [debug] [MainThread]: Command end result
[0m20:33:24.000711 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:33:24.008472 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:33:24.009957 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:33:24.025698 [info ] [MainThread]: 
[0m20:33:24.025698 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m20:33:24.025698 [info ] [MainThread]: 
[0m20:33:24.025698 [error] [MainThread]: [31mFailure in model stg_support_tickets (models\silver\stg_support_tickets.sql)[0m
[0m20:33:24.025698 [error] [MainThread]:   Database Error in model stg_support_tickets (models\silver\stg_support_tickets.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`default`.`bronze_support_ticket` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 19 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\stg_support_tickets.sql
[0m20:33:24.025698 [info ] [MainThread]: 
[0m20:33:24.035471 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\stg_support_tickets.sql
[0m20:33:24.036404 [info ] [MainThread]: 
[0m20:33:24.038531 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m20:33:24.041981 [debug] [MainThread]: Command `dbt run` failed at 20:33:24.040693 after 20.07 seconds
[0m20:33:24.043189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6CE4EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B71ADA8F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B71AD8EE0>]}
[0m20:33:24.044627 [debug] [MainThread]: Flushing usage events
[0m20:33:26.228479 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:33:42.507874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253F89FEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253FAD183A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253FAD181F0>]}


============================== 20:33:42.511320 | 40579dc9-f120-4d15-bb65-47a5b57b2e31 ==============================
[0m20:33:42.511320 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:33:42.511320 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_support_tickets', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:33:44.499978 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:33:44.499978 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:33:44.499978 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:33:46.190747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253FD6AC160>]}
[0m20:33:46.317373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538E4C6E60>]}
[0m20:33:46.317373 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:33:47.246858 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:33:47.246858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538E632230>]}
[0m20:33:47.300160 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:33:47.851616 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:33:47.867711 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\stg_support_tickets.sql
[0m20:33:48.730148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538F14CBB0>]}
[0m20:33:49.064636 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:33:49.076853 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:33:49.088025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538EE973A0>]}
[0m20:33:49.103207 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:33:49.104717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538EE5C430>]}
[0m20:33:49.104717 [info ] [MainThread]: 
[0m20:33:49.104717 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:33:49.111692 [info ] [MainThread]: 
[0m20:33:49.113147 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:33:49.114225 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:33:49.115256 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:33:49.115256 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:33:49.140351 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:33:49.140351 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:33:49.140351 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:33:52.287597 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-0d8f-1caf-a96f-1bccb3619896) - Created
[0m20:33:52.737930 [debug] [ThreadPool]: SQL status: OK in 3.600 seconds
[0m20:33:52.737930 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cdb-0d8f-1caf-a96f-1bccb3619896, command-id=01f10cdb-0db9-1b16-a21d-2176b3d2aaf8) - Closing
[0m20:33:52.737930 [debug] [ThreadPool]: On list_workspace: Close
[0m20:33:52.737930 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-0d8f-1caf-a96f-1bccb3619896) - Closing
[0m20:33:53.055058 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:33:53.055058 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:33:53.073823 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:33:53.073823 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:33:53.073823 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:33:56.161230 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-0fe2-133f-8df9-4b954a4f4c4d) - Created
[0m20:33:56.722980 [debug] [ThreadPool]: SQL status: OK in 3.650 seconds
[0m20:33:56.729365 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cdb-0fe2-133f-8df9-4b954a4f4c4d, command-id=01f10cdb-1008-1708-9773-91969e8ae81e) - Closing
[0m20:33:56.731372 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:33:56.731372 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-0fe2-133f-8df9-4b954a4f4c4d) - Closing
[0m20:33:57.015068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538EE95E10>]}
[0m20:33:57.023493 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:57.025795 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_support_tickets ...................... [RUN]
[0m20:33:57.027837 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:33:57.029862 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:33:57.029862 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:57.038620 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:33:57.038620 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:57.070788 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:33:57.087346 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:33:57.087346 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538EEF5570>]}
[0m20:33:57.123487 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:33:57.140867 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:33:57.140867 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:33:57.140867 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_support_tickets

),

cleaned as (

    select
        ticket_id
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:33:57.140867 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:34:00.098893 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cdb-1239-193d-96db-7d7527d9b9b3) - Created
[0m20:34:01.153053 [debug] [Thread-3 (]: SQL status: OK in 4.010 seconds
[0m20:34:01.153053 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10cdb-1239-193d-96db-7d7527d9b9b3, command-id=01f10cdb-1261-1659-99b4-263e5a616557) - Closing
[0m20:34:01.185188 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:34:01.185188 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:34:01.185188 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cdb-1239-193d-96db-7d7527d9b9b3) - Closing
[0m20:34:01.612017 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253F6E0AF20>]}
[0m20:34:01.612017 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_support_tickets ................. [[32mOK[0m in 4.58s]
[0m20:34:01.612017 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:34:01.620248 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:34:01.620248 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:34:01.620248 [info ] [MainThread]: 
[0m20:34:01.620248 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 12.51 seconds (12.51s).
[0m20:34:01.630094 [debug] [MainThread]: Command end result
[0m20:34:01.720494 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:34:01.737733 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:34:01.755268 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:34:01.757092 [info ] [MainThread]: 
[0m20:34:01.757092 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:34:01.757092 [info ] [MainThread]: 
[0m20:34:01.757092 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:34:01.757092 [debug] [MainThread]: Command `dbt run` succeeded at 20:34:01.757092 after 19.44 seconds
[0m20:34:01.757092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253F89FEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253F8D536D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538E4F97B0>]}
[0m20:34:01.757092 [debug] [MainThread]: Flushing usage events
[0m20:34:03.315095 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:34:54.009906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB7E3EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB91803D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB9180220>]}


============================== 20:34:54.009906 | ebbd45c1-6390-4feb-9948-78d03dd2c12e ==============================
[0m20:34:54.009906 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:34:54.009906 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'invocation_command': 'dbt test --select stg_support_tickets', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:34:55.979685 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:34:55.979685 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:34:55.979685 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:34:57.746174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB91801F0>]}
[0m20:34:57.866995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB888EB60>]}
[0m20:34:57.866995 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:34:58.849577 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:34:58.850713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB8C9A3E0>]}
[0m20:34:58.898264 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:34:59.462060 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:34:59.462060 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:34:59.462060 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:34:59.578201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFCCF35ED0>]}
[0m20:34:59.795196 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:34:59.812098 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:34:59.845471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFCCE8D810>]}
[0m20:34:59.845471 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:34:59.845471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFCCE73F10>]}
[0m20:34:59.845471 [info ] [MainThread]: 
[0m20:34:59.845471 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:34:59.861873 [info ] [MainThread]: 
[0m20:34:59.864297 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:34:59.865632 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:34:59.880139 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:34:59.880139 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:34:59.912657 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:34:59.912657 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:34:59.912657 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:35:02.956015 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-37b0-1077-a7a2-2d0721a10917) - Created
[0m20:35:03.563406 [debug] [ThreadPool]: SQL status: OK in 3.650 seconds
[0m20:35:03.572805 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cdb-37b0-1077-a7a2-2d0721a10917, command-id=01f10cdb-37d7-1acb-b87c-0b180d373abb) - Closing
[0m20:35:03.573805 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:35:03.574992 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-37b0-1077-a7a2-2d0721a10917) - Closing
[0m20:35:03.854383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFCCE8DFC0>]}
[0m20:35:03.863174 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:35:03.864191 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_support_tickets_account_id ...................... [RUN]
[0m20:35:03.866438 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:35:03.867439 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:35:03.868456 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:35:03.895804 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:35:03.911939 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:35:03.945714 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:35:03.945714 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:35:03.945714 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:35:03.945714 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:07.224745 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-3a3c-188c-bebd-1f8dc951481b) - Created
[0m20:35:07.656563 [debug] [Thread-2 (]: SQL status: OK in 3.710 seconds
[0m20:35:07.661727 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cdb-3a3c-188c-bebd-1f8dc951481b, command-id=01f10cdb-3a63-1d1c-b5b2-e6a40cf1e572) - Closing
[0m20:35:07.669253 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:35:07.669253 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-3a3c-188c-bebd-1f8dc951481b) - Closing
[0m20:35:07.955336 [info ] [Thread-2 (]: 1 of 4 PASS not_null_stg_support_tickets_account_id ............................ [[32mPASS[0m in 4.09s]
[0m20:35:07.959044 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:35:07.959044 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:35:07.961052 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:35:07.963452 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:35:07.965896 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:35:07.966904 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:35:07.982743 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:35:07.984750 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:35:07.985821 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:35:07.995977 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:35:07.995977 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:35:07.995977 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:11.136954 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-3c91-18ca-8db6-d31391ce813b) - Created
[0m20:35:11.526543 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cdb-3cb8-170e-addf-6732feb4ac0b
[0m20:35:11.528550 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:35:11.530558 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-3c91-18ca-8db6-d31391ce813b) - Closing
[0m20:35:11.817269 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:35:11.820994 [error] [Thread-2 (]: 2 of 4 ERROR not_null_stg_support_tickets_ticket_id ............................ [[31mERROR[0m in 3.86s]
[0m20:35:11.823001 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:35:11.825377 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:35:11.826887 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql.
[0m20:35:11.827842 [info ] [Thread-2 (]: 3 of 4 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:35:11.832682 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:35:11.833755 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:35:11.833755 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:35:11.862830 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:35:11.864830 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:35:11.872366 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:35:11.874374 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:35:11.877516 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:35:11.878516 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:14.997834 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-3edc-1d0d-bd13-92b5649725ff) - Created
[0m20:35:15.468816 [debug] [Thread-2 (]: SQL status: OK in 3.590 seconds
[0m20:35:15.478799 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cdb-3edc-1d0d-bd13-92b5649725ff, command-id=01f10cdb-3f06-1e9c-8a8d-eeeb7d9feda7) - Closing
[0m20:35:15.478799 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:35:15.478799 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-3edc-1d0d-bd13-92b5649725ff) - Closing
[0m20:35:15.779236 [error] [Thread-2 (]: 3 of 4 FAIL 2000 relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[31mFAIL 2000[0m in 3.95s]
[0m20:35:15.782238 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:35:15.783545 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:35:15.784551 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_support_tickets_ticket_id ......................... [RUN]
[0m20:35:15.787154 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:35:15.788155 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:35:15.789160 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:35:15.811305 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:35:15.813902 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:35:15.818386 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:35:15.818386 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:35:15.826998 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:35:15.828301 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:18.814620 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-4126-1ca3-b14d-15c4a85cc113) - Created
[0m20:35:19.244839 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cdb-414d-144b-8ac2-5c7d81e19e38
[0m20:35:19.246878 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:35:19.248916 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-4126-1ca3-b14d-15c4a85cc113) - Closing
[0m20:35:19.527925 [debug] [Thread-2 (]: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:35:19.528936 [error] [Thread-2 (]: 4 of 4 ERROR unique_stg_support_tickets_ticket_id .............................. [[31mERROR[0m in 3.74s]
[0m20:35:19.531028 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:35:19.533049 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a' to be skipped because of status 'error'.  Reason: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql.
[0m20:35:19.536947 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:35:19.538196 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:35:19.540296 [info ] [MainThread]: 
[0m20:35:19.541301 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 19.68 seconds (19.68s).
[0m20:35:19.543158 [debug] [MainThread]: Command end result
[0m20:35:19.632627 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:35:19.639265 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:35:19.655418 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:35:19.656757 [info ] [MainThread]: 
[0m20:35:19.658802 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m20:35:19.659877 [info ] [MainThread]: 
[0m20:35:19.659877 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:35:19.664587 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:35:19.666715 [info ] [MainThread]: 
[0m20:35:19.667800 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:35:19.668953 [info ] [MainThread]: 
[0m20:35:19.669958 [error] [MainThread]: [31mFailure in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)[0m
[0m20:35:19.669958 [error] [MainThread]:   Got 2000 results, configured to fail if != 0
[0m20:35:19.674117 [info ] [MainThread]: 
[0m20:35:19.675430 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:35:19.676919 [info ] [MainThread]: 
[0m20:35:19.679269 [error] [MainThread]: [31mFailure in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:35:19.680520 [error] [MainThread]:   Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:35:19.681533 [info ] [MainThread]: 
[0m20:35:19.682538 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:35:19.683735 [info ] [MainThread]: 
[0m20:35:19.685042 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=4
[0m20:35:19.688232 [debug] [MainThread]: Command `dbt test` failed at 20:35:19.687230 after 25.86 seconds
[0m20:35:19.688232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB7E3EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB818C340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFCD169CF0>]}
[0m20:35:19.689508 [debug] [MainThread]: Flushing usage events
[0m20:35:21.325864 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:39:20.631548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC661DEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC675283A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC675281F0>]}


============================== 20:39:20.631548 | 3f151b8b-a7a7-4b9e-b68f-ac83a00c757c ==============================
[0m20:39:20.631548 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:39:20.647924 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_support_tickets', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:39:22.540662 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:39:22.540662 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:39:22.540662 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:39:24.252808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC6361BDF0>]}
[0m20:39:24.406126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC6734D000>]}
[0m20:39:24.406126 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:39:25.307140 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:39:25.307140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC67005ED0>]}
[0m20:39:25.360069 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:39:25.904957 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:39:25.904957 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\stg_support_tickets.sql
[0m20:39:26.839884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC7B7513F0>]}
[0m20:39:27.182180 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:39:27.182180 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:39:27.214379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC7B4507F0>]}
[0m20:39:27.214379 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:39:27.214379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC7B451480>]}
[0m20:39:27.214379 [info ] [MainThread]: 
[0m20:39:27.214379 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:39:27.228708 [info ] [MainThread]: 
[0m20:39:27.231068 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:39:27.232130 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:39:27.232130 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:39:27.232130 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:39:27.262417 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:39:27.263419 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:39:27.264662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:39:30.387587 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-d717-1ad8-a027-128c6842b915) - Created
[0m20:39:30.992743 [debug] [ThreadPool]: SQL status: OK in 3.730 seconds
[0m20:39:30.992743 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cdb-d717-1ad8-a027-128c6842b915, command-id=01f10cdb-d740-18ca-b22b-6d8fe3264e91) - Closing
[0m20:39:31.006821 [debug] [ThreadPool]: On list_workspace: Close
[0m20:39:31.008566 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-d717-1ad8-a027-128c6842b915) - Closing
[0m20:39:31.307866 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:39:31.311575 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:39:31.321155 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:39:31.321155 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:39:31.321155 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:39:34.607191 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-d998-18ad-8fc3-8a1c2ef9e814) - Created
[0m20:39:35.257971 [debug] [ThreadPool]: SQL status: OK in 3.940 seconds
[0m20:39:35.270089 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cdb-d998-18ad-8fc3-8a1c2ef9e814, command-id=01f10cdb-d9c2-1b63-b0b9-9c0dd343bcd1) - Closing
[0m20:39:35.272095 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:39:35.272095 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-d998-18ad-8fc3-8a1c2ef9e814) - Closing
[0m20:39:35.564402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC7B453760>]}
[0m20:39:35.570443 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:39:35.570443 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_support_tickets ...................... [RUN]
[0m20:39:35.570443 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:39:35.570443 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:39:35.570443 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:39:35.592723 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:39:35.592723 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:39:35.639001 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:39:35.643327 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:39:35.645332 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC66C134F0>]}
[0m20:39:35.675355 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:39:35.690758 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:39:35.690758 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:39:35.690758 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_support_tickets

),

cleaned as (

    select
        ticket_id,
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:39:35.707509 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:39:38.817863 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cdb-dc1f-179d-9007-ca216b0b889b) - Created
[0m20:39:39.663166 [debug] [Thread-3 (]: SQL status: OK in 3.960 seconds
[0m20:39:39.663166 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10cdb-dc1f-179d-9007-ca216b0b889b, command-id=01f10cdb-dc48-1f5e-ab37-980ab3575b62) - Closing
[0m20:39:39.679036 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:39:39.694739 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:39:39.694739 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cdb-dc1f-179d-9007-ca216b0b889b) - Closing
[0m20:39:39.960155 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC6361AEF0>]}
[0m20:39:39.963618 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_support_tickets ................. [[32mOK[0m in 4.39s]
[0m20:39:39.963618 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:39:39.963618 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:39:39.963618 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:39:39.963618 [info ] [MainThread]: 
[0m20:39:39.974984 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 12.73 seconds (12.73s).
[0m20:39:39.978108 [debug] [MainThread]: Command end result
[0m20:39:40.072492 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:39:40.073352 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:39:40.093514 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:39:40.093514 [info ] [MainThread]: 
[0m20:39:40.093514 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:39:40.093514 [info ] [MainThread]: 
[0m20:39:40.093514 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:39:40.102724 [debug] [MainThread]: Command `dbt run` succeeded at 20:39:40.102724 after 19.63 seconds
[0m20:39:40.103765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC661DEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC7AF35690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC66579ED0>]}
[0m20:39:40.105186 [debug] [MainThread]: Flushing usage events
[0m20:39:41.622412 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:40:05.016567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53A83EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53CB583A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53CB581F0>]}


============================== 20:40:05.032653 | e3473b88-1981-4d70-8815-ba54b9c9edc8 ==============================
[0m20:40:05.032653 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:40:05.032653 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt test --select stg_support_tickets', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:40:07.293818 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:40:07.294819 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:40:07.295818 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:40:09.134635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53CB58460>]}
[0m20:40:09.249039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D54EC87280>]}
[0m20:40:09.249039 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:40:10.135548 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:40:10.135548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53C674A30>]}
[0m20:40:10.186012 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:40:10.701993 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:40:10.701993 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:40:10.701993 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:40:10.801982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D550926320>]}
[0m20:40:11.035232 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:40:11.035232 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:40:11.085525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D55087D1B0>]}
[0m20:40:11.085525 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:40:11.085525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D55087FBE0>]}
[0m20:40:11.085525 [info ] [MainThread]: 
[0m20:40:11.085525 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:40:11.085525 [info ] [MainThread]: 
[0m20:40:11.100946 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:40:11.102012 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:40:11.121569 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:40:11.122597 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:40:11.137342 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:40:11.151535 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:40:11.153142 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:40:14.116682 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-f129-1f82-a6b8-fa939d5d7dd4) - Created
[0m20:40:14.720555 [debug] [ThreadPool]: SQL status: OK in 3.570 seconds
[0m20:40:14.730789 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cdb-f129-1f82-a6b8-fa939d5d7dd4, command-id=01f10cdb-f14f-1304-8519-52d2b14ff0cf) - Closing
[0m20:40:14.732830 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:40:14.734343 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-f129-1f82-a6b8-fa939d5d7dd4) - Closing
[0m20:40:15.007228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D55087E140>]}
[0m20:40:15.011990 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:40:15.011990 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_support_tickets_account_id ...................... [RUN]
[0m20:40:15.011990 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:40:15.021774 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:40:15.023299 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:40:15.051962 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:40:15.051962 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:40:15.102113 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:40:15.102113 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:40:15.102113 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:40:15.102113 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:40:17.997591 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-f37d-1073-a03e-f0f4cef255f7) - Created
[0m20:40:18.710251 [debug] [Thread-2 (]: SQL status: OK in 3.590 seconds
[0m20:40:18.710251 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cdb-f37d-1073-a03e-f0f4cef255f7, command-id=01f10cdb-f3a1-1ac9-9ba8-c3718ea42b99) - Closing
[0m20:40:18.710251 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:40:18.710251 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-f37d-1073-a03e-f0f4cef255f7) - Closing
[0m20:40:18.973444 [info ] [Thread-2 (]: 1 of 4 PASS not_null_stg_support_tickets_account_id ............................ [[32mPASS[0m in 3.96s]
[0m20:40:18.976603 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:40:18.977623 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:40:18.978641 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:40:18.981144 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:40:18.983369 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:40:18.985481 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:40:18.988607 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:40:19.002405 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:40:19.008846 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:40:19.008846 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:40:19.008846 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:40:19.008846 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:40:21.924935 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-f5d1-1218-b3d1-26da52e8754c) - Created
[0m20:40:22.628686 [debug] [Thread-2 (]: SQL status: OK in 3.620 seconds
[0m20:40:22.628686 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cdb-f5d1-1218-b3d1-26da52e8754c, command-id=01f10cdb-f5f9-1c23-8d76-a59a17c48a6c) - Closing
[0m20:40:22.628686 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:40:22.628686 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-f5d1-1218-b3d1-26da52e8754c) - Closing
[0m20:40:22.913772 [info ] [Thread-2 (]: 2 of 4 PASS not_null_stg_support_tickets_ticket_id ............................. [[32mPASS[0m in 3.93s]
[0m20:40:22.913772 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:40:22.913772 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:40:22.931004 [info ] [Thread-2 (]: 3 of 4 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:40:22.933909 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:40:22.934953 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:40:22.936060 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:40:22.956711 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:40:22.956711 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:40:22.970477 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:40:22.971904 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:40:22.973067 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:40:22.974228 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:40:26.307975 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-f86c-1c5b-a1cd-e8365c1bebf0) - Created
[0m20:40:27.535309 [debug] [Thread-2 (]: SQL status: OK in 4.560 seconds
[0m20:40:27.540309 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cdb-f86c-1c5b-a1cd-e8365c1bebf0, command-id=01f10cdb-f894-192f-8de6-41b2aff447aa) - Closing
[0m20:40:27.542605 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:40:27.544608 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-f86c-1c5b-a1cd-e8365c1bebf0) - Closing
[0m20:40:27.822465 [info ] [Thread-2 (]: 3 of 4 PASS relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 4.89s]
[0m20:40:27.824664 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:40:27.825802 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:40:27.827885 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_support_tickets_ticket_id ......................... [RUN]
[0m20:40:27.830403 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:40:27.832653 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:40:27.835657 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:40:27.862073 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:40:27.864145 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:40:27.875563 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:40:27.877891 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:40:27.879398 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:40:27.881425 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:40:31.620773 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-fb94-1058-b930-750b2c9df50b) - Created
[0m20:40:32.343301 [debug] [Thread-2 (]: SQL status: OK in 4.460 seconds
[0m20:40:32.343301 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cdb-fb94-1058-b930-750b2c9df50b, command-id=01f10cdb-fbbe-1e33-8dc5-6da8b5bfc4c9) - Closing
[0m20:40:32.343301 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:40:32.353737 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-fb94-1058-b930-750b2c9df50b) - Closing
[0m20:40:32.619415 [info ] [Thread-2 (]: 4 of 4 PASS unique_stg_support_tickets_ticket_id ............................... [[32mPASS[0m in 4.79s]
[0m20:40:32.629789 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:40:32.629789 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:40:32.629789 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:40:32.629789 [info ] [MainThread]: 
[0m20:40:32.629789 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 21.53 seconds (21.53s).
[0m20:40:32.640111 [debug] [MainThread]: Command end result
[0m20:40:32.770024 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:40:32.776483 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:40:32.798121 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:40:32.798121 [info ] [MainThread]: 
[0m20:40:32.798121 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:40:32.798121 [info ] [MainThread]: 
[0m20:40:32.798121 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m20:40:32.808163 [debug] [MainThread]: Command `dbt test` succeeded at 20:40:32.808163 after 27.96 seconds
[0m20:40:32.810712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53A83EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D54EC87280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53BBADB40>]}
[0m20:40:32.810712 [debug] [MainThread]: Flushing usage events
[0m20:40:34.621863 [debug] [MainThread]: An error was encountered while trying to flush usage events
