[0m19:51:36.421070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207AB21EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207AD5303A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207AD530190>]}


============================== 19:51:36.454209 | 21be7286-f5a3-42e1-b31d-0e3215a8bab4 ==============================
[0m19:51:36.454209 [info ] [MainThread]: Running with dbt=1.11.2
[0m19:51:36.454209 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt init saas_dbt_analytics', 'use_experimental_parser': 'False', 'log_path': 'logs'}
[0m19:51:36.503890 [info ] [MainThread]: Creating dbt configuration folder at C:\Users\HP\.dbt
[0m19:51:36.510371 [debug] [MainThread]: Starter project path: C:\Users\HP\anaconda3\envs\saas\lib\site-packages\dbt\include\starter_project
[0m19:51:36.924390 [info ] [MainThread]: 
Your new dbt project "saas_dbt_analytics" was created!

For more information on how to configure the profiles.yml file,
please consult the dbt documentation here:

  https://docs.getdbt.com/docs/configure-your-profile

One more thing:

Need help? Don't hesitate to reach out to us via GitHub issues or on Slack:

  https://community.getdbt.com/

Happy modeling!

[0m19:51:36.924390 [info ] [MainThread]: Setting up your profile.
[0m19:51:46.408359 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:51:46.411894 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:51:46.411894 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:59:23.208011 [info ] [MainThread]: Profile saas_dbt_analytics written to C:\Users\HP\.dbt\profiles.yml using target's profile_template.yml and your supplied values. Run 'dbt debug' to validate the connection.
[0m19:59:23.213381 [debug] [MainThread]: Command `dbt init` succeeded at 19:59:23.212528 after 466.98 seconds
[0m19:59:23.216651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207AB21EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207AD52D2A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207AD531240>]}
[0m19:59:23.217410 [debug] [MainThread]: Flushing usage events
[0m19:59:25.213883 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:59:53.620130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F2393EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F24C98400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F24C98190>]}


============================== 19:59:53.620130 | f73c97a0-05df-427f-bdef-3c42547f405b ==============================
[0m19:59:53.620130 [info ] [MainThread]: Running with dbt=1.11.2
[0m19:59:53.636212 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'logs'}
[0m19:59:53.674659 [info ] [MainThread]: dbt version: 1.11.2
[0m19:59:53.674659 [info ] [MainThread]: python version: 3.10.19
[0m19:59:53.687834 [info ] [MainThread]: python path: C:\Users\HP\anaconda3\envs\saas\python.exe
[0m19:59:53.687834 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m19:59:55.901349 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:59:55.901349 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:59:55.903356 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:59:57.463567 [info ] [MainThread]: Using profiles dir at C:\Users\HP\.dbt
[0m19:59:57.463567 [info ] [MainThread]: Using profiles.yml file at C:\Users\HP\.dbt\profiles.yml
[0m19:59:57.463567 [info ] [MainThread]: Using dbt_project.yml file at D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml
[0m19:59:57.463567 [info ] [MainThread]: adapter type: databricks
[0m19:59:57.463567 [info ] [MainThread]: adapter version: 1.11.4
[0m19:59:57.479338 [info ] [MainThread]: Configuration:
[0m19:59:57.481206 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m19:59:57.484183 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m19:59:57.485257 [info ] [MainThread]: Required dependencies:
[0m19:59:57.487708 [debug] [MainThread]: Executing "git --help"
[0m19:59:57.592396 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:59:57.603495 [debug] [MainThread]: STDERR: "b''"
[0m19:59:57.604508 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m19:59:57.606620 [info ] [MainThread]: Connection:
[0m19:59:57.608652 [info ] [MainThread]:   host: dbc-61bb1ae2-dcbc.cloud.databricks.com
[0m19:59:57.609660 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/f7938fbc9bec1e4a
[0m19:59:57.610665 [info ] [MainThread]:   catalog: workspace
[0m19:59:57.613090 [info ] [MainThread]:   schema: analytics
[0m19:59:57.614097 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m19:59:58.619384 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:59:58.636145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'f73c97a0-05df-427f-bdef-3c42547f405b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F246360E0>]}
[0m19:59:58.636145 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m19:59:58.636145 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m19:59:58.636145 [debug] [MainThread]: Using databricks connection "debug"
[0m19:59:58.636145 [debug] [MainThread]: On debug: select 1 as id
[0m19:59:58.636145 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:00:01.869935 [error] [MainThread]: databricks-sql-connector adapter: ThriftBackend.attempt_request: Exception: %s
[0m20:00:02.613448 [error] [MainThread]: Databricks adapter: Connection(session-id=Unknown) - Exception while trying to create connection: Error during request to server: : Invalid access token.. 
Error properties: attempt=1/30, bounded-retry-delay=None, elapsed-seconds=1.6760308742523193/900.0, error-message=: Invalid access token., http-code=403, method=OpenSession, no-retry-reason=non-retryable error, original-exception=, query-id=None, session-id=None
[0m20:00:02.615456 [debug] [MainThread]: Databricks adapter: Exception while trying to execute query
select 1 as id
: Database Error
  Error during request to server: : Invalid access token.. 
[0m20:00:02.617463 [debug] [MainThread]: On debug: No close available on handle
[0m20:00:02.618743 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m20:00:02.618743 [info ] [MainThread]: [31m2 checks failed:[0m
[0m20:00:02.622136 [info ] [MainThread]: Project loading failed for the following reason:
 project path <D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml> not found

[0m20:00:02.623140 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  Database Error
    Error during request to server: : Invalid access token.. 

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m20:00:02.626564 [debug] [MainThread]: Command `dbt debug` failed at 20:00:02.625551 after 9.25 seconds
[0m20:00:02.626564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F2393EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F24636590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F2466A4D0>]}
[0m20:00:02.626564 [debug] [MainThread]: Flushing usage events
[0m20:00:04.251424 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:03:13.602756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEB861EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEB99603A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEB99601C0>]}


============================== 20:03:13.610784 | 3268c31f-7d38-41ba-a094-4a9131f36441 ==============================
[0m20:03:13.610784 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:03:13.612530 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'logs'}
[0m20:03:13.660542 [info ] [MainThread]: dbt version: 1.11.2
[0m20:03:13.661760 [info ] [MainThread]: python version: 3.10.19
[0m20:03:13.663107 [info ] [MainThread]: python path: C:\Users\HP\anaconda3\envs\saas\python.exe
[0m20:03:13.664434 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m20:03:16.317659 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:03:16.319667 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:03:16.321675 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:03:17.972658 [info ] [MainThread]: Using profiles dir at C:\Users\HP\.dbt
[0m20:03:17.972658 [info ] [MainThread]: Using profiles.yml file at C:\Users\HP\.dbt\profiles.yml
[0m20:03:17.972658 [info ] [MainThread]: Using dbt_project.yml file at D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml
[0m20:03:17.972658 [info ] [MainThread]: adapter type: databricks
[0m20:03:17.972658 [info ] [MainThread]: adapter version: 1.11.4
[0m20:03:17.988698 [info ] [MainThread]: Configuration:
[0m20:03:17.990784 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:03:17.992203 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m20:03:17.995211 [info ] [MainThread]: Required dependencies:
[0m20:03:17.997401 [debug] [MainThread]: Executing "git --help"
[0m20:03:18.110752 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:03:18.110752 [debug] [MainThread]: STDERR: "b''"
[0m20:03:18.110752 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:03:18.116292 [info ] [MainThread]: Connection:
[0m20:03:18.119312 [info ] [MainThread]:   host: dbc-61bb1ae2-dcbc.cloud.databricks.com
[0m20:03:18.121701 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/f7938fbc9bec1e4a
[0m20:03:18.121701 [info ] [MainThread]:   catalog: workspace
[0m20:03:18.126064 [info ] [MainThread]:   schema: analytics
[0m20:03:18.129106 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:03:19.207762 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:03:19.207762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '3268c31f-7d38-41ba-a094-4a9131f36441', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEBC3D94E0>]}
[0m20:03:19.207762 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m20:03:19.207762 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:03:19.207762 [debug] [MainThread]: Using databricks connection "debug"
[0m20:03:19.216075 [debug] [MainThread]: On debug: select 1 as id
[0m20:03:19.218082 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:03:22.690048 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0d-a01a-1f34-9f54-b142d554d556) - Created
[0m20:03:36.658314 [debug] [MainThread]: SQL status: OK in 17.440 seconds
[0m20:03:36.660320 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f10c0d-a01a-1f34-9f54-b142d554d556, command-id=01f10c0d-a054-12e7-afdc-b7a74d9a850a) - Closing
[0m20:03:37.002603 [debug] [MainThread]: On debug: Close
[0m20:03:37.015847 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0d-a01a-1f34-9f54-b142d554d556) - Closing
[0m20:03:37.282567 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m20:03:37.284576 [info ] [MainThread]: [31m1 check failed:[0m
[0m20:03:37.287626 [info ] [MainThread]: Project loading failed for the following reason:
 project path <D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml> not found

[0m20:03:37.291396 [debug] [MainThread]: Command `dbt debug` failed at 20:03:37.291396 after 23.94 seconds
[0m20:03:37.293732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEB861EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEB930F130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EEB930CC70>]}
[0m20:03:37.295107 [debug] [MainThread]: Flushing usage events
[0m20:03:38.979522 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:05:49.652567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B130DEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B144283A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B144281C0>]}


============================== 20:05:49.667134 | 7c5de5a1-2c51-4eb4-b151-68e7539d9d95 ==============================
[0m20:05:49.667134 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:05:49.669193 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt debug', 'use_experimental_parser': 'False', 'log_path': 'logs'}
[0m20:05:49.719033 [info ] [MainThread]: dbt version: 1.11.2
[0m20:05:49.720531 [info ] [MainThread]: python version: 3.10.19
[0m20:05:49.722595 [info ] [MainThread]: python path: C:\Users\HP\anaconda3\envs\saas\python.exe
[0m20:05:49.723615 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m20:05:51.877235 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:05:51.879242 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:05:51.879242 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:05:53.447098 [info ] [MainThread]: Using profiles dir at C:\Users\HP\.dbt
[0m20:05:53.447098 [info ] [MainThread]: Using profiles.yml file at C:\Users\HP\.dbt\profiles.yml
[0m20:05:53.447098 [info ] [MainThread]: Using dbt_project.yml file at D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml
[0m20:05:53.447098 [info ] [MainThread]: adapter type: databricks
[0m20:05:53.447098 [info ] [MainThread]: adapter version: 1.11.4
[0m20:05:53.456287 [info ] [MainThread]: Configuration:
[0m20:05:53.457786 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:05:53.459868 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m20:05:53.462523 [info ] [MainThread]: Required dependencies:
[0m20:05:53.464765 [debug] [MainThread]: Executing "git --help"
[0m20:05:53.574439 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:05:53.574439 [debug] [MainThread]: STDERR: "b''"
[0m20:05:53.574439 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:05:53.574439 [info ] [MainThread]: Connection:
[0m20:05:53.574439 [info ] [MainThread]:   host: dbc-61bb1ae2-dcbc.cloud.databricks.com
[0m20:05:53.574439 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/f7938fbc9bec1e4a
[0m20:05:53.574439 [info ] [MainThread]:   catalog: workspace
[0m20:05:53.587368 [info ] [MainThread]:   schema: analytics
[0m20:05:53.589554 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:05:54.656013 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:05:54.656013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7c5de5a1-2c51-4eb4-b151-68e7539d9d95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B16E98B50>]}
[0m20:05:54.656013 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m20:05:54.656013 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:05:54.656013 [debug] [MainThread]: Using databricks connection "debug"
[0m20:05:54.656013 [debug] [MainThread]: On debug: select 1 as id
[0m20:05:54.656013 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:05:57.827827 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0d-fca4-15d2-962b-9a6860b02b3a) - Created
[0m20:05:58.414847 [debug] [MainThread]: SQL status: OK in 3.760 seconds
[0m20:05:58.430455 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f10c0d-fca4-15d2-962b-9a6860b02b3a, command-id=01f10c0d-fccc-104d-a110-c4e06d02e42d) - Closing
[0m20:05:58.430455 [debug] [MainThread]: On debug: Close
[0m20:05:58.430455 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0d-fca4-15d2-962b-9a6860b02b3a) - Closing
[0m20:05:58.696696 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m20:05:58.696696 [info ] [MainThread]: [31m1 check failed:[0m
[0m20:05:58.696696 [info ] [MainThread]: Project loading failed for the following reason:
 project path <D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml> not found

[0m20:05:58.696696 [debug] [MainThread]: Command `dbt debug` failed at 20:05:58.696696 after 9.26 seconds
[0m20:05:58.696696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B130DEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B13DCEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B13DCF1F0>]}
[0m20:05:58.696696 [debug] [MainThread]: Flushing usage events
[0m20:06:00.294973 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:07:06.082256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002320AB4EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002320CE6C3D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002320CE6C1F0>]}


============================== 20:07:06.091335 | 99a163f3-ff17-460f-be63-6aa7b3419cb4 ==============================
[0m20:07:06.091335 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:07:06.091335 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'logs'}
[0m20:07:06.144967 [info ] [MainThread]: dbt version: 1.11.2
[0m20:07:06.146983 [info ] [MainThread]: python version: 3.10.19
[0m20:07:06.147993 [info ] [MainThread]: python path: C:\Users\HP\anaconda3\envs\saas\python.exe
[0m20:07:06.150009 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m20:07:08.240784 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:07:08.240784 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:07:08.240784 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:07:09.805008 [info ] [MainThread]: Using profiles dir at C:\Users\HP\.dbt
[0m20:07:09.805008 [info ] [MainThread]: Using profiles.yml file at C:\Users\HP\.dbt\profiles.yml
[0m20:07:09.805008 [info ] [MainThread]: Using dbt_project.yml file at D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml
[0m20:07:09.805008 [info ] [MainThread]: adapter type: databricks
[0m20:07:09.820804 [info ] [MainThread]: adapter version: 1.11.4
[0m20:07:09.824571 [info ] [MainThread]: Configuration:
[0m20:07:09.826605 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:07:09.827110 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m20:07:09.829670 [info ] [MainThread]: Required dependencies:
[0m20:07:09.831918 [debug] [MainThread]: Executing "git --help"
[0m20:07:09.943270 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:07:09.943270 [debug] [MainThread]: STDERR: "b''"
[0m20:07:09.943270 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:07:09.943270 [info ] [MainThread]: Connection:
[0m20:07:09.943270 [info ] [MainThread]:   host: dbc-61bb1ae2-dcbc.cloud.databricks.com
[0m20:07:09.943270 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/f7938fbc9bec1e4a
[0m20:07:09.943270 [info ] [MainThread]:   catalog: workspace
[0m20:07:09.956920 [info ] [MainThread]:   schema: analytics
[0m20:07:09.960571 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:07:11.032954 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:07:11.034297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '99a163f3-ff17-460f-be63-6aa7b3419cb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002320C844C40>]}
[0m20:07:11.037614 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m20:07:11.038616 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:07:11.041221 [debug] [MainThread]: Using databricks connection "debug"
[0m20:07:11.042242 [debug] [MainThread]: On debug: select 1 as id
[0m20:07:11.044542 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:07:14.288961 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0e-2a37-1768-9b1d-37edf864d1c8) - Created
[0m20:07:14.751037 [debug] [MainThread]: SQL status: OK in 3.710 seconds
[0m20:07:14.755054 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f10c0e-2a37-1768-9b1d-37edf864d1c8, command-id=01f10c0e-2a5f-1ea9-b266-3790ea7938e3) - Closing
[0m20:07:14.757063 [debug] [MainThread]: On debug: Close
[0m20:07:14.757063 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0e-2a37-1768-9b1d-37edf864d1c8) - Closing
[0m20:07:15.032499 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m20:07:15.032499 [info ] [MainThread]: [31m1 check failed:[0m
[0m20:07:15.032499 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  dbt_project.yml does not parse to a dictionary


[0m20:07:15.032499 [debug] [MainThread]: Command `dbt debug` failed at 20:07:15.032499 after 9.16 seconds
[0m20:07:15.032499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002320AB4EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002322002A380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002320C7ECD30>]}
[0m20:07:15.043462 [debug] [MainThread]: Flushing usage events
[0m20:07:17.587339 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:09:24.899896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A7937DEB60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A795AFC370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A795AFC190>]}


============================== 20:09:24.915662 | 51c41592-e577-43de-8bd9-22e5c8d7659a ==============================
[0m20:09:24.915662 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:09:24.919789 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:09:24.968324 [info ] [MainThread]: dbt version: 1.11.2
[0m20:09:24.969637 [info ] [MainThread]: python version: 3.10.19
[0m20:09:24.970643 [info ] [MainThread]: python path: C:\Users\HP\anaconda3\envs\saas\python.exe
[0m20:09:24.971796 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m20:09:27.152646 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:09:27.152646 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:09:27.168738 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:09:28.849697 [info ] [MainThread]: Using profiles dir at C:\Users\HP\.dbt
[0m20:09:28.849697 [info ] [MainThread]: Using profiles.yml file at C:\Users\HP\.dbt\profiles.yml
[0m20:09:28.849697 [info ] [MainThread]: Using dbt_project.yml file at D:\DataScience\saas-databricks-dbt-analytics\dbt_project.yml
[0m20:09:28.849697 [info ] [MainThread]: adapter type: databricks
[0m20:09:28.849697 [info ] [MainThread]: adapter version: 1.11.4
[0m20:09:29.089657 [info ] [MainThread]: Configuration:
[0m20:09:29.091714 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:09:29.092749 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:09:29.094825 [info ] [MainThread]: Required dependencies:
[0m20:09:29.096568 [debug] [MainThread]: Executing "git --help"
[0m20:09:29.196018 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:09:29.198425 [debug] [MainThread]: STDERR: "b''"
[0m20:09:29.199425 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:09:29.200596 [info ] [MainThread]: Connection:
[0m20:09:29.201659 [info ] [MainThread]:   host: dbc-61bb1ae2-dcbc.cloud.databricks.com
[0m20:09:29.202867 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/f7938fbc9bec1e4a
[0m20:09:29.204285 [info ] [MainThread]:   catalog: workspace
[0m20:09:29.206481 [info ] [MainThread]:   schema: analytics
[0m20:09:29.208878 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:09:30.258403 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:09:30.258403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '51c41592-e577-43de-8bd9-22e5c8d7659a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A795544760>]}
[0m20:09:30.258403 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m20:09:30.258403 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:09:30.258403 [debug] [MainThread]: Using databricks connection "debug"
[0m20:09:30.258403 [debug] [MainThread]: On debug: select 1 as id
[0m20:09:30.272990 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:09:33.421070 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0e-7d27-1155-9b4a-ed63d230c2c9) - Created
[0m20:09:33.804280 [debug] [MainThread]: SQL status: OK in 3.530 seconds
[0m20:09:33.804280 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f10c0e-7d27-1155-9b4a-ed63d230c2c9, command-id=01f10c0e-7d4c-1580-bbf2-2f89002c53f7) - Closing
[0m20:09:33.820359 [debug] [MainThread]: On debug: Close
[0m20:09:33.822371 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f10c0e-7d27-1155-9b4a-ed63d230c2c9) - Closing
[0m20:09:34.068711 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m20:09:34.084777 [info ] [MainThread]: [32mAll checks passed![0m
[0m20:09:34.095722 [debug] [MainThread]: Command `dbt debug` succeeded at 20:09:34.095722 after 9.43 seconds
[0m20:09:34.097767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A7937DEB60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A794B4D6F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A795ABE410>]}
[0m20:09:34.098769 [debug] [MainThread]: Flushing usage events
[0m20:09:36.050463 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:10:52.869218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC18AEBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC2BF0370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC2BF0190>]}


============================== 20:10:52.876147 | 15cd9480-c59e-4877-b303-a0362fad9010 ==============================
[0m20:10:52.876147 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:10:52.880123 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'invocation_command': 'dbt run --select stg_accounts', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:10:55.032591 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:10:55.032591 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:10:55.032591 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:10:56.956353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC5514160>]}
[0m20:10:57.088913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC55149A0>]}
[0m20:10:57.088913 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:10:58.152952 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:10:58.152952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC2727A90>]}
[0m20:10:58.195102 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:10:58.199501 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m20:10:58.200508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC26B3430>]}
[0m20:11:02.185958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AD6B600D0>]}
[0m20:11:02.386434 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:11:02.386434 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:11:02.425343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AD6A85E40>]}
[0m20:11:02.425343 [info ] [MainThread]: Found 1 model, 731 macros
[0m20:11:02.433696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AD6B9CD00>]}
[0m20:11:02.437713 [info ] [MainThread]: 
[0m20:11:02.439841 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:11:02.443211 [info ] [MainThread]: 
[0m20:11:02.444805 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:11:02.444805 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:11:02.451482 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:11:02.451482 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:11:02.487243 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:11:02.487243 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:11:02.493530 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:11:05.702964 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0e-b423-1538-b5f5-18586c52cdb9) - Created
[0m20:11:06.715161 [debug] [ThreadPool]: SQL status: OK in 4.220 seconds
[0m20:11:06.715161 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c0e-b423-1538-b5f5-18586c52cdb9, command-id=01f10c0e-b44d-1faf-8b0f-686f968e3f62) - Closing
[0m20:11:06.728912 [debug] [ThreadPool]: On list_workspace: Close
[0m20:11:06.730922 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0e-b423-1538-b5f5-18586c52cdb9) - Closing
[0m20:11:07.011679 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:11:07.012682 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:11:07.026302 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:11:07.026302 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:11:07.029056 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:11:10.132814 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0e-b6cb-1a9e-9d5c-b87f6a2ec2dc) - Created
[0m20:11:11.202029 [debug] [ThreadPool]: SQL status: OK in 4.170 seconds
[0m20:11:11.217837 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c0e-b6cb-1a9e-9d5c-b87f6a2ec2dc, command-id=01f10c0e-b6f2-1c3a-b797-702248e40961) - Closing
[0m20:11:11.217837 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:11:11.217837 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0e-b6cb-1a9e-9d5c-b87f6a2ec2dc) - Closing
[0m20:11:11.512045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AD6A0BFD0>]}
[0m20:11:11.537462 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_accounts
[0m20:11:11.537462 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_accounts ............................. [RUN]
[0m20:11:11.537462 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_accounts) - Creating connection
[0m20:11:11.543855 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_accounts'
[0m20:11:11.545862 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_accounts
[0m20:11:11.577143 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_accounts"
[0m20:11:11.577143 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_accounts
[0m20:11:11.624761 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:11:11.642854 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:11:11.642854 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AD6B63430>]}
[0m20:11:11.677173 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_accounts`
[0m20:11:11.703795 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_accounts"
[0m20:11:11.710269 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_accounts"
[0m20:11:11.711269 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_accounts: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_accounts"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_accounts`
  
  as (
    select *
from workspace.default.bronze_accounts
  )

[0m20:11:11.713733 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:11:14.982261 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10c0e-b9a9-100a-a7cf-f4daef3a7503) - Created
[0m20:11:17.655128 [debug] [Thread-3 (]: SQL status: OK in 5.940 seconds
[0m20:11:17.655128 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10c0e-b9a9-100a-a7cf-f4daef3a7503, command-id=01f10c0e-b9d6-1c80-96ec-d17fcaa6249d) - Closing
[0m20:11:17.687017 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:11:17.687017 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_accounts: Close
[0m20:11:17.687017 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10c0e-b9a9-100a-a7cf-f4daef3a7503) - Closing
[0m20:11:18.003403 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15cd9480-c59e-4877-b303-a0362fad9010', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AD6BD3700>]}
[0m20:11:18.003403 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_accounts ........................ [[32mOK[0m in 6.47s]
[0m20:11:18.003403 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_accounts
[0m20:11:18.003403 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:11:18.019398 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:11:18.019398 [info ] [MainThread]: 
[0m20:11:18.019398 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 15.57 seconds (15.57s).
[0m20:11:18.025822 [debug] [MainThread]: Command end result
[0m20:11:18.101982 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:11:18.110091 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:11:18.126668 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:11:18.127674 [info ] [MainThread]: 
[0m20:11:18.128678 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:11:18.130867 [info ] [MainThread]: 
[0m20:11:18.131874 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:11:18.135396 [debug] [MainThread]: Command `dbt run` succeeded at 20:11:18.135396 after 25.54 seconds
[0m20:11:18.137854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC18AEBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC5514130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AC1C4EA40>]}
[0m20:11:18.139292 [debug] [MainThread]: Flushing usage events
[0m20:11:19.811181 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:17:01.591093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8DECEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8F2103A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8F2101C0>]}


============================== 20:17:01.591093 | 4e4abcc2-3db5-4957-8ef8-245b9836fd6a ==============================
[0m20:17:01.591093 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:17:01.591093 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select stg_accounts', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:17:03.817851 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:17:03.817851 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:17:03.817851 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:17:05.918463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD91831660>]}
[0m20:17:06.081421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8F1DCD60>]}
[0m20:17:06.081421 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:17:07.112268 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:17:07.112268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8ED06B90>]}
[0m20:17:07.144897 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:17:07.677583 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m20:17:07.677583 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\silver\schema.yml
[0m20:17:07.677583 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\stg_accounts.sql
[0m20:17:08.664017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA3340640>]}
[0m20:17:08.885952 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:17:08.885952 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:17:08.920110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA3343100>]}
[0m20:17:08.923846 [info ] [MainThread]: Found 1 model, 4 data tests, 731 macros
[0m20:17:08.923846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA2EAC790>]}
[0m20:17:08.923846 [info ] [MainThread]: 
[0m20:17:08.923846 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:17:08.933014 [info ] [MainThread]: 
[0m20:17:08.935131 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:17:08.936429 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:17:08.937783 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:17:08.937783 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:17:09.123214 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:17:09.123214 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:17:09.123214 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:17:12.411726 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0f-8ebc-143d-b8dc-08c1f5faeb18) - Created
[0m20:17:13.002931 [debug] [ThreadPool]: SQL status: OK in 3.880 seconds
[0m20:17:13.014735 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c0f-8ebc-143d-b8dc-08c1f5faeb18, command-id=01f10c0f-8ee3-1b00-ad01-518ee67cc1fc) - Closing
[0m20:17:13.016742 [debug] [ThreadPool]: On list_workspace: Close
[0m20:17:13.018750 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0f-8ebc-143d-b8dc-08c1f5faeb18) - Closing
[0m20:17:13.288510 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:17:13.290592 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:17:13.304379 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:17:13.304379 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:17:13.304379 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:17:16.380339 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0f-9118-19c5-8dd7-c075398fcd21) - Created
[0m20:17:17.400780 [debug] [ThreadPool]: SQL status: OK in 4.080 seconds
[0m20:17:17.400780 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c0f-9118-19c5-8dd7-c075398fcd21, command-id=01f10c0f-913f-1d48-a36d-ea0602dd55d9) - Closing
[0m20:17:17.400780 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:17:17.400780 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0f-9118-19c5-8dd7-c075398fcd21) - Closing
[0m20:17:17.680060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD887C61D0>]}
[0m20:17:17.685421 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_accounts
[0m20:17:17.685421 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_accounts ............................. [RUN]
[0m20:17:17.685421 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_accounts) - Creating connection
[0m20:17:17.685421 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_accounts'
[0m20:17:17.685421 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_accounts
[0m20:17:17.717454 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_accounts"
[0m20:17:17.718456 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_accounts
[0m20:17:17.763371 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:17:17.775509 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:17:17.775509 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA2F19C30>]}
[0m20:17:17.809278 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_accounts`
[0m20:17:17.836326 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_accounts"
[0m20:17:17.838350 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_accounts"
[0m20:17:17.839357 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_accounts: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_accounts"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_accounts`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_accounts

),

cleaned as (

    select
        account_id,
        account_name,
        industry,
        country,
        signup_date,
        referral_source,
        plan_tier,
        seats,
        is_trial,
        churn_flag,
        ingestion_ts

    from source

    where account_id is not null

)

select *
from cleaned
  )

[0m20:17:17.841654 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:17:21.004615 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10c0f-93da-1c42-85ee-997d2b0b4799) - Created
[0m20:17:22.093786 [debug] [Thread-3 (]: SQL status: OK in 4.250 seconds
[0m20:17:22.095794 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10c0f-93da-1c42-85ee-997d2b0b4799, command-id=01f10c0f-9403-1535-af2f-a8884efad486) - Closing
[0m20:17:22.118142 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:17:22.118142 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_accounts: Close
[0m20:17:22.118142 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10c0f-93da-1c42-85ee-997d2b0b4799) - Closing
[0m20:17:22.415662 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e4abcc2-3db5-4957-8ef8-245b9836fd6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8B30AF20>]}
[0m20:17:22.415662 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_accounts ........................ [[32mOK[0m in 4.73s]
[0m20:17:22.415662 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_accounts
[0m20:17:22.415662 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:17:22.415662 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:17:22.415662 [info ] [MainThread]: 
[0m20:17:22.428287 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 13.48 seconds (13.48s).
[0m20:17:22.431388 [debug] [MainThread]: Command end result
[0m20:17:22.510892 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:17:22.517962 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:17:22.525293 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:17:22.525293 [info ] [MainThread]: 
[0m20:17:22.525293 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:17:22.525293 [info ] [MainThread]: 
[0m20:17:22.525293 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:17:22.541159 [debug] [MainThread]: Command `dbt run` succeeded at 20:17:22.540128 after 21.17 seconds
[0m20:17:22.542169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD8DECEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA30C79A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADA30C67A0>]}
[0m20:17:22.574947 [debug] [MainThread]: Flushing usage events
[0m20:17:24.983783 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:17:45.797008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224491AEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244B4C0370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244B4C0190>]}


============================== 20:17:45.806483 | 011ab80f-3c63-46d8-9689-8607f603d904 ==============================
[0m20:17:45.806483 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:17:45.807984 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt test --select stg_accounts', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:17:48.305834 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:17:48.307858 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:17:48.308859 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:17:50.624650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244DAE16F0>]}
[0m20:17:50.786763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244B4953F0>]}
[0m20:17:50.789044 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:17:51.881751 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:17:51.884757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244AFF3340>]}
[0m20:17:51.924925 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:17:52.564663 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:17:52.565674 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:17:52.566700 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:17:52.697222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245F1BE050>]}
[0m20:17:52.977513 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:17:52.985342 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:17:53.048660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245F034100>]}
[0m20:17:53.051131 [info ] [MainThread]: Found 1 model, 4 data tests, 731 macros
[0m20:17:53.053159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245F082830>]}
[0m20:17:53.058360 [info ] [MainThread]: 
[0m20:17:53.060896 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:17:53.062910 [info ] [MainThread]: 
[0m20:17:53.065941 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:17:53.066960 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:17:53.072827 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:17:53.074844 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:17:53.114475 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:17:53.116912 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:17:53.117933 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:17:56.619721 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0f-a916-17b1-b39e-220953ede289) - Created
[0m20:17:57.198480 [debug] [ThreadPool]: SQL status: OK in 4.080 seconds
[0m20:17:57.214867 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c0f-a916-17b1-b39e-220953ede289, command-id=01f10c0f-a93c-1e6e-a3df-2a441dd34f9a) - Closing
[0m20:17:57.214867 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:17:57.214867 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c0f-a916-17b1-b39e-220953ede289) - Closing
[0m20:17:57.493300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '011ab80f-3c63-46d8-9689-8607f603d904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245F035690>]}
[0m20:17:57.509511 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:17:57.509511 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_accounts_account_id ............................. [RUN]
[0m20:17:57.513940 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m20:17:57.513940 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108'
[0m20:17:57.515947 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:17:57.553374 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:17:57.561144 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:17:57.603796 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:17:57.603796 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:17:57.603796 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:17:57.612263 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:18:00.900625 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-aba4-113e-b3a4-2119701a66b3) - Created
[0m20:18:02.261558 [debug] [Thread-2 (]: SQL status: OK in 4.650 seconds
[0m20:18:02.267090 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c0f-aba4-113e-b3a4-2119701a66b3, command-id=01f10c0f-abc8-1d92-ac20-e4d7261366f9) - Closing
[0m20:18:02.275437 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: Close
[0m20:18:02.275437 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-aba4-113e-b3a4-2119701a66b3) - Closing
[0m20:18:02.551903 [info ] [Thread-2 (]: 1 of 4 PASS not_null_stg_accounts_account_id ................................... [[32mPASS[0m in 5.04s]
[0m20:18:02.553908 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:18:02.553908 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:18:02.558721 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_accounts_seats .................................. [RUN]
[0m20:18:02.561964 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39) - Creating connection
[0m20:18:02.563974 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39'
[0m20:18:02.565986 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:18:02.585985 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:18:02.591270 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:18:02.604228 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:18:02.608243 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:18:02.610252 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select seats
from `workspace`.`analytics`.`stg_accounts`
where seats is null



  
  
      
    ) dbt_internal_test
[0m20:18:02.610252 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:18:06.234728 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-aed1-103e-9d9c-5a96587b330c) - Created
[0m20:18:07.116602 [debug] [Thread-2 (]: SQL status: OK in 4.510 seconds
[0m20:18:07.118060 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c0f-aed1-103e-9d9c-5a96587b330c, command-id=01f10c0f-aef8-1ac2-9e41-86f0f2477ce1) - Closing
[0m20:18:07.118060 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: Close
[0m20:18:07.118060 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-aed1-103e-9d9c-5a96587b330c) - Closing
[0m20:18:07.406011 [info ] [Thread-2 (]: 2 of 4 PASS not_null_stg_accounts_seats ........................................ [[32mPASS[0m in 4.84s]
[0m20:18:07.408020 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:18:07.410029 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:18:07.412038 [info ] [Thread-2 (]: 3 of 4 START test not_null_stg_accounts_signup_date ............................ [RUN]
[0m20:18:07.414114 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a) - Creating connection
[0m20:18:07.416118 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a'
[0m20:18:07.418789 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:18:07.436484 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:18:07.438947 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:18:07.438947 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:18:07.438947 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:18:07.451234 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signup_date
from `workspace`.`analytics`.`stg_accounts`
where signup_date is null



  
  
      
    ) dbt_internal_test
[0m20:18:07.451786 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:18:10.896743 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-b195-15d0-8ae7-771496aafc1f) - Created
[0m20:18:11.586684 [debug] [Thread-2 (]: SQL status: OK in 4.130 seconds
[0m20:18:11.588695 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c0f-b195-15d0-8ae7-771496aafc1f, command-id=01f10c0f-b1be-13cf-a8a7-570a4bd07a87) - Closing
[0m20:18:11.592256 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: Close
[0m20:18:11.594265 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-b195-15d0-8ae7-771496aafc1f) - Closing
[0m20:18:11.860820 [info ] [Thread-2 (]: 3 of 4 PASS not_null_stg_accounts_signup_date .................................. [[32mPASS[0m in 4.45s]
[0m20:18:11.873782 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:18:11.873782 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:18:11.878053 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_accounts_account_id ............................... [RUN]
[0m20:18:11.880577 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m20:18:11.881600 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79'
[0m20:18:11.882831 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:18:11.885577 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:18:11.904773 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:18:11.908305 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:18:11.908305 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:18:11.917309 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:18:11.919543 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:18:15.083606 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-b414-1081-bd85-183224d84f16) - Created
[0m20:18:17.828628 [debug] [Thread-2 (]: SQL status: OK in 5.910 seconds
[0m20:18:17.828628 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c0f-b414-1081-bd85-183224d84f16, command-id=01f10c0f-b43f-1be7-ab18-34b5683db3a6) - Closing
[0m20:18:17.828628 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: Close
[0m20:18:17.844643 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c0f-b414-1081-bd85-183224d84f16) - Closing
[0m20:18:18.146478 [info ] [Thread-2 (]: 4 of 4 PASS unique_stg_accounts_account_id ..................................... [[32mPASS[0m in 6.27s]
[0m20:18:18.146478 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:18:18.156391 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:18:18.158399 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:18:18.160406 [info ] [MainThread]: 
[0m20:18:18.163434 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 25.10 seconds (25.10s).
[0m20:18:18.168100 [debug] [MainThread]: Command end result
[0m20:18:18.260018 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:18:18.267419 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:18:18.286629 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:18:18.287960 [info ] [MainThread]: 
[0m20:18:18.289974 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:18:18.291987 [info ] [MainThread]: 
[0m20:18:18.294094 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m20:18:18.297182 [debug] [MainThread]: Command `dbt test` succeeded at 20:18:18.297182 after 32.74 seconds
[0m20:18:18.299840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224491AEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244B4953F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244A519390>]}
[0m20:18:18.301623 [debug] [MainThread]: Flushing usage events
[0m20:18:20.012827 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:32:58.137611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEDEC9EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEE0FBC3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEE0FBC1C0>]}


============================== 20:32:58.153536 | a7675eb7-9980-408b-a476-0c7fcf903ad2 ==============================
[0m20:32:58.153536 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:32:58.163299 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_subscriptions', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:33:02.492230 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:33:02.492230 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:33:02.492230 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:33:06.549413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEDFFBCDC0>]}
[0m20:33:06.825205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEE06C1C60>]}
[0m20:33:06.829233 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:33:08.908516 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:33:08.912544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEE0AE3E80>]}
[0m20:33:08.964871 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:33:10.033353 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m20:33:10.038393 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\silver\stg_subscriptions.sql
[0m20:33:10.040406 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\schema.yml
[0m20:33:11.381785 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_subscriptions'
in package 'saas_dbt_analytics' (models\silver\schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m20:33:11.381785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF4FCE800>]}
[0m20:33:11.726169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF510C130>]}
[0m20:33:12.147497 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:33:12.161945 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:33:12.225101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF511FAC0>]}
[0m20:33:12.225101 [info ] [MainThread]: Found 2 models, 8 data tests, 731 macros
[0m20:33:12.225101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF511F130>]}
[0m20:33:12.225101 [info ] [MainThread]: 
[0m20:33:12.243001 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:33:12.250774 [info ] [MainThread]: 
[0m20:33:12.254504 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:33:12.256524 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:33:12.267790 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:33:12.272748 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:33:12.323523 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:33:12.326128 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:33:12.326128 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:33:19.213563 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c11-ceea-1c8f-add2-5749a64bb795) - Created
[0m20:33:33.245862 [debug] [ThreadPool]: SQL status: OK in 20.920 seconds
[0m20:33:33.269104 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c11-ceea-1c8f-add2-5749a64bb795, command-id=01f10c11-cf27-1da2-a5a9-47c45eccdf59) - Closing
[0m20:33:33.651432 [debug] [ThreadPool]: On list_workspace: Close
[0m20:33:33.655436 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c11-ceea-1c8f-add2-5749a64bb795) - Closing
[0m20:33:34.001824 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:33:34.004914 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:33:34.031272 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:33:34.034400 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:33:34.037485 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:33:40.313568 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c11-db7b-132c-b18b-45103b25b060) - Created
[0m20:33:43.094544 [debug] [ThreadPool]: SQL status: OK in 9.060 seconds
[0m20:33:43.106146 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c11-db7b-132c-b18b-45103b25b060, command-id=01f10c11-dbec-13e1-8cab-04c60288f490) - Closing
[0m20:33:43.112278 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:33:43.115407 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c11-db7b-132c-b18b-45103b25b060) - Closing
[0m20:33:43.431790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF510D900>]}
[0m20:33:43.451230 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_subscriptions
[0m20:33:43.455480 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_subscriptions ........................ [RUN]
[0m20:33:43.461776 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_subscriptions) - Creating connection
[0m20:33:43.464902 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_subscriptions'
[0m20:33:43.468066 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_subscriptions
[0m20:33:43.546019 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_subscriptions"
[0m20:33:43.551281 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_subscriptions
[0m20:33:43.995279 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:33:44.006291 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:33:44.012675 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF4C9A680>]}
[0m20:33:44.113356 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_subscriptions`
[0m20:33:44.150770 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_subscriptions"
[0m20:33:44.158610 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_subscriptions"
[0m20:33:44.162874 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_subscriptions"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_subscriptions`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_subscriptions

),

cleaned as (

    select
        subscription_id,
        account_id,
        start_date,
        end_date,
        plan_tier,
        seats,
        mrr_amount,
        arr_amount,
        is_trial,
        upgrade_flag,
        downgrade_flag,
        churn_flag,
        billing_frequency,
        auto_renew_flag,
        ingestion_ts

    from source

    where subscription_id is not null
      and account_id is not null

)

select *
from cleaned
  )

[0m20:33:44.165893 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:33:50.140991 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10c11-e167-1176-898e-2b94ab8a5d53) - Created
[0m20:33:54.278153 [debug] [Thread-3 (]: SQL status: OK in 10.110 seconds
[0m20:33:54.282149 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10c11-e167-1176-898e-2b94ab8a5d53, command-id=01f10c11-e22c-16db-a620-6d8d6a6a9c08) - Closing
[0m20:33:54.327915 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:33:54.337326 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_subscriptions: Close
[0m20:33:54.340824 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10c11-e167-1176-898e-2b94ab8a5d53) - Closing
[0m20:33:55.278338 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7675eb7-9980-408b-a476-0c7fcf903ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEF51EF850>]}
[0m20:33:55.282340 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_subscriptions ................... [[32mOK[0m in 11.81s]
[0m20:33:55.287375 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_subscriptions
[0m20:33:55.291635 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:33:55.294715 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:33:55.298112 [info ] [MainThread]: 
[0m20:33:55.302135 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 43.05 seconds (43.05s).
[0m20:33:55.308139 [debug] [MainThread]: Command end result
[0m20:33:55.471671 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:33:55.486646 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:33:55.514675 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:33:55.517685 [info ] [MainThread]: 
[0m20:33:55.521890 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:33:55.525751 [info ] [MainThread]: 
[0m20:33:55.530377 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:33:55.536249 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m20:33:55.544802 [debug] [MainThread]: Command `dbt run` succeeded at 20:33:55.543794 after 57.96 seconds
[0m20:33:55.546802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEDEC9EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEE000F910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEDCE99F60>]}
[0m20:33:55.550178 [debug] [MainThread]: Flushing usage events
[0m20:33:59.356220 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:34:25.416958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024222A8EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024224DBC370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024224DBC190>]}


============================== 20:34:25.432202 | ed075333-bf7c-4de4-874c-22fdc38b1145 ==============================
[0m20:34:25.432202 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:34:25.435360 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt test --select stg_subscriptions', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:34:30.663584 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:34:30.669187 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:34:30.673220 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:34:36.317869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024220E9FE50>]}
[0m20:34:36.707054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024224DBEA70>]}
[0m20:34:36.715116 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:34:39.890891 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:34:39.892901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242248DB7F0>]}
[0m20:34:39.998145 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:34:41.293399 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:34:41.295409 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:34:41.298431 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:34:41.504969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024238AF4130>]}
[0m20:34:41.929381 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:34:41.949719 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:34:42.050332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024238A5C730>]}
[0m20:34:42.053680 [info ] [MainThread]: Found 2 models, 8 data tests, 731 macros
[0m20:34:42.059583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024238A5CA30>]}
[0m20:34:42.069009 [info ] [MainThread]: 
[0m20:34:42.076064 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:34:42.081110 [info ] [MainThread]: 
[0m20:34:42.088122 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:34:42.088122 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:34:42.142330 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:34:42.145357 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:34:42.207839 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:34:42.209850 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:34:42.214945 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:34:47.724454 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c12-03c3-12e4-b92b-bb7eb9d05954) - Created
[0m20:34:48.468427 [debug] [ThreadPool]: SQL status: OK in 6.250 seconds
[0m20:34:48.490031 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10c12-03c3-12e4-b92b-bb7eb9d05954, command-id=01f10c12-03ea-198c-bc7b-dd5808228461) - Closing
[0m20:34:48.495092 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:34:48.497108 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10c12-03c3-12e4-b92b-bb7eb9d05954) - Closing
[0m20:34:48.785600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed075333-bf7c-4de4-874c-22fdc38b1145', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024238A53D90>]}
[0m20:34:48.804844 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:34:48.807392 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_subscriptions_account_id ........................ [RUN]
[0m20:34:48.807392 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9) - Creating connection
[0m20:34:48.814407 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9'
[0m20:34:48.818930 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:34:48.893431 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:34:48.904885 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:34:48.988617 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:34:48.995644 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:34:49.002992 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_subscriptions`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:34:49.005032 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:34:56.705455 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-08a8-1b75-9339-1b184874d489) - Created
[0m20:34:58.555759 [debug] [Thread-2 (]: SQL status: OK in 9.550 seconds
[0m20:34:58.575070 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c12-08a8-1b75-9339-1b184874d489, command-id=01f10c12-0945-13c8-997f-38a34beb7c35) - Closing
[0m20:34:58.587453 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: Close
[0m20:34:58.600709 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-08a8-1b75-9339-1b184874d489) - Closing
[0m20:34:59.566366 [info ] [Thread-2 (]: 1 of 4 PASS not_null_stg_subscriptions_account_id .............................. [[32mPASS[0m in 10.76s]
[0m20:34:59.576413 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:34:59.580626 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:34:59.587138 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_subscriptions_subscription_id ................... [RUN]
[0m20:34:59.594229 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91) - Creating connection
[0m20:34:59.596246 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m20:34:59.603334 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:34:59.641122 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:34:59.644322 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:34:59.665949 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:34:59.680532 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:34:59.680532 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:34:59.687912 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:06.630012 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-0f01-19b8-bb36-fcc068a53738) - Created
[0m20:35:07.878697 [debug] [Thread-2 (]: SQL status: OK in 8.200 seconds
[0m20:35:07.898435 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c12-0f01-19b8-bb36-fcc068a53738, command-id=01f10c12-0f30-14cc-bb77-904e8f10fcd1) - Closing
[0m20:35:07.902922 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m20:35:07.905960 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-0f01-19b8-bb36-fcc068a53738) - Closing
[0m20:35:08.188912 [info ] [Thread-2 (]: 2 of 4 PASS not_null_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 8.60s]
[0m20:35:08.195965 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:35:08.195965 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:35:08.203334 [info ] [Thread-2 (]: 3 of 4 START test relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:35:08.203334 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90) - Creating connection
[0m20:35:08.210631 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90'
[0m20:35:08.210631 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:35:08.262527 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:35:08.270012 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:35:08.285809 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:35:08.295189 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:35:08.298510 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_subscriptions`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:35:08.302701 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:14.511294 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-13b2-15f9-8079-e8ad53f5bb18) - Created
[0m20:35:18.939841 [debug] [Thread-2 (]: SQL status: OK in 10.640 seconds
[0m20:35:18.958122 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c12-13b2-15f9-8079-e8ad53f5bb18, command-id=01f10c12-13e1-1d54-bd53-680159649629) - Closing
[0m20:35:18.965589 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: Close
[0m20:35:18.969623 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-13b2-15f9-8079-e8ad53f5bb18) - Closing
[0m20:35:19.331449 [info ] [Thread-2 (]: 3 of 4 PASS relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 11.13s]
[0m20:35:19.340753 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:35:19.340753 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:35:19.340753 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_subscriptions_subscription_id ..................... [RUN]
[0m20:35:19.361169 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e) - Creating connection
[0m20:35:19.368223 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m20:35:19.380185 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:35:19.458788 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:35:19.465041 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:35:19.481401 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:35:19.484501 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:35:19.492705 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:35:19.492705 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:25.886259 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-1a7a-10f8-bf68-f3ee7224ac59) - Created
[0m20:35:27.855709 [debug] [Thread-2 (]: SQL status: OK in 8.360 seconds
[0m20:35:27.864871 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10c12-1a7a-10f8-bf68-f3ee7224ac59, command-id=01f10c12-1aa9-1199-a5c7-e342e52c11cd) - Closing
[0m20:35:27.874034 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m20:35:27.878681 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10c12-1a7a-10f8-bf68-f3ee7224ac59) - Closing
[0m20:35:28.167972 [info ] [Thread-2 (]: 4 of 4 PASS unique_stg_subscriptions_subscription_id ........................... [[32mPASS[0m in 8.81s]
[0m20:35:28.175761 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:35:28.192211 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:35:28.195566 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:35:28.205153 [info ] [MainThread]: 
[0m20:35:28.211108 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 46.12 seconds (46.12s).
[0m20:35:28.221114 [debug] [MainThread]: Command end result
[0m20:35:28.392279 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:35:28.410813 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:35:28.438658 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:35:28.447790 [info ] [MainThread]: 
[0m20:35:28.447790 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:35:28.447790 [info ] [MainThread]: 
[0m20:35:28.458982 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m20:35:28.466200 [debug] [MainThread]: Command `dbt test` succeeded at 20:35:28.466200 after 63.43 seconds
[0m20:35:28.466200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024222A8EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024222DD1960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242228BFA30>]}
[0m20:35:28.466200 [debug] [MainThread]: Flushing usage events
[0m20:35:32.222554 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:52:10.773661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEDBBEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEFECC370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEFECC1C0>]}


============================== 19:52:10.802205 | c38ba085-fd5f-4df6-a40a-da2e8fc6536e ==============================
[0m19:52:10.802205 [info ] [MainThread]: Running with dbt=1.11.2
[0m19:52:10.809561 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt deps', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m19:52:11.506968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c38ba085-fd5f-4df6-a40a-da2e8fc6536e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEFE964A0>]}
[0m19:52:11.636792 [debug] [MainThread]: Set downloads directory='C:\Users\HP\AppData\Local\Temp\dbt-downloads-y442wvki'
[0m19:52:11.640323 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m19:52:13.986777 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m19:52:13.996762 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m19:52:16.125851 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m19:52:16.193606 [info ] [MainThread]: Updating lock file in file path: D:\DataScience\saas-databricks-dbt-analytics/package-lock.yml
[0m19:52:16.227584 [debug] [MainThread]: Set downloads directory='C:\Users\HP\AppData\Local\Temp\dbt-downloads-hmgk8ca2'
[0m19:52:16.247474 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m19:52:18.977488 [info ] [MainThread]: Installed from version 1.1.1
[0m19:52:18.978988 [info ] [MainThread]: Updated version available: 1.3.3
[0m19:52:18.981021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'c38ba085-fd5f-4df6-a40a-da2e8fc6536e', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEEF07730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DF015F910>]}
[0m19:52:18.983091 [info ] [MainThread]: 
[0m19:52:18.985235 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m19:52:19.006219 [debug] [MainThread]: Command `dbt deps` succeeded at 19:52:18.992774 after 8.80 seconds
[0m19:52:19.009222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEDBBEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DEEF07730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DF00420B0>]}
[0m19:52:19.012608 [debug] [MainThread]: Flushing usage events
[0m19:52:21.752218 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:56:24.878154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8351AEB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8374D0370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8374D01C0>]}


============================== 19:56:24.892771 | 8cad52e2-978f-4050-88df-c6c0eefc613c ==============================
[0m19:56:24.892771 [info ] [MainThread]: Running with dbt=1.11.2
[0m19:56:24.897097 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_feature_usage', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m19:56:29.146658 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m19:56:29.146658 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m19:56:29.162544 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m19:56:32.769784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B836E4BF70>]}
[0m19:56:33.003280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B839EC4AC0>]}
[0m19:56:33.007299 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m19:56:34.527047 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:56:34.527047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B836FDEA70>]}
[0m19:56:34.631265 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m19:56:35.056909 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m19:56:35.058654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B836BF7310>]}
[0m19:56:42.928299 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_subscriptions'
in package 'saas_dbt_analytics' (models\silver\schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m19:56:42.928299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B1D0FA0>]}
[0m19:56:43.430457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B420130>]}
[0m19:56:43.794840 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m19:56:43.829785 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m19:56:43.884614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B41F880>]}
[0m19:56:43.886621 [info ] [MainThread]: Found 3 models, 11 data tests, 845 macros
[0m19:56:43.892384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B423310>]}
[0m19:56:43.900418 [info ] [MainThread]: 
[0m19:56:43.902426 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:56:43.904969 [info ] [MainThread]: 
[0m19:56:43.908545 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:56:43.913028 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:56:43.921147 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m19:56:43.922890 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m19:56:43.966438 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m19:56:43.966438 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m19:56:43.966438 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:56:48.823352 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd5-e02b-1a67-a9a9-43ea8fdb2f24) - Created
[0m19:57:05.419925 [debug] [ThreadPool]: SQL status: OK in 21.450 seconds
[0m19:57:05.447240 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd5-e02b-1a67-a9a9-43ea8fdb2f24, command-id=01f10cd5-e06c-15b4-87dd-4a5b8960d9b8) - Closing
[0m19:57:05.919594 [debug] [ThreadPool]: On list_workspace: Close
[0m19:57:05.919594 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd5-e02b-1a67-a9a9-43ea8fdb2f24) - Closing
[0m19:57:07.597221 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m19:57:07.597221 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m19:57:07.623866 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m19:57:07.625875 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m19:57:07.627882 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:57:13.818219 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd5-ef20-126d-b358-352d29695b7c) - Created
[0m19:57:16.808190 [debug] [ThreadPool]: SQL status: OK in 9.180 seconds
[0m19:57:16.816454 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd5-ef20-126d-b358-352d29695b7c, command-id=01f10cd5-ef50-1757-bfd0-73fdf73ae23d) - Closing
[0m19:57:16.821043 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m19:57:16.827057 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd5-ef20-126d-b358-352d29695b7c) - Closing
[0m19:57:17.810085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B41F280>]}
[0m19:57:17.849939 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_feature_usage
[0m19:57:17.857609 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_feature_usage ........................ [RUN]
[0m19:57:17.857609 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_feature_usage) - Creating connection
[0m19:57:17.857609 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_feature_usage'
[0m19:57:17.857609 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_feature_usage
[0m19:57:17.927464 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_feature_usage"
[0m19:57:17.932060 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_feature_usage
[0m19:57:18.030257 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m19:57:18.038140 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m19:57:18.046232 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B657310>]}
[0m19:57:18.130280 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_feature_usage`
[0m19:57:18.178565 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_feature_usage"
[0m19:57:18.178565 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_feature_usage"
[0m19:57:18.184504 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_feature_usage: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_feature_usage"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_feature_usage`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_feature_usage
    where subscription_id is not null

),

aggregated as (

    select
        subscription_id,
        usage_date,
        feature_name,

        sum(usage_count)              as total_usage_count,
        sum(usage_duration_secs)      as total_usage_duration_secs,
        sum(error_count)              as total_error_count,

        max(is_beta_feature)          as is_beta_feature

    from source
    group by
        subscription_id,
        usage_date,
        feature_name

)

select *
from aggregated
  )

[0m19:57:18.184504 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:57:23.127614 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd5-f4ae-1366-9098-3bd08a52c948) - Created
[0m19:57:27.026920 [debug] [Thread-3 (]: SQL status: OK in 8.840 seconds
[0m19:57:27.026920 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10cd5-f4ae-1366-9098-3bd08a52c948, command-id=01f10cd5-f4dc-1fc3-bf67-7fb6d7ced3d3) - Closing
[0m19:57:27.069125 [debug] [Thread-3 (]: Applying tags to relation None
[0m19:57:27.076765 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_feature_usage: Close
[0m19:57:27.078771 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd5-f4ae-1366-9098-3bd08a52c948) - Closing
[0m19:57:27.361541 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8cad52e2-978f-4050-88df-c6c0eefc613c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8335BAE90>]}
[0m19:57:27.363554 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_feature_usage ................... [[32mOK[0m in 9.49s]
[0m19:57:27.369899 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_feature_usage
[0m19:57:27.377558 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m19:57:27.377558 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:57:27.381932 [info ] [MainThread]: 
[0m19:57:27.388340 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 43.47 seconds (43.47s).
[0m19:57:27.394254 [debug] [MainThread]: Command end result
[0m19:57:27.585644 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m19:57:27.585644 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m19:57:27.617360 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m19:57:27.617360 [info ] [MainThread]: 
[0m19:57:27.617360 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:57:27.617360 [info ] [MainThread]: 
[0m19:57:27.617360 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m19:57:27.633661 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 3 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m19:57:27.637640 [debug] [MainThread]: Command `dbt run` succeeded at 19:57:27.637640 after 63.14 seconds
[0m19:57:27.642120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8351AEB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B84B1CA500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8364C2A40>]}
[0m19:57:27.645080 [debug] [MainThread]: Flushing usage events
[0m19:57:32.754198 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:00:54.974578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204FF9BEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020481E803A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020481E801F0>]}


============================== 20:00:54.992347 | 5cd8634b-16fc-46fc-96e1-3a8c6d143e48 ==============================
[0m20:00:54.992347 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:00:54.992347 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt test --select stg_feature_usage', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:00:58.221334 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:00:58.237335 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:00:58.237335 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:01:00.914153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020481E80460>]}
[0m20:01:01.141623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048470FE80>]}
[0m20:01:01.145697 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:01:02.712283 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:01:02.712283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048189BD60>]}
[0m20:01:02.784099 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:01:03.621996 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:01:03.621996 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:01:03.621996 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:01:03.805114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020495C20130>]}
[0m20:01:04.151249 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:01:04.160481 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:01:04.224594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020495A43DF0>]}
[0m20:01:04.226340 [info ] [MainThread]: Found 3 models, 11 data tests, 845 macros
[0m20:01:04.229016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020495A43880>]}
[0m20:01:04.231025 [info ] [MainThread]: 
[0m20:01:04.231025 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:01:04.237580 [info ] [MainThread]: 
[0m20:01:04.237580 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:01:04.243298 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:01:04.280937 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:01:04.282946 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:01:04.322114 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:01:04.322114 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:01:04.328559 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:09.747072 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd6-7bc2-13a0-b594-425e2bf90c54) - Created
[0m20:01:11.602687 [debug] [ThreadPool]: SQL status: OK in 7.260 seconds
[0m20:01:11.622085 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd6-7bc2-13a0-b594-425e2bf90c54, command-id=01f10cd6-7bf1-16dc-b432-0f44325941d3) - Closing
[0m20:01:11.622085 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:01:11.622085 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd6-7bc2-13a0-b594-425e2bf90c54) - Closing
[0m20:01:12.099899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5cd8634b-16fc-46fc-96e1-3a8c6d143e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020495A41BD0>]}
[0m20:01:12.115734 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:01:12.124139 [info ] [Thread-2 (]: 1 of 3 START test dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [RUN]
[0m20:01:12.131740 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77) - Creating connection
[0m20:01:12.131740 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77'
[0m20:01:12.131740 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:01:12.212189 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:01:12.218772 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:01:12.274331 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:01:12.290455 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:01:12.290455 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  





with validation_errors as (

    select
        subscription_id, usage_date, feature_name
    from `workspace`.`analytics`.`stg_feature_usage`
    group by subscription_id, usage_date, feature_name
    having count(*) > 1

)

select *
from validation_errors



  
  
      
    ) dbt_internal_test
[0m20:01:12.290455 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:01:17.523896 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd6-7fe6-12f1-89a8-96b12cbfb0ca) - Created
[0m20:01:20.730311 [debug] [Thread-2 (]: SQL status: OK in 8.440 seconds
[0m20:01:20.741079 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd6-7fe6-12f1-89a8-96b12cbfb0ca, command-id=01f10cd6-8092-15f7-b3da-730f2b9d431b) - Closing
[0m20:01:20.744941 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: Close
[0m20:01:20.756982 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd6-7fe6-12f1-89a8-96b12cbfb0ca) - Closing
[0m20:01:21.217363 [info ] [Thread-2 (]: 1 of 3 PASS dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [[32mPASS[0m in 9.09s]
[0m20:01:21.217363 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:01:21.217363 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:01:21.217363 [info ] [Thread-2 (]: 2 of 3 START test not_null_stg_feature_usage_subscription_id ................... [RUN]
[0m20:01:21.231337 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3) - Creating connection
[0m20:01:21.235424 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3'
[0m20:01:21.238808 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:01:21.292476 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:01:21.293201 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:01:21.303846 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:01:21.309493 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:01:21.312292 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_feature_usage`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:01:21.312292 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:01:25.924265 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd6-8565-156b-a4f3-ddb0f0fce13d) - Created
[0m20:01:27.056003 [debug] [Thread-2 (]: SQL status: OK in 5.740 seconds
[0m20:01:27.061909 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd6-8565-156b-a4f3-ddb0f0fce13d, command-id=01f10cd6-8594-19b3-a1f9-7a172c6b6a5d) - Closing
[0m20:01:27.063920 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: Close
[0m20:01:27.065930 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd6-8565-156b-a4f3-ddb0f0fce13d) - Closing
[0m20:01:27.467184 [info ] [Thread-2 (]: 2 of 3 PASS not_null_stg_feature_usage_subscription_id ......................... [[32mPASS[0m in 6.24s]
[0m20:01:27.473201 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:01:27.476732 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:01:27.480983 [info ] [Thread-2 (]: 3 of 3 START test relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [RUN]
[0m20:01:27.484129 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850) - Creating connection
[0m20:01:27.487194 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850'
[0m20:01:27.489202 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:01:27.525662 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:01:27.525662 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:01:27.547673 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:01:27.549679 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:01:27.552392 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select subscription_id as from_field
    from `workspace`.`analytics`.`stg_feature_usage`
    where subscription_id is not null
),

parent as (
    select subscription_id as to_field
    from `workspace`.`analytics`.`stg_subscriptions`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:01:27.552392 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:01:32.170052 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd6-891f-171a-8af3-356cb43c13c7) - Created
[0m20:01:37.706369 [debug] [Thread-2 (]: SQL status: OK in 10.150 seconds
[0m20:01:37.710395 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd6-891f-171a-8af3-356cb43c13c7, command-id=01f10cd6-894d-1e3f-98c1-f5ac7d4b2e19) - Closing
[0m20:01:37.714773 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: Close
[0m20:01:37.718876 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd6-891f-171a-8af3-356cb43c13c7) - Closing
[0m20:01:38.056090 [info ] [Thread-2 (]: 3 of 3 PASS relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [[32mPASS[0m in 10.57s]
[0m20:01:38.060197 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:01:38.064679 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:01:38.068761 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:01:38.068761 [info ] [MainThread]: 
[0m20:01:38.074926 [info ] [MainThread]: Finished running 3 data tests in 0 hours 0 minutes and 33.83 seconds (33.83s).
[0m20:01:38.080956 [debug] [MainThread]: Command end result
[0m20:01:38.259220 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:01:38.269060 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:01:38.294905 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:01:38.294905 [info ] [MainThread]: 
[0m20:01:38.299092 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:01:38.299092 [info ] [MainThread]: 
[0m20:01:38.303346 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m20:01:38.307431 [debug] [MainThread]: Command `dbt test` succeeded at 20:01:38.307431 after 43.62 seconds
[0m20:01:38.307431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204FF9BEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048470FE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020495C04D30>]}
[0m20:01:38.311469 [debug] [MainThread]: Flushing usage events
[0m20:01:40.770587 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:09:48.817527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BB96AEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBB9C03A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBB9C01F0>]}


============================== 20:09:48.817527 | af6587a2-21f4-4203-8626-69ccc8a8f436 ==============================
[0m20:09:48.817527 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:09:48.817527 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_churn_events', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:09:51.984785 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:09:51.984785 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:09:51.984785 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:09:54.771857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBB9C0460>]}
[0m20:09:55.000479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BCEC2B730>]}
[0m20:09:55.000479 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:09:56.608509 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:09:56.608509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBB4DC700>]}
[0m20:09:56.704233 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:09:57.614024 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m20:09:57.614024 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\silver\stg_churn_events.sql
[0m20:09:57.618050 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\schema.yml
[0m20:09:58.839153 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_churn_events'
in package 'saas_dbt_analytics' (models\silver\schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m20:09:58.839153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BCFABEFE0>]}
[0m20:09:59.237970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BCFB14130>]}
[0m20:09:59.799040 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:09:59.804292 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:09:59.854442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BCF99D510>]}
[0m20:09:59.854442 [info ] [MainThread]: Found 4 models, 15 data tests, 845 macros
[0m20:09:59.854442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BCF902F80>]}
[0m20:09:59.863814 [info ] [MainThread]: 
[0m20:09:59.865337 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:09:59.865337 [info ] [MainThread]: 
[0m20:09:59.865337 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:09:59.871978 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:09:59.878819 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:09:59.880207 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:09:59.922152 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:09:59.922152 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:09:59.922152 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:10:04.451653 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd7-ba82-114b-819a-0e4fc2ad35c6) - Created
[0m20:10:05.090597 [debug] [ThreadPool]: SQL status: OK in 5.170 seconds
[0m20:10:05.114843 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd7-ba82-114b-819a-0e4fc2ad35c6, command-id=01f10cd7-baa8-16c7-8e44-7fe951c123a8) - Closing
[0m20:10:05.121114 [debug] [ThreadPool]: On list_workspace: Close
[0m20:10:05.122122 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd7-ba82-114b-819a-0e4fc2ad35c6) - Closing
[0m20:10:05.433920 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:10:05.437731 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:10:05.461559 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:10:05.463568 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:10:05.463568 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:10:09.793562 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd7-bda5-1015-a859-0b5b49fc2bb0) - Created
[0m20:10:10.549727 [debug] [ThreadPool]: SQL status: OK in 5.080 seconds
[0m20:10:10.561794 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd7-bda5-1015-a859-0b5b49fc2bb0, command-id=01f10cd7-bdd6-1cb7-8bc2-f325a4979d3d) - Closing
[0m20:10:10.565556 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:10:10.567567 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd7-bda5-1015-a859-0b5b49fc2bb0) - Closing
[0m20:10:10.852397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BCF841330>]}
[0m20:10:10.858658 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_churn_events
[0m20:10:10.858658 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_churn_events ......................... [RUN]
[0m20:10:10.876583 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_churn_events) - Creating connection
[0m20:10:10.876583 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_churn_events'
[0m20:10:10.876583 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_churn_events
[0m20:10:10.919410 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_churn_events"
[0m20:10:10.930464 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_churn_events
[0m20:10:10.986661 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:10:10.986661 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:10:10.994391 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBE3BB070>]}
[0m20:10:11.036618 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_churn_events`
[0m20:10:11.071280 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_churn_events"
[0m20:10:11.073288 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_churn_events"
[0m20:10:11.075297 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_churn_events: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_churn_events"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_churn_events`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_churn_events

),

cleaned as (

    select
        churn_event_id,
        account_id,
        churn_date,
        reason_code,
        refund_amount_usd,
        preceding_upgrade_flag,
        preceding_downgrade_flag,
        is_reactivation,
        feedback_text,
        ingestion_ts

    from source

    where churn_event_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:10:11.076812 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:10:15.443785 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd7-c106-18e2-8775-fa3f33f3375b) - Created
[0m20:10:16.884337 [debug] [Thread-3 (]: SQL status: OK in 5.810 seconds
[0m20:10:16.890371 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10cd7-c106-18e2-8775-fa3f33f3375b, command-id=01f10cd7-c135-160b-8da8-c0d68afedbf5) - Closing
[0m20:10:16.954255 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:10:16.964025 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_churn_events: Close
[0m20:10:16.968037 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd7-c106-18e2-8775-fa3f33f3375b) - Closing
[0m20:10:17.297706 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af6587a2-21f4-4203-8626-69ccc8a8f436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BB7ABAF20>]}
[0m20:10:17.297706 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_churn_events .................... [[32mOK[0m in 6.41s]
[0m20:10:17.307750 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_churn_events
[0m20:10:17.312587 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:10:17.318669 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:10:17.320771 [info ] [MainThread]: 
[0m20:10:17.324075 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 17.46 seconds (17.46s).
[0m20:10:17.334831 [debug] [MainThread]: Command end result
[0m20:10:17.484939 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:10:17.501106 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:10:17.516210 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:10:17.524315 [info ] [MainThread]: 
[0m20:10:17.524315 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:10:17.524315 [info ] [MainThread]: 
[0m20:10:17.531418 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:10:17.535290 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m20:10:17.541344 [debug] [MainThread]: Command `dbt run` succeeded at 20:10:17.540743 after 28.98 seconds
[0m20:10:17.544525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BB96AEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBDFAFD00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BBDFAFD60>]}
[0m20:10:17.544525 [debug] [MainThread]: Flushing usage events
[0m20:10:20.356697 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:10:52.351933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB83BCABF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB85EEC3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB85EEC1F0>]}


============================== 20:10:52.364071 | 735c705a-0114-48f7-b9e6-15fbf044a604 ==============================
[0m20:10:52.364071 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:10:52.364071 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt test --select stg_churn_events', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:10:55.900432 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:10:55.900432 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:10:55.900432 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:10:59.285679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB85EEC460>]}
[0m20:10:59.550915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB85EEE0B0>]}
[0m20:10:59.550915 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:11:01.273010 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:11:01.273010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB859F9120>]}
[0m20:11:01.354454 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:11:02.395643 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:11:02.395643 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:11:02.395643 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:11:02.630400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB99C850C0>]}
[0m20:11:03.048602 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:11:03.057657 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:11:03.139237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB99C87220>]}
[0m20:11:03.139237 [info ] [MainThread]: Found 4 models, 15 data tests, 845 macros
[0m20:11:03.139237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB99C86F80>]}
[0m20:11:03.151221 [info ] [MainThread]: 
[0m20:11:03.151221 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:11:03.159519 [info ] [MainThread]: 
[0m20:11:03.163622 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:11:03.165630 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:11:03.202319 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:11:03.206914 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:11:03.266656 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:11:03.269743 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:11:03.270795 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:11:08.385994 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd7-e093-1b48-8306-29c519b57bdc) - Created
[0m20:11:10.042059 [debug] [ThreadPool]: SQL status: OK in 6.770 seconds
[0m20:11:10.063658 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd7-e093-1b48-8306-29c519b57bdc, command-id=01f10cd7-e0c2-1b69-948f-362489930d03) - Closing
[0m20:11:10.063658 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:11:10.071823 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd7-e093-1b48-8306-29c519b57bdc) - Closing
[0m20:11:10.376762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '735c705a-0114-48f7-b9e6-15fbf044a604', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB99C65FF0>]}
[0m20:11:10.397297 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:11:10.401324 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_churn_events_account_id ......................... [RUN]
[0m20:11:10.407389 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d) - Creating connection
[0m20:11:10.407389 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d'
[0m20:11:10.407389 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:11:10.478825 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:11:10.489315 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:11:10.577832 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:11:10.585052 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:11:10.585052 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_churn_events`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:11:10.585052 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:11:15.353445 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-e4ba-1660-abd0-ecaf46d2c4e1) - Created
[0m20:11:16.426860 [debug] [Thread-2 (]: SQL status: OK in 5.840 seconds
[0m20:11:16.437250 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd7-e4ba-1660-abd0-ecaf46d2c4e1, command-id=01f10cd7-e4eb-12c9-8266-6221bd2dc5d0) - Closing
[0m20:11:16.457593 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: Close
[0m20:11:16.457593 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-e4ba-1660-abd0-ecaf46d2c4e1) - Closing
[0m20:11:16.720032 [info ] [Thread-2 (]: 1 of 4 PASS not_null_stg_churn_events_account_id ............................... [[32mPASS[0m in 6.31s]
[0m20:11:16.728385 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:11:16.728385 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:11:16.738588 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_churn_events_churn_event_id ..................... [RUN]
[0m20:11:16.744404 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc) - Creating connection
[0m20:11:16.750978 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc'
[0m20:11:16.761260 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:11:16.781454 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:11:16.789745 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:11:16.800251 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:11:16.800251 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:11:16.800251 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select churn_event_id
from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is null



  
  
      
    ) dbt_internal_test
[0m20:11:16.800251 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:11:21.658145 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-e87f-1a82-a5d5-fd5db0d8deaa) - Created
[0m20:11:22.709562 [debug] [Thread-2 (]: SQL status: OK in 5.910 seconds
[0m20:11:22.709562 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd7-e87f-1a82-a5d5-fd5db0d8deaa, command-id=01f10cd7-e8ac-1ec8-b0bf-70e0d3496901) - Closing
[0m20:11:22.709562 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: Close
[0m20:11:22.721470 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-e87f-1a82-a5d5-fd5db0d8deaa) - Closing
[0m20:11:23.029269 [info ] [Thread-2 (]: 2 of 4 PASS not_null_stg_churn_events_churn_event_id ........................... [[32mPASS[0m in 6.28s]
[0m20:11:23.029269 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:11:23.045501 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:11:23.045501 [info ] [Thread-2 (]: 3 of 4 START test relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:11:23.045501 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b) - Creating connection
[0m20:11:23.061524 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b'
[0m20:11:23.061524 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:11:23.126590 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:11:23.126590 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:11:23.143328 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:11:23.155988 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:11:23.158777 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_churn_events`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:11:23.160131 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:11:28.050356 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-ec4b-1830-b4cb-e660aae1b46b) - Created
[0m20:11:30.653932 [debug] [Thread-2 (]: SQL status: OK in 7.490 seconds
[0m20:11:30.675662 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd7-ec4b-1830-b4cb-e660aae1b46b, command-id=01f10cd7-ec7a-1fa5-b0db-374fdbae777a) - Closing
[0m20:11:30.679896 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: Close
[0m20:11:30.679896 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-ec4b-1830-b4cb-e660aae1b46b) - Closing
[0m20:11:30.956605 [info ] [Thread-2 (]: 3 of 4 PASS relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 7.91s]
[0m20:11:30.956605 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:11:30.956605 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:11:30.972320 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_churn_events_churn_event_id ....................... [RUN]
[0m20:11:30.974226 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947) - Creating connection
[0m20:11:30.974226 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947'
[0m20:11:30.980526 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:11:31.031912 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:11:31.035928 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:11:31.041670 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:11:31.041670 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:11:31.055091 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    churn_event_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is not null
group by churn_event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:11:31.058278 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:11:36.256578 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-f139-1d6b-bd6b-6149963b91ea) - Created
[0m20:11:37.258986 [debug] [Thread-2 (]: SQL status: OK in 6.200 seconds
[0m20:11:37.274984 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd7-f139-1d6b-bd6b-6149963b91ea, command-id=01f10cd7-f15f-1b9c-8998-8ee1f5099238) - Closing
[0m20:11:37.274984 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: Close
[0m20:11:37.274984 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd7-f139-1d6b-bd6b-6149963b91ea) - Closing
[0m20:11:37.562249 [info ] [Thread-2 (]: 4 of 4 PASS unique_stg_churn_events_churn_event_id ............................. [[32mPASS[0m in 6.59s]
[0m20:11:37.562249 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:11:37.578341 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:11:37.580694 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:11:37.582702 [info ] [MainThread]: 
[0m20:11:37.587271 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 34.42 seconds (34.42s).
[0m20:11:37.589279 [debug] [MainThread]: Command end result
[0m20:11:37.802391 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:11:37.802391 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:11:37.843196 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:11:37.843196 [info ] [MainThread]: 
[0m20:11:37.843196 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:11:37.850237 [info ] [MainThread]: 
[0m20:11:37.854251 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m20:11:37.854251 [debug] [MainThread]: Command `dbt test` succeeded at 20:11:37.854251 after 45.82 seconds
[0m20:11:37.859886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB83BCABF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB85EEE0B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB99903DC0>]}
[0m20:11:37.861896 [debug] [MainThread]: Flushing usage events
[0m20:11:41.051089 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:15:28.097352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B23BEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B46DC3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B46DC1F0>]}


============================== 20:15:28.107491 | 6d379967-da85-49c2-9669-b8a7b2d067d5 ==============================
[0m20:15:28.107491 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:15:28.113232 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select stg_support_tickets', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:15:31.316805 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:15:31.316805 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:15:31.324710 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:15:34.023659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B46DC460>]}
[0m20:15:34.232381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B6F57EB0>]}
[0m20:15:34.232381 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:15:35.732125 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:15:35.732125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B4164D00>]}
[0m20:15:35.798322 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:15:36.640154 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m20:15:36.649683 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\silver\stg_support_tickets.sql
[0m20:15:36.649683 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\schema.yml
[0m20:15:37.758588 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on
'stg_support_tickets' in package 'saas_dbt_analytics'
(models\silver\schema.yml). Arguments to generic tests should be nested under
the `arguments` property.
[0m20:15:37.758588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9C87EAE60>]}
[0m20:15:38.059713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9C8854E50>]}
[0m20:15:38.561324 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:15:38.561324 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:15:38.611647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9C85EC340>]}
[0m20:15:38.611647 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:15:38.615764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9C85EE4A0>]}
[0m20:15:38.621505 [info ] [MainThread]: 
[0m20:15:38.621505 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:15:38.625954 [info ] [MainThread]: 
[0m20:15:38.626975 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:15:38.626975 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:15:38.630675 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:15:38.630675 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:15:38.680026 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:15:38.682034 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:15:38.682034 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:15:43.441701 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-8487-19d1-9021-71af68136ee3) - Created
[0m20:15:44.201940 [debug] [ThreadPool]: SQL status: OK in 5.520 seconds
[0m20:15:44.221549 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd8-8487-19d1-9021-71af68136ee3, command-id=01f10cd8-84b7-14b7-ba5c-ab3d124405a1) - Closing
[0m20:15:44.233592 [debug] [ThreadPool]: On list_workspace: Close
[0m20:15:44.233592 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-8487-19d1-9021-71af68136ee3) - Closing
[0m20:15:44.613532 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:15:44.613532 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:15:44.633848 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:15:44.635857 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:15:44.637867 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:15:49.306871 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-880d-1bd5-b227-6e90a759100a) - Created
[0m20:15:51.062032 [debug] [ThreadPool]: SQL status: OK in 6.420 seconds
[0m20:15:51.089756 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd8-880d-1bd5-b227-6e90a759100a, command-id=01f10cd8-8833-1a17-8350-4f142a8b6944) - Closing
[0m20:15:51.095525 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:15:51.099549 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-880d-1bd5-b227-6e90a759100a) - Closing
[0m20:15:51.430679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9C860ED40>]}
[0m20:15:51.437456 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:15:51.448163 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_support_tickets ...................... [RUN]
[0m20:15:51.457230 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:15:51.457230 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:15:51.457230 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:15:51.506885 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:15:51.506885 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:15:51.577729 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:15:51.593391 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:15:51.593391 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B4637E80>]}
[0m20:15:51.649033 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:15:51.673752 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:15:51.693527 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:15:51.695536 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_churn_events

),

cleaned as (

    select
        ticket_id
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:15:51.695536 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:15:56.851131 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd8-8c84-15de-8088-ec7b6b712268) - Created
[0m20:15:58.687165 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_churn_events

),

cleaned as (

    select
        ticket_id
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `ingestion_ts`, `churn_date`, `churn_event_id`, `reason_code`]. SQLSTATE: 42703; line 31 pos 10
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `ingestion_ts`, `churn_date`, `churn_event_id`, `reason_code`]. SQLSTATE: 42703; line 31 pos 10
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `ingestion_ts`, `churn_date`, `churn_event_id`, `reason_code`]. SQLSTATE: 42703; line 31 pos 10
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd8-8cb3-1f53-ab11-a9e770809385
[0m20:15:58.687165 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:15:58.700508 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd8-8c84-15de-8088-ec7b6b712268) - Closing
[0m20:15:59.031094 [debug] [Thread-3 (]: Database Error in model stg_support_tickets (models\silver\stg_support_tickets.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `ingestion_ts`, `churn_date`, `churn_event_id`, `reason_code`]. SQLSTATE: 42703; line 31 pos 10
  compiled code at target\run\saas_dbt_analytics\models\silver\stg_support_tickets.sql
[0m20:15:59.046924 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6d379967-da85-49c2-9669-b8a7b2d067d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B3DC5DE0>]}
[0m20:15:59.046924 [error] [Thread-3 (]: 1 of 1 ERROR creating sql view model analytics.stg_support_tickets ............. [[31mERROR[0m in 7.58s]
[0m20:15:59.046924 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:15:59.046924 [debug] [Thread-6 (]: Marking all children of 'model.saas_dbt_analytics.stg_support_tickets' to be skipped because of status 'error'.  Reason: Database Error in model stg_support_tickets (models\silver\stg_support_tickets.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `ingestion_ts`, `churn_date`, `churn_event_id`, `reason_code`]. SQLSTATE: 42703; line 31 pos 10
  compiled code at target\run\saas_dbt_analytics\models\silver\stg_support_tickets.sql.
[0m20:15:59.064836 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:15:59.073105 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:15:59.073105 [info ] [MainThread]: 
[0m20:15:59.073105 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 20.45 seconds (20.45s).
[0m20:15:59.073105 [debug] [MainThread]: Command end result
[0m20:15:59.240379 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:15:59.250685 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:15:59.272328 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:15:59.274337 [info ] [MainThread]: 
[0m20:15:59.276346 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m20:15:59.276346 [info ] [MainThread]: 
[0m20:15:59.280618 [error] [MainThread]: [31mFailure in model stg_support_tickets (models\silver\stg_support_tickets.sql)[0m
[0m20:15:59.286069 [error] [MainThread]:   Database Error in model stg_support_tickets (models\silver\stg_support_tickets.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `ingestion_ts`, `churn_date`, `churn_event_id`, `reason_code`]. SQLSTATE: 42703; line 31 pos 10
  compiled code at target\run\saas_dbt_analytics\models\silver\stg_support_tickets.sql
[0m20:15:59.290084 [info ] [MainThread]: 
[0m20:15:59.292094 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\stg_support_tickets.sql
[0m20:15:59.294103 [info ] [MainThread]: 
[0m20:15:59.298122 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m20:15:59.300132 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m20:15:59.306231 [debug] [MainThread]: Command `dbt run` failed at 20:15:59.304152 after 31.48 seconds
[0m20:15:59.308242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B23BEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9C80D4250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B46A5F30>]}
[0m20:15:59.310488 [debug] [MainThread]: Flushing usage events
[0m20:16:02.584470 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:16:25.120227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE6688EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE68BA0370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE68BA01C0>]}


============================== 20:16:25.135430 | 36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc ==============================
[0m20:16:25.135430 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:16:25.135430 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt test --select stg_support_tickets', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:16:28.300756 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:16:28.300756 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:16:28.300756 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:16:31.083178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE68BA0430>]}
[0m20:16:31.298926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE6B59CC10>]}
[0m20:16:31.300931 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:16:32.818109 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:16:32.821221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE6852B4F0>]}
[0m20:16:32.885398 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:16:33.823032 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:16:33.825804 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:16:33.825804 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:16:34.016275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE7C986410>]}
[0m20:16:34.397560 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:16:34.407606 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:16:34.482004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE7C905630>]}
[0m20:16:34.484013 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:16:34.486019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE7C7CA920>]}
[0m20:16:34.488949 [info ] [MainThread]: 
[0m20:16:34.495717 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:16:34.499037 [info ] [MainThread]: 
[0m20:16:34.502713 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:16:34.504722 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:16:34.538086 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:16:34.538086 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:16:34.592125 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:16:34.594362 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:16:34.596370 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:16:40.067165 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-a636-1667-bc53-ede6e3357308) - Created
[0m20:16:40.808429 [debug] [ThreadPool]: SQL status: OK in 6.210 seconds
[0m20:16:40.826699 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd8-a636-1667-bc53-ede6e3357308, command-id=01f10cd8-a675-1012-9dfa-edc8fc66e1b7) - Closing
[0m20:16:40.831168 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:16:40.833006 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-a636-1667-bc53-ede6e3357308) - Closing
[0m20:16:41.098883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36c55d8e-b0b4-47b8-ac0d-02f0e0f9d2cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE7C8D3490>]}
[0m20:16:41.127857 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:16:41.131997 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_support_tickets_account_id ...................... [RUN]
[0m20:16:41.131997 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:16:41.131997 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:16:41.143765 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:16:41.267767 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:16:41.267767 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:16:41.365416 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:16:41.365416 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:16:41.381230 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:16:41.383673 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:16:46.236944 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-a9fc-136f-b720-bb7532ae63a0) - Created
[0m20:16:47.037495 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd8-aa22-15b6-9cff-4679c2a0e796
[0m20:16:47.046413 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:16:47.051449 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-a9fc-136f-b720-bb7532ae63a0) - Closing
[0m20:16:47.492482 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql
[0m20:16:47.496555 [error] [Thread-2 (]: 1 of 4 ERROR not_null_stg_support_tickets_account_id ........................... [[31mERROR[0m in 6.36s]
[0m20:16:47.500764 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:16:47.505286 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:16:47.509777 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql.
[0m20:16:47.514226 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:16:47.524299 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:16:47.528326 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:16:47.534379 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:16:47.584262 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:16:47.588286 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:16:47.600954 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:16:47.605173 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:16:47.605173 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:16:47.609278 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:16:52.154223 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-ad7a-150e-9faf-5286b99066ba) - Created
[0m20:16:52.969893 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd8-ada9-1c90-aabd-4571963cf4b4
[0m20:16:52.977979 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:16:52.977979 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-ad7a-150e-9faf-5286b99066ba) - Closing
[0m20:16:53.284861 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:16:53.288900 [error] [Thread-2 (]: 2 of 4 ERROR not_null_stg_support_tickets_ticket_id ............................ [[31mERROR[0m in 5.76s]
[0m20:16:53.294931 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:16:53.301202 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:16:53.303215 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql.
[0m20:16:53.305237 [info ] [Thread-2 (]: 3 of 4 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:16:53.313608 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:16:53.317962 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:16:53.322242 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:16:53.392553 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:16:53.398745 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:16:53.419378 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:16:53.425473 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:16:53.429493 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:16:53.429493 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:16:58.285010 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-b124-124a-b4b3-ef8e287ee06e) - Created
[0m20:16:59.020905 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd8-b151-1753-9a23-5c13ae42c010
[0m20:16:59.026902 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:16:59.030906 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-b124-124a-b4b3-ef8e287ee06e) - Closing
[0m20:16:59.325199 [debug] [Thread-2 (]: Database Error in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:16:59.328203 [error] [Thread-2 (]: 3 of 4 ERROR relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[31mERROR[0m in 6.01s]
[0m20:16:59.334434 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:16:59.341872 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:16:59.345962 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e' to be skipped because of status 'error'.  Reason: Database Error in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql.
[0m20:16:59.350216 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_support_tickets_ticket_id ......................... [RUN]
[0m20:16:59.363488 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:16:59.367953 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:16:59.372953 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:16:59.421228 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:16:59.426626 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:16:59.441942 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:16:59.446749 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:16:59.449940 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:16:59.452108 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:17:04.511579 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-b4e2-1982-b5b6-b8ea3717a723) - Created
[0m20:17:05.264270 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd8-b508-19a5-b25c-b0bb05371ddd
[0m20:17:05.269270 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:17:05.272333 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd8-b4e2-1982-b5b6-b8ea3717a723) - Closing
[0m20:17:06.186238 [debug] [Thread-2 (]: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:17:06.189241 [error] [Thread-2 (]: 4 of 4 ERROR unique_stg_support_tickets_ticket_id .............................. [[31mERROR[0m in 6.83s]
[0m20:17:06.192594 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:17:06.194814 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a' to be skipped because of status 'error'.  Reason: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql.
[0m20:17:06.199061 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:17:06.201061 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:17:06.204063 [info ] [MainThread]: 
[0m20:17:06.206220 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 31.70 seconds (31.70s).
[0m20:17:06.211485 [debug] [MainThread]: Command end result
[0m20:17:06.419416 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:17:06.430356 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:17:06.466849 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:17:06.468850 [info ] [MainThread]: 
[0m20:17:06.471853 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m20:17:06.475181 [info ] [MainThread]: 
[0m20:17:06.478962 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)[0m
[0m20:17:06.482074 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql
[0m20:17:06.486033 [info ] [MainThread]: 
[0m20:17:06.489756 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql
[0m20:17:06.493230 [info ] [MainThread]: 
[0m20:17:06.495611 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:17:06.499164 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:17:06.502172 [info ] [MainThread]: 
[0m20:17:06.504616 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:17:06.507746 [info ] [MainThread]: 
[0m20:17:06.509857 [error] [MainThread]: [31mFailure in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)[0m
[0m20:17:06.513065 [error] [MainThread]:   Database Error in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:17:06.515065 [info ] [MainThread]: 
[0m20:17:06.518789 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:17:06.521441 [info ] [MainThread]: 
[0m20:17:06.524539 [error] [MainThread]: [31mFailure in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:17:06.526541 [error] [MainThread]:   Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:17:06.528538 [info ] [MainThread]: 
[0m20:17:06.532099 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:17:06.534098 [info ] [MainThread]: 
[0m20:17:06.537687 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=4 SKIP=0 NO-OP=0 TOTAL=4
[0m20:17:06.541738 [debug] [MainThread]: Command `dbt test` failed at 20:17:06.541738 after 41.70 seconds
[0m20:17:06.543738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE6688EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE67BFB910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CE7CBBA9E0>]}
[0m20:17:06.545875 [debug] [MainThread]: Flushing usage events
[0m20:17:10.478393 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:18:49.168347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017063DBEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000170660DC3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000170660DC1F0>]}


============================== 20:18:49.175059 | cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c ==============================
[0m20:18:49.175059 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:18:49.175059 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt test', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:18:52.368143 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:18:52.368143 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:18:52.368143 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:18:55.145978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000170660DC460>]}
[0m20:18:55.367978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001706896FF70>]}
[0m20:18:55.367978 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:18:57.018964 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:18:57.018964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017065BEB340>]}
[0m20:18:57.101301 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:18:58.029787 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:18:58.030353 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\stg_feature_usage.sql
[0m20:18:58.035798 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\schema.yml
[0m20:18:59.824284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001707A2DC130>]}
[0m20:19:00.428504 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:19:00.433094 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:19:00.513740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001707A0CD900>]}
[0m20:19:00.519114 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:19:00.521757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001707A0CDB70>]}
[0m20:19:00.528512 [info ] [MainThread]: 
[0m20:19:00.528512 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:19:00.533210 [info ] [MainThread]: 
[0m20:19:00.538305 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:19:00.542719 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:19:00.569178 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:19:00.577545 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:19:00.618783 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:19:00.618783 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:19:00.618783 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:19:05.579303 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-fd04-1a09-b823-3f451d0c759f) - Created
[0m20:19:07.536002 [debug] [ThreadPool]: SQL status: OK in 6.920 seconds
[0m20:19:07.547189 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd8-fd04-1a09-b823-3f451d0c759f, command-id=01f10cd8-fd33-1580-ad88-3297b3d07c16) - Closing
[0m20:19:07.547189 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:19:07.547189 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd8-fd04-1a09-b823-3f451d0c759f) - Closing
[0m20:19:07.840543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cb886c60-afd3-480e-a3fd-c5bf3ec0bc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001707A033C10>]}
[0m20:19:07.864691 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:19:07.866549 [info ] [Thread-2 (]: 1 of 19 START test dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [RUN]
[0m20:19:07.869882 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77) - Creating connection
[0m20:19:07.871889 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77'
[0m20:19:07.873902 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:19:07.901658 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:19:07.912333 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:19:07.978476 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:19:07.978476 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:19:07.978476 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  





with validation_errors as (

    select
        subscription_id, usage_date, feature_name
    from `workspace`.`analytics`.`stg_feature_usage`
    group by subscription_id, usage_date, feature_name
    having count(*) > 1

)

select *
from validation_errors



  
  
      
    ) dbt_internal_test
[0m20:19:07.978476 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:13.157657 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-0187-1248-b4f4-86dbfabea05f) - Created
[0m20:19:13.781774 [debug] [Thread-2 (]: SQL status: OK in 5.800 seconds
[0m20:19:13.793581 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-0187-1248-b4f4-86dbfabea05f, command-id=01f10cd9-01b6-17a2-b714-a5297392cf04) - Closing
[0m20:19:13.807372 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: Close
[0m20:19:13.807372 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-0187-1248-b4f4-86dbfabea05f) - Closing
[0m20:19:14.111045 [info ] [Thread-2 (]: 1 of 19 PASS dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [[32mPASS[0m in 6.24s]
[0m20:19:14.117507 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:19:14.120528 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:19:14.126170 [info ] [Thread-2 (]: 2 of 19 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m20:19:14.141265 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m20:19:14.146236 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108'
[0m20:19:14.149801 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:19:14.191446 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:19:14.197584 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:19:14.212327 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:19:14.226611 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:19:14.226611 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:14.226611 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:18.480012 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-04b4-15f3-8461-bb0c9f2bfc57) - Created
[0m20:19:19.410389 [debug] [Thread-2 (]: SQL status: OK in 5.180 seconds
[0m20:19:19.423811 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-04b4-15f3-8461-bb0c9f2bfc57, command-id=01f10cd9-04e4-1226-9b8d-a4dfef5ffbc0) - Closing
[0m20:19:19.427107 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: Close
[0m20:19:19.427107 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-04b4-15f3-8461-bb0c9f2bfc57) - Closing
[0m20:19:19.735531 [info ] [Thread-2 (]: 2 of 19 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 5.60s]
[0m20:19:19.746259 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:19:19.746259 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:19:19.746259 [info ] [Thread-2 (]: 3 of 19 START test not_null_stg_accounts_seats ................................. [RUN]
[0m20:19:19.758125 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39) - Creating connection
[0m20:19:19.760523 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39'
[0m20:19:19.765462 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:19:19.805196 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:19:19.810605 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:19:19.821068 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:19:19.825725 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:19:19.827829 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select seats
from `workspace`.`analytics`.`stg_accounts`
where seats is null



  
  
      
    ) dbt_internal_test
[0m20:19:19.827829 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:24.421586 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-083f-10a4-be2f-341dbc3d78ea) - Created
[0m20:19:25.253266 [debug] [Thread-2 (]: SQL status: OK in 5.430 seconds
[0m20:19:25.257296 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-083f-10a4-be2f-341dbc3d78ea, command-id=01f10cd9-086d-1b36-a4f3-38f50d720a26) - Closing
[0m20:19:25.265161 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: Close
[0m20:19:25.265161 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-083f-10a4-be2f-341dbc3d78ea) - Closing
[0m20:19:25.554432 [info ] [Thread-2 (]: 3 of 19 PASS not_null_stg_accounts_seats ....................................... [[32mPASS[0m in 5.81s]
[0m20:19:25.560295 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:19:25.562305 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:19:25.566325 [info ] [Thread-2 (]: 4 of 19 START test not_null_stg_accounts_signup_date ........................... [RUN]
[0m20:19:25.571002 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a) - Creating connection
[0m20:19:25.575022 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a'
[0m20:19:25.577089 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:19:25.616377 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:19:25.620399 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:19:25.638708 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:19:25.640716 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:19:25.646957 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signup_date
from `workspace`.`analytics`.`stg_accounts`
where signup_date is null



  
  
      
    ) dbt_internal_test
[0m20:19:25.646957 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:30.259140 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-0bb9-1295-aad8-c00553781d7d) - Created
[0m20:19:31.074158 [debug] [Thread-2 (]: SQL status: OK in 5.430 seconds
[0m20:19:31.089428 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-0bb9-1295-aad8-c00553781d7d, command-id=01f10cd9-0be7-1c51-b0e2-fe61fde08660) - Closing
[0m20:19:31.093264 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: Close
[0m20:19:31.095430 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-0bb9-1295-aad8-c00553781d7d) - Closing
[0m20:19:31.380193 [info ] [Thread-2 (]: 4 of 19 PASS not_null_stg_accounts_signup_date ................................. [[32mPASS[0m in 5.81s]
[0m20:19:31.390756 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:19:31.392849 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:19:31.395694 [info ] [Thread-2 (]: 5 of 19 START test not_null_stg_churn_events_account_id ........................ [RUN]
[0m20:19:31.400224 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d) - Creating connection
[0m20:19:31.401637 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d'
[0m20:19:31.401637 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:19:31.426488 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:19:31.434376 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:19:31.449089 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:19:31.453104 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:19:31.455112 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_churn_events`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:31.457471 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:36.097125 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-0f34-16f1-ab30-6dc7ff3fca3d) - Created
[0m20:19:36.613379 [debug] [Thread-2 (]: SQL status: OK in 5.160 seconds
[0m20:19:36.620479 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-0f34-16f1-ab30-6dc7ff3fca3d, command-id=01f10cd9-0f64-1559-97ff-4b2a596924b0) - Closing
[0m20:19:36.623404 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: Close
[0m20:19:36.626706 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-0f34-16f1-ab30-6dc7ff3fca3d) - Closing
[0m20:19:36.931255 [info ] [Thread-2 (]: 5 of 19 PASS not_null_stg_churn_events_account_id .............................. [[32mPASS[0m in 5.53s]
[0m20:19:36.932765 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:19:36.934773 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:19:36.934773 [info ] [Thread-2 (]: 6 of 19 START test not_null_stg_churn_events_churn_event_id .................... [RUN]
[0m20:19:36.941378 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc) - Creating connection
[0m20:19:36.942943 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc'
[0m20:19:36.946281 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:19:36.984593 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:19:36.986601 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:19:36.991584 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:19:37.005636 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:19:37.007698 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select churn_event_id
from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:37.010889 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:41.384642 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-1260-12f9-addc-7cd60a11d92b) - Created
[0m20:19:41.839508 [debug] [Thread-2 (]: SQL status: OK in 4.830 seconds
[0m20:19:41.847885 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-1260-12f9-addc-7cd60a11d92b, command-id=01f10cd9-1288-1e6f-b7c6-2eac9b0f6c4b) - Closing
[0m20:19:41.856212 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: Close
[0m20:19:41.856212 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-1260-12f9-addc-7cd60a11d92b) - Closing
[0m20:19:42.147371 [info ] [Thread-2 (]: 6 of 19 PASS not_null_stg_churn_events_churn_event_id .......................... [[32mPASS[0m in 5.21s]
[0m20:19:42.155993 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:19:42.158000 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:19:42.162017 [info ] [Thread-2 (]: 7 of 19 START test not_null_stg_feature_usage_subscription_id .................. [RUN]
[0m20:19:42.200000 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3) - Creating connection
[0m20:19:42.203760 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3'
[0m20:19:42.205958 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:19:42.225866 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:19:42.231922 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:19:42.242377 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:19:42.252034 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:19:42.252034 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_feature_usage`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:42.255924 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:46.676522 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-158a-100b-8470-5c3f519bf171) - Created
[0m20:19:47.366812 [debug] [Thread-2 (]: SQL status: OK in 5.110 seconds
[0m20:19:47.377769 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-158a-100b-8470-5c3f519bf171, command-id=01f10cd9-15b0-1fcb-b47d-edf06fb03482) - Closing
[0m20:19:47.382789 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: Close
[0m20:19:47.382789 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-158a-100b-8470-5c3f519bf171) - Closing
[0m20:19:47.675815 [info ] [Thread-2 (]: 7 of 19 PASS not_null_stg_feature_usage_subscription_id ........................ [[32mPASS[0m in 5.48s]
[0m20:19:47.677823 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:19:47.677823 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:19:47.686960 [info ] [Thread-2 (]: 8 of 19 START test not_null_stg_subscriptions_account_id ....................... [RUN]
[0m20:19:47.691351 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9) - Creating connection
[0m20:19:47.691351 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9'
[0m20:19:47.691351 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:19:47.758182 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:19:47.758182 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:19:47.782624 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:19:47.790590 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:19:47.790590 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_subscriptions`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:47.790590 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:52.173194 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-18c8-1c59-9070-de0daa125ab2) - Created
[0m20:19:53.306003 [debug] [Thread-2 (]: SQL status: OK in 5.520 seconds
[0m20:19:53.306003 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-18c8-1c59-9070-de0daa125ab2, command-id=01f10cd9-18f7-1e14-9ace-eb486593c72a) - Closing
[0m20:19:53.322159 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: Close
[0m20:19:53.322159 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-18c8-1c59-9070-de0daa125ab2) - Closing
[0m20:19:53.640357 [info ] [Thread-2 (]: 8 of 19 PASS not_null_stg_subscriptions_account_id ............................. [[32mPASS[0m in 5.95s]
[0m20:19:53.640357 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:19:53.640357 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:19:53.640357 [info ] [Thread-2 (]: 9 of 19 START test not_null_stg_subscriptions_subscription_id .................. [RUN]
[0m20:19:53.640357 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91) - Creating connection
[0m20:19:53.651181 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m20:19:53.651181 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:19:53.680142 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:19:53.680142 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:19:53.689730 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:19:53.689730 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:19:53.703276 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:53.705076 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:19:58.100053 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-1c53-1427-b7be-6a85cb3ae095) - Created
[0m20:19:59.038069 [debug] [Thread-2 (]: SQL status: OK in 5.330 seconds
[0m20:19:59.042085 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-1c53-1427-b7be-6a85cb3ae095, command-id=01f10cd9-1c82-11a7-8935-24b31e4b58be) - Closing
[0m20:19:59.046104 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m20:19:59.046104 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-1c53-1427-b7be-6a85cb3ae095) - Closing
[0m20:19:59.344431 [info ] [Thread-2 (]: 9 of 19 PASS not_null_stg_subscriptions_subscription_id ........................ [[32mPASS[0m in 5.70s]
[0m20:19:59.347697 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:19:59.347697 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:19:59.347697 [info ] [Thread-2 (]: 10 of 19 START test not_null_stg_support_tickets_account_id .................... [RUN]
[0m20:19:59.357505 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:19:59.360290 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:19:59.361830 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:19:59.406367 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:19:59.406367 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:19:59.420476 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:19:59.423074 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:19:59.423074 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:19:59.423074 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:03.835644 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-1fbd-10f0-baf9-a4a04cb968e2) - Created
[0m20:20:04.660171 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd9-1fec-1415-bf9e-84d9c99ecefc
[0m20:20:04.660171 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:20:04.660171 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-1fbd-10f0-baf9-a4a04cb968e2) - Closing
[0m20:20:05.706411 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql
[0m20:20:05.706411 [error] [Thread-2 (]: 10 of 19 ERROR not_null_stg_support_tickets_account_id ......................... [[31mERROR[0m in 6.36s]
[0m20:20:05.706411 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:20:05.706411 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:20:05.706411 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql.
[0m20:20:05.722152 [info ] [Thread-2 (]: 11 of 19 START test not_null_stg_support_tickets_ticket_id ..................... [RUN]
[0m20:20:05.727655 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:20:05.730551 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:20:05.730551 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:20:05.749503 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:20:05.755754 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:20:05.769629 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:20:05.771519 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:20:05.771519 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:20:05.771519 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:10.299741 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-2398-1a86-83af-133431112140) - Created
[0m20:20:11.042476 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd9-23c6-1b29-8d33-6707542b1537
[0m20:20:11.042476 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:20:11.042476 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-2398-1a86-83af-133431112140) - Closing
[0m20:20:11.363878 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:20:11.364886 [error] [Thread-2 (]: 11 of 19 ERROR not_null_stg_support_tickets_ticket_id .......................... [[31mERROR[0m in 5.64s]
[0m20:20:11.364886 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:20:11.364886 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:20:11.364886 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql.
[0m20:20:11.378156 [info ] [Thread-2 (]: 12 of 19 START test relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:20:11.389359 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b) - Creating connection
[0m20:20:11.391371 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b'
[0m20:20:11.399570 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:20:11.453851 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:20:11.453851 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:20:11.477261 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:20:11.477261 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:20:11.477261 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_churn_events`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:20:11.485298 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:16.851319 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-277f-1fe9-a04f-454b97c12f3f) - Created
[0m20:20:17.373074 [debug] [Thread-2 (]: SQL status: OK in 5.890 seconds
[0m20:20:17.384865 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-277f-1fe9-a04f-454b97c12f3f, command-id=01f10cd9-27ae-1c3d-b257-d12700200b05) - Closing
[0m20:20:17.389082 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: Close
[0m20:20:17.393098 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-277f-1fe9-a04f-454b97c12f3f) - Closing
[0m20:20:17.676323 [info ] [Thread-2 (]: 12 of 19 PASS relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 6.29s]
[0m20:20:17.678331 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:20:17.682517 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:20:17.682517 [info ] [Thread-2 (]: 13 of 19 START test relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [RUN]
[0m20:20:17.691791 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850) - Creating connection
[0m20:20:17.696654 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850'
[0m20:20:17.696654 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:20:17.739153 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:20:17.739153 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:20:17.757516 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:20:17.768220 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:20:17.768220 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select subscription_id as from_field
    from `workspace`.`analytics`.`stg_feature_usage`
    where subscription_id is not null
),

parent as (
    select subscription_id as to_field
    from `workspace`.`analytics`.`stg_subscriptions`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:20:17.773824 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:22.072636 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-2a9c-18be-832a-da2d8b931eb7) - Created
[0m20:20:22.683479 [debug] [Thread-2 (]: SQL status: OK in 4.910 seconds
[0m20:20:22.699373 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-2a9c-18be-832a-da2d8b931eb7, command-id=01f10cd9-2ac9-1848-9c87-7a47fefcf551) - Closing
[0m20:20:22.699373 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: Close
[0m20:20:22.699373 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-2a9c-18be-832a-da2d8b931eb7) - Closing
[0m20:20:23.000889 [info ] [Thread-2 (]: 13 of 19 PASS relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [[32mPASS[0m in 5.31s]
[0m20:20:23.002897 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:20:23.010093 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:20:23.012102 [info ] [Thread-2 (]: 14 of 19 START test relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:20:23.017376 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90) - Creating connection
[0m20:20:23.021401 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90'
[0m20:20:23.025429 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:20:23.090002 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:20:23.090002 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:20:23.117795 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:20:23.117795 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:20:23.123988 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_subscriptions`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:20:23.123988 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:27.501852 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-2dd8-1638-98d2-1829145a71d5) - Created
[0m20:20:28.734033 [debug] [Thread-2 (]: SQL status: OK in 5.610 seconds
[0m20:20:28.740059 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-2dd8-1638-98d2-1829145a71d5, command-id=01f10cd9-2e07-18b9-8387-ea5b69af6232) - Closing
[0m20:20:28.742339 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: Close
[0m20:20:28.744349 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-2dd8-1638-98d2-1829145a71d5) - Closing
[0m20:20:29.050364 [info ] [Thread-2 (]: 14 of 19 PASS relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 6.03s]
[0m20:20:29.053664 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:20:29.053664 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:20:29.053664 [info ] [Thread-2 (]: 15 of 19 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:20:29.062145 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:20:29.065160 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:20:29.067252 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:20:29.086471 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:20:29.086471 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:20:29.100778 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:20:29.100778 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:20:29.110574 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:20:29.110574 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:34.569183 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-320f-112c-a238-c8a2322f439c) - Created
[0m20:20:35.482660 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd9-323e-1d46-871d-b9ae9f5d5ef0
[0m20:20:35.482660 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:20:35.482660 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-320f-112c-a238-c8a2322f439c) - Closing
[0m20:20:36.434988 [debug] [Thread-2 (]: Database Error in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:20:36.434988 [error] [Thread-2 (]: 15 of 19 ERROR relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[31mERROR[0m in 7.38s]
[0m20:20:36.442845 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:20:36.442845 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:20:36.442845 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e' to be skipped because of status 'error'.  Reason: Database Error in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql.
[0m20:20:36.450202 [info ] [Thread-2 (]: 16 of 19 START test unique_stg_accounts_account_id ............................. [RUN]
[0m20:20:36.456092 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m20:20:36.460287 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79'
[0m20:20:36.461097 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:20:36.521625 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:20:36.529670 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:20:36.546545 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:20:36.546545 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:20:36.554837 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:20:36.554837 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:40.916443 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-35d6-19e9-9216-547acc51a52f) - Created
[0m20:20:41.732798 [debug] [Thread-2 (]: SQL status: OK in 5.180 seconds
[0m20:20:41.754696 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-35d6-19e9-9216-547acc51a52f, command-id=01f10cd9-3604-1b8f-b8af-ef3c98de47e6) - Closing
[0m20:20:41.760724 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: Close
[0m20:20:41.764481 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-35d6-19e9-9216-547acc51a52f) - Closing
[0m20:20:42.045805 [info ] [Thread-2 (]: 16 of 19 PASS unique_stg_accounts_account_id ................................... [[32mPASS[0m in 5.59s]
[0m20:20:42.047813 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:20:42.047813 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:20:42.047813 [info ] [Thread-2 (]: 17 of 19 START test unique_stg_churn_events_churn_event_id ..................... [RUN]
[0m20:20:42.062752 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947) - Creating connection
[0m20:20:42.067288 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947'
[0m20:20:42.072686 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:20:42.100754 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:20:42.100754 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:20:42.121983 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:20:42.129996 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:20:42.134526 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    churn_event_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is not null
group by churn_event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:20:42.138045 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:46.548670 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-3930-1d36-92aa-5373ea66b94f) - Created
[0m20:20:47.085929 [debug] [Thread-2 (]: SQL status: OK in 4.950 seconds
[0m20:20:47.097384 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-3930-1d36-92aa-5373ea66b94f, command-id=01f10cd9-3960-1208-9e09-065b53557aee) - Closing
[0m20:20:47.101957 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: Close
[0m20:20:47.101957 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-3930-1d36-92aa-5373ea66b94f) - Closing
[0m20:20:47.505567 [info ] [Thread-2 (]: 17 of 19 PASS unique_stg_churn_events_churn_event_id ........................... [[32mPASS[0m in 5.44s]
[0m20:20:47.510567 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:20:47.514238 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:20:47.520526 [info ] [Thread-2 (]: 18 of 19 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m20:20:47.526052 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e) - Creating connection
[0m20:20:47.526052 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m20:20:47.530154 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:20:47.562496 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:20:47.566552 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:20:47.580156 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:20:47.581337 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:20:47.581337 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:20:47.581337 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:51.975899 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-3c6d-1603-be3a-1c1be93d17bb) - Created
[0m20:20:53.028091 [debug] [Thread-2 (]: SQL status: OK in 5.450 seconds
[0m20:20:53.039941 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-3c6d-1603-be3a-1c1be93d17bb, command-id=01f10cd9-3c9c-167e-9c5c-dcb8118ea334) - Closing
[0m20:20:53.042016 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m20:20:53.044099 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-3c6d-1603-be3a-1c1be93d17bb) - Closing
[0m20:20:53.332577 [info ] [Thread-2 (]: 18 of 19 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 5.81s]
[0m20:20:53.332577 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:20:53.332577 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:20:53.340636 [info ] [Thread-2 (]: 19 of 19 START test unique_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:20:53.340636 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:20:53.348774 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:20:53.354359 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:20:53.383604 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:20:53.383604 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:20:53.398445 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:20:53.413895 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:20:53.413895 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:20:53.420068 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:20:57.813971 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-3fe9-1852-8391-db0a5ef1a39c) - Created
[0m20:20:58.531773 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd9-4017-14dd-85a7-f90749a778d4
[0m20:20:58.535792 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:20:58.537799 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-3fe9-1852-8391-db0a5ef1a39c) - Closing
[0m20:20:59.569424 [debug] [Thread-2 (]: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:20:59.571430 [error] [Thread-2 (]: 19 of 19 ERROR unique_stg_support_tickets_ticket_id ............................ [[31mERROR[0m in 6.23s]
[0m20:20:59.575183 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:20:59.575183 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a' to be skipped because of status 'error'.  Reason: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql.
[0m20:20:59.575183 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:20:59.575183 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:20:59.575183 [info ] [MainThread]: 
[0m20:20:59.575183 [info ] [MainThread]: Finished running 19 data tests in 0 hours 1 minutes and 59.04 seconds (119.04s).
[0m20:20:59.602796 [debug] [MainThread]: Command end result
[0m20:20:59.754614 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:20:59.763194 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:20:59.789109 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:20:59.789109 [info ] [MainThread]: 
[0m20:20:59.789109 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m20:20:59.789109 [info ] [MainThread]: 
[0m20:20:59.789109 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)[0m
[0m20:20:59.800113 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_account_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql
[0m20:20:59.803449 [info ] [MainThread]: 
[0m20:20:59.805725 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_account_id.sql
[0m20:20:59.808037 [info ] [MainThread]: 
[0m20:20:59.811565 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:20:59.814724 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:20:59.816734 [info ] [MainThread]: 
[0m20:20:59.819006 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:20:59.821256 [info ] [MainThread]: 
[0m20:20:59.825543 [error] [MainThread]: [31mFailure in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)[0m
[0m20:20:59.826940 [error] [MainThread]:   Database Error in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:20:59.826940 [info ] [MainThread]: 
[0m20:20:59.834739 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:20:59.834739 [info ] [MainThread]: 
[0m20:20:59.840045 [error] [MainThread]: [31mFailure in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:20:59.840391 [error] [MainThread]:   Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`analytics`.`stg_support_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:20:59.842917 [info ] [MainThread]: 
[0m20:20:59.842917 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:20:59.850817 [info ] [MainThread]: 
[0m20:20:59.852825 [info ] [MainThread]: Done. PASS=15 WARN=0 ERROR=4 SKIP=0 NO-OP=0 TOTAL=19
[0m20:20:59.855930 [debug] [MainThread]: Command `dbt test` failed at 20:20:59.855930 after 130.96 seconds
[0m20:20:59.859030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017063DBEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001707A0CCA30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017079D9EC80>]}
[0m20:20:59.859030 [debug] [MainThread]: Flushing usage events
[0m20:21:03.130713 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:21:31.918683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DE934EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEB6683A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEB6681F0>]}


============================== 20:21:31.936623 | 6683c104-e4dc-4f36-b64f-efbfe9909710 ==============================
[0m20:21:31.936623 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:21:31.936623 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_support_tickets', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:21:35.155826 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:21:35.155826 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:21:35.155826 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:21:38.005231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DE775BE50>]}
[0m20:21:38.259476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEAFB6AD0>]}
[0m20:21:38.259476 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:21:39.781904 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:21:39.783910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEB1809D0>]}
[0m20:21:39.855628 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:21:40.733144 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:21:40.733144 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\stg_support_tickets.sql
[0m20:21:42.105970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DFF880BE0>]}
[0m20:21:42.586846 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:21:42.602790 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:21:42.642618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DFF59E710>]}
[0m20:21:42.644689 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:21:42.644689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DFF583010>]}
[0m20:21:42.650566 [info ] [MainThread]: 
[0m20:21:42.654927 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:21:42.656469 [info ] [MainThread]: 
[0m20:21:42.661997 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:21:42.665974 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:21:42.676226 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:21:42.676226 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:21:42.707387 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:21:42.734743 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:21:42.734743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:21:47.890975 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd9-5dc2-1423-b1bb-b3aa1137ec5a) - Created
[0m20:21:48.377222 [debug] [ThreadPool]: SQL status: OK in 5.640 seconds
[0m20:21:48.538689 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd9-5dc2-1423-b1bb-b3aa1137ec5a, command-id=01f10cd9-5df1-1c70-9779-c64d9e0f14b3) - Closing
[0m20:21:48.547149 [debug] [ThreadPool]: On list_workspace: Close
[0m20:21:48.547149 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd9-5dc2-1423-b1bb-b3aa1137ec5a) - Closing
[0m20:21:48.875346 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:21:48.887246 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:21:48.916480 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:21:48.918489 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:21:48.921271 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:21:53.722213 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd9-613d-18e5-9ab5-f276a5f47876) - Created
[0m20:21:54.651238 [debug] [ThreadPool]: SQL status: OK in 5.730 seconds
[0m20:21:54.660298 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd9-613d-18e5-9ab5-f276a5f47876, command-id=01f10cd9-616c-12b6-b824-f649d77e6ef1) - Closing
[0m20:21:54.662304 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:21:54.664313 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd9-613d-18e5-9ab5-f276a5f47876) - Closing
[0m20:21:55.008298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DFF59F430>]}
[0m20:21:55.016875 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:21:55.021301 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_support_tickets ...................... [RUN]
[0m20:21:55.021301 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:21:55.021301 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:21:55.021301 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:21:55.054419 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:21:55.056229 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:21:55.134682 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:21:55.134682 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:21:55.134682 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEAD53850>]}
[0m20:21:55.195365 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:21:55.219940 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:21:55.219940 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:21:55.227955 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_support_tickets

),

cleaned as (

    select
        ticket_id
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:21:55.227955 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:22:01.507996 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd9-65de-1640-97ed-77fd98d4f66e) - Created
[0m20:22:03.286818 [debug] [Thread-3 (]: SQL status: OK in 8.060 seconds
[0m20:22:03.290572 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10cd9-65de-1640-97ed-77fd98d4f66e, command-id=01f10cd9-660e-1414-9327-3e882a20e0d6) - Closing
[0m20:22:03.321703 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:22:03.321703 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:22:03.337495 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cd9-65de-1640-97ed-77fd98d4f66e) - Closing
[0m20:22:03.593370 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6683c104-e4dc-4f36-b64f-efbfe9909710', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DE775AF50>]}
[0m20:22:03.593370 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_support_tickets ................. [[32mOK[0m in 8.57s]
[0m20:22:03.593370 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:22:03.602840 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:22:03.608414 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:22:03.611393 [info ] [MainThread]: 
[0m20:22:03.612033 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 20.95 seconds (20.95s).
[0m20:22:03.615201 [debug] [MainThread]: Command end result
[0m20:22:03.774012 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:22:03.774012 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:22:03.790158 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:22:03.806204 [info ] [MainThread]: 
[0m20:22:03.806204 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:22:03.806204 [info ] [MainThread]: 
[0m20:22:03.814032 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:22:03.818081 [debug] [MainThread]: Command `dbt run` succeeded at 20:22:03.818081 after 32.18 seconds
[0m20:22:03.821188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DE934EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEA6BD390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028DEB66AF20>]}
[0m20:22:03.821188 [debug] [MainThread]: Flushing usage events
[0m20:22:07.340141 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:22:54.247016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3F9D3ABF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3FB07C370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3FB07C1C0>]}


============================== 20:22:54.263494 | 9123fd91-589d-48f6-96c9-207bcbeea95b ==============================
[0m20:22:54.263494 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:22:54.263494 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt test', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:22:57.807925 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:22:57.809931 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:22:57.809931 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:23:00.563582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3FB07C190>]}
[0m20:23:00.770759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38E3BB730>]}
[0m20:23:00.786765 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:23:02.279430 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:23:02.283444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38EA99CF0>]}
[0m20:23:02.345515 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:23:03.306224 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:23:03.308235 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:23:03.312305 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:23:03.478412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38F106080>]}
[0m20:23:03.875431 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:23:03.881482 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:23:03.945696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38F05D420>]}
[0m20:23:03.945696 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:23:03.945696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38F05CFA0>]}
[0m20:23:03.961753 [info ] [MainThread]: 
[0m20:23:03.977561 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:23:03.977561 [info ] [MainThread]: 
[0m20:23:03.987803 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:23:03.989811 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:23:04.032528 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:23:04.040434 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:23:04.107576 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:23:04.111185 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:23:04.115125 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:23:08.988607 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd9-8e18-134f-9d8e-302140043e55) - Created
[0m20:23:09.713022 [debug] [ThreadPool]: SQL status: OK in 5.600 seconds
[0m20:23:09.713022 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cd9-8e18-134f-9d8e-302140043e55, command-id=01f10cd9-8e47-1be3-9d5b-1f52badf4286) - Closing
[0m20:23:09.730060 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:23:09.730060 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cd9-8e18-134f-9d8e-302140043e55) - Closing
[0m20:23:10.022601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9123fd91-589d-48f6-96c9-207bcbeea95b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38F05C190>]}
[0m20:23:10.042336 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:23:10.044343 [info ] [Thread-2 (]: 1 of 19 START test dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [RUN]
[0m20:23:10.046089 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77) - Creating connection
[0m20:23:10.046089 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77'
[0m20:23:10.046089 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:23:10.124270 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:23:10.126279 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:23:10.187650 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:23:10.191667 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:23:10.193676 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  





with validation_errors as (

    select
        subscription_id, usage_date, feature_name
    from `workspace`.`analytics`.`stg_feature_usage`
    group by subscription_id, usage_date, feature_name
    having count(*) > 1

)

select *
from validation_errors



  
  
      
    ) dbt_internal_test
[0m20:23:10.195686 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:14.618703 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-9175-14e8-96e5-7dc424120d0c) - Created
[0m20:23:15.143579 [debug] [Thread-2 (]: SQL status: OK in 4.950 seconds
[0m20:23:15.155888 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-9175-14e8-96e5-7dc424120d0c, command-id=01f10cd9-91a2-1fed-bc37-70a9cf608087) - Closing
[0m20:23:15.167431 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: Close
[0m20:23:15.169438 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-9175-14e8-96e5-7dc424120d0c) - Closing
[0m20:23:15.435535 [info ] [Thread-2 (]: 1 of 19 PASS dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [[32mPASS[0m in 5.39s]
[0m20:23:15.446155 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:23:15.451530 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:23:15.456922 [info ] [Thread-2 (]: 2 of 19 START test not_null_stg_accounts_account_id ............................ [RUN]
[0m20:23:15.462794 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m20:23:15.464804 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108'
[0m20:23:15.464804 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:23:15.509581 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:23:15.511712 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:23:15.523750 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:23:15.526826 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:23:15.526826 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:15.526826 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:20.458947 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-94ed-1ba9-8d73-1c4af1bac831) - Created
[0m20:23:20.969875 [debug] [Thread-2 (]: SQL status: OK in 5.440 seconds
[0m20:23:20.993359 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-94ed-1ba9-8d73-1c4af1bac831, command-id=01f10cd9-951d-1f43-b9f0-5585b1d03507) - Closing
[0m20:23:20.997376 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: Close
[0m20:23:20.999384 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-94ed-1ba9-8d73-1c4af1bac831) - Closing
[0m20:23:21.297180 [info ] [Thread-2 (]: 2 of 19 PASS not_null_stg_accounts_account_id .................................. [[32mPASS[0m in 5.84s]
[0m20:23:21.299189 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:23:21.299189 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:23:21.299189 [info ] [Thread-2 (]: 3 of 19 START test not_null_stg_accounts_seats ................................. [RUN]
[0m20:23:21.312727 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39) - Creating connection
[0m20:23:21.316464 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39'
[0m20:23:21.316464 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:23:21.361806 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:23:21.361806 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:23:21.382648 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:23:21.382648 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:23:21.382648 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select seats
from `workspace`.`analytics`.`stg_accounts`
where seats is null



  
  
      
    ) dbt_internal_test
[0m20:23:21.392351 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:25.782703 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-981b-19e0-94cd-8b4c311c1eb9) - Created
[0m20:23:26.305213 [debug] [Thread-2 (]: SQL status: OK in 4.910 seconds
[0m20:23:26.314999 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-981b-19e0-94cd-8b4c311c1eb9, command-id=01f10cd9-984b-14d3-a702-d5535dd52a61) - Closing
[0m20:23:26.321024 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: Close
[0m20:23:26.324780 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-981b-19e0-94cd-8b4c311c1eb9) - Closing
[0m20:23:26.605454 [info ] [Thread-2 (]: 3 of 19 PASS not_null_stg_accounts_seats ....................................... [[32mPASS[0m in 5.31s]
[0m20:23:26.621372 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:23:26.621372 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:23:26.621372 [info ] [Thread-2 (]: 4 of 19 START test not_null_stg_accounts_signup_date ........................... [RUN]
[0m20:23:26.633240 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a) - Creating connection
[0m20:23:26.638430 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a'
[0m20:23:26.641578 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:23:26.660940 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:23:26.660940 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:23:26.676192 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:23:26.691507 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:23:26.694310 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signup_date
from `workspace`.`analytics`.`stg_accounts`
where signup_date is null



  
  
      
    ) dbt_internal_test
[0m20:23:26.694310 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:31.314001 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-9b66-13a9-8e07-a972e2c8c1fa) - Created
[0m20:23:32.037282 [debug] [Thread-2 (]: SQL status: OK in 5.340 seconds
[0m20:23:32.045310 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-9b66-13a9-8e07-a972e2c8c1fa, command-id=01f10cd9-9b95-110e-b702-71000ab46432) - Closing
[0m20:23:32.049064 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: Close
[0m20:23:32.051074 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-9b66-13a9-8e07-a972e2c8c1fa) - Closing
[0m20:23:32.344507 [info ] [Thread-2 (]: 4 of 19 PASS not_null_stg_accounts_signup_date ................................. [[32mPASS[0m in 5.72s]
[0m20:23:32.344507 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:23:32.344507 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:23:32.344507 [info ] [Thread-2 (]: 5 of 19 START test not_null_stg_churn_events_account_id ........................ [RUN]
[0m20:23:32.360801 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d) - Creating connection
[0m20:23:32.364625 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d'
[0m20:23:32.370660 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:23:32.393203 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:23:32.398910 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:23:32.408708 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:23:32.408708 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:23:32.408708 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_churn_events`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:32.408708 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:37.149662 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-9ee2-193f-ae3b-e863c8bf8209) - Created
[0m20:23:37.663910 [debug] [Thread-2 (]: SQL status: OK in 5.260 seconds
[0m20:23:37.671439 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-9ee2-193f-ae3b-e863c8bf8209, command-id=01f10cd9-9f11-1548-a993-9c63538b04ec) - Closing
[0m20:23:37.675459 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: Close
[0m20:23:37.675459 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-9ee2-193f-ae3b-e863c8bf8209) - Closing
[0m20:23:37.968279 [info ] [Thread-2 (]: 5 of 19 PASS not_null_stg_churn_events_account_id .............................. [[32mPASS[0m in 5.62s]
[0m20:23:37.968279 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:23:37.968279 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:23:37.968279 [info ] [Thread-2 (]: 6 of 19 START test not_null_stg_churn_events_churn_event_id .................... [RUN]
[0m20:23:37.981300 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc) - Creating connection
[0m20:23:37.984627 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc'
[0m20:23:37.985760 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:23:38.009435 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:23:38.020466 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:23:38.034271 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:23:38.038023 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:23:38.042093 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select churn_event_id
from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:38.042093 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:42.579223 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-a222-1290-b07f-46702b93ff36) - Created
[0m20:23:43.083423 [debug] [Thread-2 (]: SQL status: OK in 5.040 seconds
[0m20:23:43.099387 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-a222-1290-b07f-46702b93ff36, command-id=01f10cd9-a24e-159a-b0e1-44f2620ca6a3) - Closing
[0m20:23:43.099387 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: Close
[0m20:23:43.099387 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-a222-1290-b07f-46702b93ff36) - Closing
[0m20:23:43.413183 [info ] [Thread-2 (]: 6 of 19 PASS not_null_stg_churn_events_churn_event_id .......................... [[32mPASS[0m in 5.44s]
[0m20:23:43.418274 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:23:43.418274 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:23:43.418274 [info ] [Thread-2 (]: 7 of 19 START test not_null_stg_feature_usage_subscription_id .................. [RUN]
[0m20:23:43.429183 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3) - Creating connection
[0m20:23:43.433531 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3'
[0m20:23:43.439307 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:23:43.468964 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:23:43.472334 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:23:43.484720 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:23:43.484720 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:23:43.489025 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_feature_usage`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:43.489025 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:47.709360 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-a536-1068-ba36-094f9c4a81ab) - Created
[0m20:23:48.218827 [debug] [Thread-2 (]: SQL status: OK in 4.720 seconds
[0m20:23:48.226653 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-a536-1068-ba36-094f9c4a81ab, command-id=01f10cd9-a565-1bdf-8755-e5c60abb22f4) - Closing
[0m20:23:48.229678 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: Close
[0m20:23:48.233699 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-a536-1068-ba36-094f9c4a81ab) - Closing
[0m20:23:48.512027 [info ] [Thread-2 (]: 7 of 19 PASS not_null_stg_feature_usage_subscription_id ........................ [[32mPASS[0m in 5.08s]
[0m20:23:48.523167 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:23:48.527192 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:23:48.531903 [info ] [Thread-2 (]: 8 of 19 START test not_null_stg_subscriptions_account_id ....................... [RUN]
[0m20:23:48.549455 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9) - Creating connection
[0m20:23:48.551196 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9'
[0m20:23:48.557426 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:23:48.581524 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:23:48.585665 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:23:48.594727 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:23:48.594727 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:23:48.602733 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_subscriptions`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:48.605114 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:52.862270 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-a849-1346-bb6a-0818aa844882) - Created
[0m20:23:53.440573 [debug] [Thread-2 (]: SQL status: OK in 4.840 seconds
[0m20:23:53.452639 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-a849-1346-bb6a-0818aa844882, command-id=01f10cd9-a86f-1249-a7d5-17c6fe9c3a95) - Closing
[0m20:23:53.458666 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: Close
[0m20:23:53.462421 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-a849-1346-bb6a-0818aa844882) - Closing
[0m20:23:53.753269 [info ] [Thread-2 (]: 8 of 19 PASS not_null_stg_subscriptions_account_id ............................. [[32mPASS[0m in 5.21s]
[0m20:23:53.758011 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:23:53.758011 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:23:53.758011 [info ] [Thread-2 (]: 9 of 19 START test not_null_stg_subscriptions_subscription_id .................. [RUN]
[0m20:23:53.769064 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91) - Creating connection
[0m20:23:53.774664 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m20:23:53.776674 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:23:53.820573 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:23:53.822289 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:23:53.844660 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:23:53.847193 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:23:53.849541 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:53.849541 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:23:58.128505 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-ab66-1617-b756-152aa5a40ac6) - Created
[0m20:23:58.757162 [debug] [Thread-2 (]: SQL status: OK in 4.910 seconds
[0m20:23:58.766877 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-ab66-1617-b756-152aa5a40ac6, command-id=01f10cd9-ab94-1083-8620-3b6f5bac7d0d) - Closing
[0m20:23:58.766877 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m20:23:58.766877 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-ab66-1617-b756-152aa5a40ac6) - Closing
[0m20:23:59.068497 [info ] [Thread-2 (]: 9 of 19 PASS not_null_stg_subscriptions_subscription_id ........................ [[32mPASS[0m in 5.30s]
[0m20:23:59.073544 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:23:59.073544 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:23:59.077781 [info ] [Thread-2 (]: 10 of 19 START test not_null_stg_support_tickets_account_id .................... [RUN]
[0m20:23:59.083882 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:23:59.088129 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:23:59.088129 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:23:59.106040 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:23:59.115969 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:23:59.120743 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:23:59.131638 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:23:59.131638 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:23:59.137075 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:03.673209 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-aeb1-195b-a9b5-02ec5465cda6) - Created
[0m20:24:04.911050 [debug] [Thread-2 (]: SQL status: OK in 5.770 seconds
[0m20:24:04.921009 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-aeb1-195b-a9b5-02ec5465cda6, command-id=01f10cd9-aee1-10b5-87da-505ad60351e5) - Closing
[0m20:24:04.924766 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:24:04.924766 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-aeb1-195b-a9b5-02ec5465cda6) - Closing
[0m20:24:05.223699 [info ] [Thread-2 (]: 10 of 19 PASS not_null_stg_support_tickets_account_id .......................... [[32mPASS[0m in 6.14s]
[0m20:24:05.226713 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:24:05.228721 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:24:05.228721 [info ] [Thread-2 (]: 11 of 19 START test not_null_stg_support_tickets_ticket_id ..................... [RUN]
[0m20:24:05.235370 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:24:05.237380 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:24:05.239448 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:24:05.276118 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:24:05.286971 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:24:05.286971 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:24:05.302729 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:24:05.302729 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:24:05.302729 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:10.532566 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-b24b-1e42-8a2e-fa053a5c416d) - Created
[0m20:24:11.046605 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd9-b2f7-1a52-bc34-75b721befea9
[0m20:24:11.050625 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:24:11.052633 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-b24b-1e42-8a2e-fa053a5c416d) - Closing
[0m20:24:11.378291 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:24:11.378291 [error] [Thread-2 (]: 11 of 19 ERROR not_null_stg_support_tickets_ticket_id .......................... [[31mERROR[0m in 6.15s]
[0m20:24:11.378291 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:24:11.378291 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:24:11.378291 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql.
[0m20:24:11.378291 [info ] [Thread-2 (]: 12 of 19 START test relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:24:11.398894 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b) - Creating connection
[0m20:24:11.398894 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b'
[0m20:24:11.403095 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:24:11.431838 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:24:11.434852 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:24:11.440154 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:24:11.451790 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:24:11.453182 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_churn_events`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:24:11.455776 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:15.960254 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-b605-10c5-b077-4c3ce953d76f) - Created
[0m20:24:16.480739 [debug] [Thread-2 (]: SQL status: OK in 5.020 seconds
[0m20:24:16.492805 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-b605-10c5-b077-4c3ce953d76f, command-id=01f10cd9-b635-10cf-ae52-6bd986d5c05c) - Closing
[0m20:24:16.498574 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: Close
[0m20:24:16.502098 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-b605-10c5-b077-4c3ce953d76f) - Closing
[0m20:24:16.882318 [info ] [Thread-2 (]: 12 of 19 PASS relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 5.48s]
[0m20:24:16.889231 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:24:16.893255 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:24:16.895267 [info ] [Thread-2 (]: 13 of 19 START test relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [RUN]
[0m20:24:16.901922 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850) - Creating connection
[0m20:24:16.906005 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850'
[0m20:24:16.909779 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:24:16.958318 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:24:16.968168 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:24:16.974886 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:24:16.983082 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:24:16.987130 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select subscription_id as from_field
    from `workspace`.`analytics`.`stg_feature_usage`
    where subscription_id is not null
),

parent as (
    select subscription_id as to_field
    from `workspace`.`analytics`.`stg_subscriptions`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:24:16.987130 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:21.683180 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-b96e-1c0b-9f3d-c601749ea503) - Created
[0m20:24:22.212663 [debug] [Thread-2 (]: SQL status: OK in 5.230 seconds
[0m20:24:22.228364 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-b96e-1c0b-9f3d-c601749ea503, command-id=01f10cd9-b99e-124b-bf94-95bfabc13cc0) - Closing
[0m20:24:22.234392 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: Close
[0m20:24:22.238412 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-b96e-1c0b-9f3d-c601749ea503) - Closing
[0m20:24:22.517980 [info ] [Thread-2 (]: 13 of 19 PASS relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [[32mPASS[0m in 5.62s]
[0m20:24:22.517980 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:24:22.517980 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:24:22.532884 [info ] [Thread-2 (]: 14 of 19 START test relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:24:22.533825 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90) - Creating connection
[0m20:24:22.540926 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90'
[0m20:24:22.540926 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:24:22.571586 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:24:22.571586 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:24:22.586217 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:24:22.586217 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:24:22.597011 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_subscriptions`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:24:22.599019 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:27.431432 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-bcda-1d83-bc77-0049c95a3e1f) - Created
[0m20:24:27.945048 [debug] [Thread-2 (]: SQL status: OK in 5.350 seconds
[0m20:24:27.951801 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-bcda-1d83-bc77-0049c95a3e1f, command-id=01f10cd9-bd09-1e5b-a9e4-7cd4edc156a1) - Closing
[0m20:24:27.955819 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: Close
[0m20:24:27.957827 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-bcda-1d83-bc77-0049c95a3e1f) - Closing
[0m20:24:28.267783 [info ] [Thread-2 (]: 14 of 19 PASS relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 5.73s]
[0m20:24:28.267783 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:24:28.279847 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:24:28.279847 [info ] [Thread-2 (]: 15 of 19 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:24:28.285301 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:24:28.288243 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:24:28.290250 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:24:28.319984 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:24:28.332862 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:24:28.335090 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:24:28.347910 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:24:28.350789 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:24:28.350789 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:33.060973 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-c036-192a-94e4-082da57a7ebe) - Created
[0m20:24:34.822690 [debug] [Thread-2 (]: SQL status: OK in 6.470 seconds
[0m20:24:34.828724 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-c036-192a-94e4-082da57a7ebe, command-id=01f10cd9-c065-18f7-acbf-8449f50d6aec) - Closing
[0m20:24:34.831855 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:24:34.833402 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-c036-192a-94e4-082da57a7ebe) - Closing
[0m20:24:35.213188 [error] [Thread-2 (]: 15 of 19 FAIL 2000 relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[31mFAIL 2000[0m in 6.93s]
[0m20:24:35.215807 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:24:35.215807 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:24:35.215807 [info ] [Thread-2 (]: 16 of 19 START test unique_stg_accounts_account_id ............................. [RUN]
[0m20:24:35.224075 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m20:24:35.224075 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79'
[0m20:24:35.227723 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:24:35.252449 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:24:35.261411 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:24:35.271761 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:24:35.279109 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:24:35.283441 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:24:35.283441 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:39.716922 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-c42d-162c-8659-1bede9ee2e2b) - Created
[0m20:24:40.233871 [debug] [Thread-2 (]: SQL status: OK in 4.950 seconds
[0m20:24:40.235876 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-c42d-162c-8659-1bede9ee2e2b, command-id=01f10cd9-c45b-1f54-9abc-dfef111d2a79) - Closing
[0m20:24:40.241253 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: Close
[0m20:24:40.241253 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-c42d-162c-8659-1bede9ee2e2b) - Closing
[0m20:24:40.641298 [info ] [Thread-2 (]: 16 of 19 PASS unique_stg_accounts_account_id ................................... [[32mPASS[0m in 5.42s]
[0m20:24:40.643307 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:24:40.643307 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:24:40.643307 [info ] [Thread-2 (]: 17 of 19 START test unique_stg_churn_events_churn_event_id ..................... [RUN]
[0m20:24:40.652632 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947) - Creating connection
[0m20:24:40.654517 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947'
[0m20:24:40.658302 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:24:40.701767 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:24:40.706844 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:24:40.720456 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:24:40.724037 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:24:40.724037 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    churn_event_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is not null
group by churn_event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:24:40.728602 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:45.249648 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-c778-1c5b-9427-c883e49ab687) - Created
[0m20:24:45.712103 [debug] [Thread-2 (]: SQL status: OK in 4.980 seconds
[0m20:24:45.716580 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-c778-1c5b-9427-c883e49ab687, command-id=01f10cd9-c7a8-1a3c-817b-52f0c0083407) - Closing
[0m20:24:45.722136 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: Close
[0m20:24:45.722136 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-c778-1c5b-9427-c883e49ab687) - Closing
[0m20:24:46.639513 [info ] [Thread-2 (]: 17 of 19 PASS unique_stg_churn_events_churn_event_id ........................... [[32mPASS[0m in 6.00s]
[0m20:24:46.649844 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:24:46.651853 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:24:46.655877 [info ] [Thread-2 (]: 18 of 19 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m20:24:46.661913 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e) - Creating connection
[0m20:24:46.666036 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m20:24:46.673342 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:24:46.688362 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:24:46.696994 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:24:46.704092 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:24:46.713605 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:24:46.715012 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:24:46.717777 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:51.187039 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-cb04-16cb-aece-184d20c53358) - Created
[0m20:24:51.714840 [debug] [Thread-2 (]: SQL status: OK in 5.000 seconds
[0m20:24:51.725248 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cd9-cb04-16cb-aece-184d20c53358, command-id=01f10cd9-cb33-14e0-89ea-51a8ec237de1) - Closing
[0m20:24:51.729265 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m20:24:51.733288 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-cb04-16cb-aece-184d20c53358) - Closing
[0m20:24:52.004606 [info ] [Thread-2 (]: 18 of 19 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 5.34s]
[0m20:24:52.012912 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:24:52.014743 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:24:52.014743 [info ] [Thread-2 (]: 19 of 19 START test unique_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:24:52.024302 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:24:52.024302 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:24:52.024302 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:24:52.216913 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:24:52.216913 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:24:52.229618 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:24:52.229618 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:24:52.236686 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:24:52.236686 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:24:56.827992 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-ce5e-1c24-a88b-5a1b781ed90b) - Created
[0m20:24:57.334547 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cd9-ce8f-1174-9ecf-3f1f40c5a038
[0m20:24:57.338561 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:24:57.340569 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cd9-ce5e-1c24-a88b-5a1b781ed90b) - Closing
[0m20:24:57.642119 [debug] [Thread-2 (]: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:24:57.645232 [error] [Thread-2 (]: 19 of 19 ERROR unique_stg_support_tickets_ticket_id ............................ [[31mERROR[0m in 5.62s]
[0m20:24:57.645232 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:24:57.645232 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a' to be skipped because of status 'error'.  Reason: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql.
[0m20:24:57.654655 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:24:57.658043 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:24:57.659245 [info ] [MainThread]: 
[0m20:24:57.663742 [info ] [MainThread]: Finished running 19 data tests in 0 hours 1 minutes and 53.67 seconds (113.67s).
[0m20:24:57.675536 [debug] [MainThread]: Command end result
[0m20:24:57.818871 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:24:57.827082 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:24:57.850744 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:24:57.869474 [info ] [MainThread]: 
[0m20:24:57.869474 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m20:24:57.869474 [info ] [MainThread]: 
[0m20:24:57.869474 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:24:57.880632 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:24:57.885374 [info ] [MainThread]: 
[0m20:24:57.887490 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:24:57.889000 [info ] [MainThread]: 
[0m20:24:57.892153 [error] [MainThread]: [31mFailure in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)[0m
[0m20:24:57.894162 [error] [MainThread]:   Got 2000 results, configured to fail if != 0
[0m20:24:57.898091 [info ] [MainThread]: 
[0m20:24:57.899460 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:24:57.899460 [info ] [MainThread]: 
[0m20:24:57.903957 [error] [MainThread]: [31mFailure in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:24:57.907640 [error] [MainThread]:   Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:24:57.909856 [info ] [MainThread]: 
[0m20:24:57.911863 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:24:57.914741 [info ] [MainThread]: 
[0m20:24:57.915901 [info ] [MainThread]: Done. PASS=16 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=19
[0m20:24:57.915901 [debug] [MainThread]: Command `dbt test` failed at 20:24:57.915901 after 123.99 seconds
[0m20:24:57.922251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3F9D3ABF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3CF57AE60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3CF57A470>]}
[0m20:24:57.922251 [debug] [MainThread]: Flushing usage events
[0m20:25:00.197009 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:29:03.341970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215753AEC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215766F83A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215766F81F0>]}


============================== 20:29:03.345178 | 7913153b-d8c7-40e0-a51f-c7512868ac90 ==============================
[0m20:29:03.345178 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:29:03.354763 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_support_tickets', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:29:06.992823 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:29:06.995027 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:29:06.997306 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:29:09.979862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021578F8FDC0>]}
[0m20:29:10.237021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021509F93D60>]}
[0m20:29:10.240674 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:29:11.868751 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:29:11.870760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002150A15E2C0>]}
[0m20:29:11.928657 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:29:12.843214 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:29:12.843214 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:29:12.843214 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:29:13.011432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002150A7C9F30>]}
[0m20:29:13.392553 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:29:13.400409 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:29:13.445166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002150A7A7A30>]}
[0m20:29:13.447173 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:29:13.447173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002150A8445E0>]}
[0m20:29:13.451136 [info ] [MainThread]: 
[0m20:29:13.451136 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:29:13.451136 [info ] [MainThread]: 
[0m20:29:13.459382 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:29:13.461341 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:29:13.466997 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:29:13.466997 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:29:13.510847 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:29:13.510847 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:29:13.510847 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:29:18.343548 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-6a36-14b3-80e6-92c3eecf1a37) - Created
[0m20:29:19.239114 [debug] [ThreadPool]: SQL status: OK in 5.730 seconds
[0m20:29:19.269628 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cda-6a36-14b3-80e6-92c3eecf1a37, command-id=01f10cda-6a9f-1de9-a9cd-b4bd17961c16) - Closing
[0m20:29:19.270382 [debug] [ThreadPool]: On list_workspace: Close
[0m20:29:19.277361 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-6a36-14b3-80e6-92c3eecf1a37) - Closing
[0m20:29:19.742568 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:29:19.749911 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:29:19.778101 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:29:19.778101 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:29:19.778101 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:29:25.619522 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-6e97-19fd-b0eb-f6128ac2c8fe) - Created
[0m20:29:27.279433 [debug] [ThreadPool]: SQL status: OK in 7.500 seconds
[0m20:29:27.291821 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cda-6e97-19fd-b0eb-f6128ac2c8fe, command-id=01f10cda-6ec7-1019-a088-b001e52fff16) - Closing
[0m20:29:27.295459 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:29:27.300287 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-6e97-19fd-b0eb-f6128ac2c8fe) - Closing
[0m20:29:27.594967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002150A7BAC50>]}
[0m20:29:27.616892 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:29:27.636415 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_support_tickets ...................... [RUN]
[0m20:29:27.636415 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:29:27.643021 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:29:27.645027 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:29:27.692131 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:29:27.694226 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:29:27.758982 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:29:27.771565 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:29:27.774617 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002150A963100>]}
[0m20:29:27.820288 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:29:27.847563 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:29:27.851577 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:29:27.853585 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_support_tickets

),

cleaned as (

    select
        ticket_id
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:29:27.853585 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:29:33.203591 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cda-72b0-1880-a5e9-ff6756874ddd) - Created
[0m20:29:34.741850 [debug] [Thread-3 (]: SQL status: OK in 6.890 seconds
[0m20:29:34.747656 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10cda-72b0-1880-a5e9-ff6756874ddd, command-id=01f10cda-738b-171e-abe7-0431ec1fb563) - Closing
[0m20:29:34.784687 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:29:34.793070 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:29:34.796797 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cda-72b0-1880-a5e9-ff6756874ddd) - Closing
[0m20:29:35.086734 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7913153b-d8c7-40e0-a51f-c7512868ac90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215727E6F80>]}
[0m20:29:35.092347 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_support_tickets ................. [[32mOK[0m in 7.44s]
[0m20:29:35.094355 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:29:35.098373 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:29:35.101543 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:29:35.105559 [info ] [MainThread]: 
[0m20:29:35.108746 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 21.64 seconds (21.64s).
[0m20:29:35.115913 [debug] [MainThread]: Command end result
[0m20:29:35.284181 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:29:35.291071 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:29:35.320742 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:29:35.323604 [info ] [MainThread]: 
[0m20:29:35.325482 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:29:35.327554 [info ] [MainThread]: 
[0m20:29:35.329562 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:29:35.333811 [debug] [MainThread]: Command `dbt run` succeeded at 20:29:35.333811 after 32.29 seconds
[0m20:29:35.337829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215753AEC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215756F0CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021509F93D60>]}
[0m20:29:35.339929 [debug] [MainThread]: Flushing usage events
[0m20:29:37.705913 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:30:09.864476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002091069EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209129B03A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209129B01F0>]}


============================== 20:30:09.864476 | 4e62b041-003d-441f-8d15-f8a01a44fac9 ==============================
[0m20:30:09.864476 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:30:09.880671 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt test --select stg_support_tickets', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:30:13.057014 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:30:13.057014 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:30:13.057014 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:30:15.763236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209129B0460>]}
[0m20:30:15.957872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002091523FEB0>]}
[0m20:30:15.957872 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:30:17.401677 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:30:17.401677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209124C9DB0>]}
[0m20:30:17.458735 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:30:18.237397 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:30:18.237397 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:30:18.237397 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:30:18.425415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002092677A290>]}
[0m20:30:18.769319 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:30:18.777558 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:30:18.837739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209266C3730>]}
[0m20:30:18.839746 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:30:18.842656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209266C3C70>]}
[0m20:30:18.842656 [info ] [MainThread]: 
[0m20:30:18.850869 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:30:18.853526 [info ] [MainThread]: 
[0m20:30:18.854322 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:30:18.854322 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:30:18.887744 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:30:18.887744 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:30:18.935902 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:30:18.937908 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:30:18.940922 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:30:23.380796 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-9104-1ea5-8a6d-d0434dfadd63) - Created
[0m20:30:24.114491 [debug] [ThreadPool]: SQL status: OK in 5.170 seconds
[0m20:30:24.117747 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cda-9104-1ea5-8a6d-d0434dfadd63, command-id=01f10cda-9133-1af7-a218-cc9376201dbd) - Closing
[0m20:30:24.117747 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:30:24.117747 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-9104-1ea5-8a6d-d0434dfadd63) - Closing
[0m20:30:24.398072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e62b041-003d-441f-8d15-f8a01a44fac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209266C3160>]}
[0m20:30:24.413827 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:30:24.425741 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_support_tickets_account_id ...................... [RUN]
[0m20:30:24.429723 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:30:24.429723 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:30:24.429723 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:30:24.538424 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:30:24.540435 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:30:24.603528 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:30:24.619271 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:30:24.619271 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:30:24.619271 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:30:28.911580 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-944f-1742-8475-feb4ae657f9b) - Created
[0m20:30:29.526136 [debug] [Thread-2 (]: SQL status: OK in 4.910 seconds
[0m20:30:29.534553 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cda-944f-1742-8475-feb4ae657f9b, command-id=01f10cda-947e-1ee3-99a9-88cdb5ee9150) - Closing
[0m20:30:29.544588 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:30:29.546595 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-944f-1742-8475-feb4ae657f9b) - Closing
[0m20:30:29.832189 [info ] [Thread-2 (]: 1 of 4 PASS not_null_stg_support_tickets_account_id ............................ [[32mPASS[0m in 5.39s]
[0m20:30:29.832189 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:30:29.840212 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:30:29.842220 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:30:29.849102 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:30:29.851471 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:30:29.855529 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:30:29.882986 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:30:29.882986 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:30:29.899718 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:30:29.899718 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:30:29.899718 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:30:29.913963 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:30:35.258473 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-9819-14a2-b6ef-a3b98053bbf1) - Created
[0m20:30:36.080007 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cda-9847-1eab-bc5e-150d3683f892
[0m20:30:36.084021 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:30:36.086028 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-9819-14a2-b6ef-a3b98053bbf1) - Closing
[0m20:30:37.026961 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:30:37.028969 [error] [Thread-2 (]: 2 of 4 ERROR not_null_stg_support_tickets_ticket_id ............................ [[31mERROR[0m in 7.18s]
[0m20:30:37.034992 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:30:37.039007 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:30:37.040752 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql.
[0m20:30:37.042763 [info ] [Thread-2 (]: 3 of 4 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:30:37.057359 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:30:37.059371 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:30:37.061381 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:30:37.114784 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:30:37.119261 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:30:37.131665 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:30:37.135666 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:30:37.135666 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:30:37.139388 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:30:41.708728 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-9bf1-1dc7-ae40-a4a5adfe7409) - Created
[0m20:30:42.247906 [debug] [Thread-2 (]: SQL status: OK in 5.110 seconds
[0m20:30:42.247906 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cda-9bf1-1dc7-ae40-a4a5adfe7409, command-id=01f10cda-9c29-1a28-905a-f474737458e4) - Closing
[0m20:30:42.247906 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:30:42.247906 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-9bf1-1dc7-ae40-a4a5adfe7409) - Closing
[0m20:30:42.532766 [error] [Thread-2 (]: 3 of 4 FAIL 2000 relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[31mFAIL 2000[0m in 5.48s]
[0m20:30:42.537604 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:30:42.541628 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:30:42.543639 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_support_tickets_ticket_id ......................... [RUN]
[0m20:30:42.549082 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:30:42.554606 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:30:42.554606 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:30:42.614182 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:30:42.614182 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:30:42.636891 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:30:42.636891 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:30:42.645146 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:30:42.645146 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:30:50.006064 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-a0e3-1481-a21c-045946788aa2) - Created
[0m20:30:51.050304 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cda-a112-1f66-9b1e-b7f9fa2c2ec0
[0m20:30:51.054323 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:30:51.058342 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cda-a0e3-1481-a21c-045946788aa2) - Closing
[0m20:30:51.439003 [debug] [Thread-2 (]: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:30:51.439003 [error] [Thread-2 (]: 4 of 4 ERROR unique_stg_support_tickets_ticket_id .............................. [[31mERROR[0m in 8.89s]
[0m20:30:51.439003 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:30:51.454802 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a' to be skipped because of status 'error'.  Reason: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql.
[0m20:30:51.454802 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:30:51.465922 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:30:51.467931 [info ] [MainThread]: 
[0m20:30:51.472239 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 32.61 seconds (32.61s).
[0m20:30:51.485956 [debug] [MainThread]: Command end result
[0m20:30:51.635022 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:30:51.648313 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:30:51.664440 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:30:51.680119 [info ] [MainThread]: 
[0m20:30:51.680119 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m20:30:51.680119 [info ] [MainThread]: 
[0m20:30:51.680119 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:30:51.689793 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:30:51.692439 [info ] [MainThread]: 
[0m20:30:51.696211 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:30:51.701107 [info ] [MainThread]: 
[0m20:30:51.705508 [error] [MainThread]: [31mFailure in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)[0m
[0m20:30:51.705508 [error] [MainThread]:   Got 2000 results, configured to fail if != 0
[0m20:30:51.712932 [info ] [MainThread]: 
[0m20:30:51.712932 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:30:51.721767 [info ] [MainThread]: 
[0m20:30:51.721767 [error] [MainThread]: [31mFailure in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:30:51.728913 [error] [MainThread]:   Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:30:51.729415 [info ] [MainThread]: 
[0m20:30:51.734970 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:30:51.734970 [info ] [MainThread]: 
[0m20:30:51.741306 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=4
[0m20:30:51.750829 [debug] [MainThread]: Command `dbt test` failed at 20:30:51.750829 after 42.16 seconds
[0m20:30:51.753029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002091069EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020911A0AA40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020926980F40>]}
[0m20:30:51.757593 [debug] [MainThread]: Flushing usage events
[0m20:30:55.736668 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:33:04.162744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6CE4EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6F160370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6F1601C0>]}


============================== 20:33:04.171306 | 272b1b18-a860-4285-a52e-87380a6596b6 ==============================
[0m20:33:04.171306 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:33:04.171306 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select stg_support_tickets', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:33:06.371164 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:33:06.371164 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:33:06.373170 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:33:08.251165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B716BB5B0>]}
[0m20:33:08.388232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B71B562F0>]}
[0m20:33:08.388232 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:33:09.339443 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:33:09.341539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B02B8E7D0>]}
[0m20:33:09.377941 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:33:09.956091 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:33:09.963127 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\stg_support_tickets.sql
[0m20:33:10.918330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B03638F40>]}
[0m20:33:11.389582 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:33:11.401133 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:33:11.429556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B0338A740>]}
[0m20:33:11.431564 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:33:11.431564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B03362680>]}
[0m20:33:11.434700 [info ] [MainThread]: 
[0m20:33:11.437137 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:33:11.439426 [info ] [MainThread]: 
[0m20:33:11.440796 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:33:11.441804 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:33:11.445048 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:33:11.447992 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:33:11.477500 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:33:11.480068 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:33:11.480068 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:33:14.630649 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-f71e-12cd-9407-3f2f8e59b9a7) - Created
[0m20:33:15.207827 [debug] [ThreadPool]: SQL status: OK in 3.730 seconds
[0m20:33:15.207827 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cda-f71e-12cd-9407-3f2f8e59b9a7, command-id=01f10cda-f748-14f2-8d5c-b623b4ba6746) - Closing
[0m20:33:15.207827 [debug] [ThreadPool]: On list_workspace: Close
[0m20:33:15.207827 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-f71e-12cd-9407-3f2f8e59b9a7) - Closing
[0m20:33:15.524147 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:33:15.524147 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:33:15.539907 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:33:15.539907 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:33:15.539907 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:33:18.805893 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-f99b-17d4-8b72-1b62dc5281b8) - Created
[0m20:33:19.488251 [debug] [ThreadPool]: SQL status: OK in 3.950 seconds
[0m20:33:19.488251 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cda-f99b-17d4-8b72-1b62dc5281b8, command-id=01f10cda-f9c6-1ba3-adcc-dfcd1cb8d5db) - Closing
[0m20:33:19.488251 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:33:19.488251 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cda-f99b-17d4-8b72-1b62dc5281b8) - Closing
[0m20:33:19.788790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B0338AC80>]}
[0m20:33:19.804525 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:19.804525 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_support_tickets ...................... [RUN]
[0m20:33:19.804525 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:33:19.804525 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:33:19.804525 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:19.826783 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:33:19.826783 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:19.866968 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:33:19.870971 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:33:19.872992 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6E857850>]}
[0m20:33:19.909725 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:33:19.928054 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:33:19.930059 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:33:19.932083 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    select
    ticket_id,
    account_id,
    submitted_at,
    closed_at,
    resolution_time_hours,
    priority,
    first_response_time_minutes,
    satisfaction_score,
    escalation_flag,
    ingestion_ts
from workspace.default.bronze_support_ticket
where ticket_id is not null
  and account_id is not null
  )

[0m20:33:19.933090 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:33:22.972111 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cda-fc18-1cae-83aa-e5291314d074) - Created
[0m20:33:23.588359 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    select
    ticket_id,
    account_id,
    submitted_at,
    closed_at,
    resolution_time_hours,
    priority,
    first_response_time_minutes,
    satisfaction_score,
    escalation_flag,
    ingestion_ts
from workspace.default.bronze_support_ticket
where ticket_id is not null
  and account_id is not null
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`default`.`bronze_support_ticket` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 19 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`default`.`bronze_support_ticket` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 19 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`default`.`bronze_support_ticket` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 19 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cda-fc40-1940-857f-a6a312df990d
[0m20:33:23.588359 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:33:23.588359 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cda-fc18-1cae-83aa-e5291314d074) - Closing
[0m20:33:23.881926 [debug] [Thread-3 (]: Database Error in model stg_support_tickets (models\silver\stg_support_tickets.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`default`.`bronze_support_ticket` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 19 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\stg_support_tickets.sql
[0m20:33:23.881926 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '272b1b18-a860-4285-a52e-87380a6596b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6CD011B0>]}
[0m20:33:23.881926 [error] [Thread-3 (]: 1 of 1 ERROR creating sql view model analytics.stg_support_tickets ............. [[31mERROR[0m in 4.08s]
[0m20:33:23.881926 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:23.881926 [debug] [Thread-6 (]: Marking all children of 'model.saas_dbt_analytics.stg_support_tickets' to be skipped because of status 'error'.  Reason: Database Error in model stg_support_tickets (models\silver\stg_support_tickets.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`default`.`bronze_support_ticket` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 19 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\stg_support_tickets.sql.
[0m20:33:23.881926 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:33:23.898008 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:33:23.900385 [info ] [MainThread]: 
[0m20:33:23.901885 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 12.46 seconds (12.46s).
[0m20:33:23.904198 [debug] [MainThread]: Command end result
[0m20:33:24.000711 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:33:24.008472 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:33:24.009957 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:33:24.025698 [info ] [MainThread]: 
[0m20:33:24.025698 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m20:33:24.025698 [info ] [MainThread]: 
[0m20:33:24.025698 [error] [MainThread]: [31mFailure in model stg_support_tickets (models\silver\stg_support_tickets.sql)[0m
[0m20:33:24.025698 [error] [MainThread]:   Database Error in model stg_support_tickets (models\silver\stg_support_tickets.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`default`.`bronze_support_ticket` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 19 pos 5
  compiled code at target\run\saas_dbt_analytics\models\silver\stg_support_tickets.sql
[0m20:33:24.025698 [info ] [MainThread]: 
[0m20:33:24.035471 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\stg_support_tickets.sql
[0m20:33:24.036404 [info ] [MainThread]: 
[0m20:33:24.038531 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m20:33:24.041981 [debug] [MainThread]: Command `dbt run` failed at 20:33:24.040693 after 20.07 seconds
[0m20:33:24.043189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6CE4EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B71ADA8F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B71AD8EE0>]}
[0m20:33:24.044627 [debug] [MainThread]: Flushing usage events
[0m20:33:26.228479 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:33:42.507874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253F89FEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253FAD183A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253FAD181F0>]}


============================== 20:33:42.511320 | 40579dc9-f120-4d15-bb65-47a5b57b2e31 ==============================
[0m20:33:42.511320 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:33:42.511320 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_support_tickets', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:33:44.499978 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:33:44.499978 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:33:44.499978 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:33:46.190747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253FD6AC160>]}
[0m20:33:46.317373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538E4C6E60>]}
[0m20:33:46.317373 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:33:47.246858 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:33:47.246858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538E632230>]}
[0m20:33:47.300160 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:33:47.851616 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:33:47.867711 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\stg_support_tickets.sql
[0m20:33:48.730148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538F14CBB0>]}
[0m20:33:49.064636 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:33:49.076853 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:33:49.088025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538EE973A0>]}
[0m20:33:49.103207 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:33:49.104717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538EE5C430>]}
[0m20:33:49.104717 [info ] [MainThread]: 
[0m20:33:49.104717 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:33:49.111692 [info ] [MainThread]: 
[0m20:33:49.113147 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:33:49.114225 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:33:49.115256 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:33:49.115256 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:33:49.140351 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:33:49.140351 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:33:49.140351 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:33:52.287597 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-0d8f-1caf-a96f-1bccb3619896) - Created
[0m20:33:52.737930 [debug] [ThreadPool]: SQL status: OK in 3.600 seconds
[0m20:33:52.737930 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cdb-0d8f-1caf-a96f-1bccb3619896, command-id=01f10cdb-0db9-1b16-a21d-2176b3d2aaf8) - Closing
[0m20:33:52.737930 [debug] [ThreadPool]: On list_workspace: Close
[0m20:33:52.737930 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-0d8f-1caf-a96f-1bccb3619896) - Closing
[0m20:33:53.055058 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:33:53.055058 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:33:53.073823 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:33:53.073823 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:33:53.073823 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:33:56.161230 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-0fe2-133f-8df9-4b954a4f4c4d) - Created
[0m20:33:56.722980 [debug] [ThreadPool]: SQL status: OK in 3.650 seconds
[0m20:33:56.729365 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cdb-0fe2-133f-8df9-4b954a4f4c4d, command-id=01f10cdb-1008-1708-9773-91969e8ae81e) - Closing
[0m20:33:56.731372 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:33:56.731372 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-0fe2-133f-8df9-4b954a4f4c4d) - Closing
[0m20:33:57.015068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538EE95E10>]}
[0m20:33:57.023493 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:57.025795 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_support_tickets ...................... [RUN]
[0m20:33:57.027837 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:33:57.029862 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:33:57.029862 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:57.038620 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:33:57.038620 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:33:57.070788 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:33:57.087346 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:33:57.087346 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538EEF5570>]}
[0m20:33:57.123487 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:33:57.140867 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:33:57.140867 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:33:57.140867 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_support_tickets

),

cleaned as (

    select
        ticket_id
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:33:57.140867 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:34:00.098893 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cdb-1239-193d-96db-7d7527d9b9b3) - Created
[0m20:34:01.153053 [debug] [Thread-3 (]: SQL status: OK in 4.010 seconds
[0m20:34:01.153053 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10cdb-1239-193d-96db-7d7527d9b9b3, command-id=01f10cdb-1261-1659-99b4-263e5a616557) - Closing
[0m20:34:01.185188 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:34:01.185188 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:34:01.185188 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cdb-1239-193d-96db-7d7527d9b9b3) - Closing
[0m20:34:01.612017 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '40579dc9-f120-4d15-bb65-47a5b57b2e31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253F6E0AF20>]}
[0m20:34:01.612017 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_support_tickets ................. [[32mOK[0m in 4.58s]
[0m20:34:01.612017 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:34:01.620248 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:34:01.620248 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:34:01.620248 [info ] [MainThread]: 
[0m20:34:01.620248 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 12.51 seconds (12.51s).
[0m20:34:01.630094 [debug] [MainThread]: Command end result
[0m20:34:01.720494 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:34:01.737733 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:34:01.755268 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:34:01.757092 [info ] [MainThread]: 
[0m20:34:01.757092 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:34:01.757092 [info ] [MainThread]: 
[0m20:34:01.757092 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:34:01.757092 [debug] [MainThread]: Command `dbt run` succeeded at 20:34:01.757092 after 19.44 seconds
[0m20:34:01.757092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253F89FEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253F8D536D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002538E4F97B0>]}
[0m20:34:01.757092 [debug] [MainThread]: Flushing usage events
[0m20:34:03.315095 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:34:54.009906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB7E3EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB91803D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB9180220>]}


============================== 20:34:54.009906 | ebbd45c1-6390-4feb-9948-78d03dd2c12e ==============================
[0m20:34:54.009906 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:34:54.009906 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'invocation_command': 'dbt test --select stg_support_tickets', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:34:55.979685 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:34:55.979685 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:34:55.979685 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:34:57.746174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB91801F0>]}
[0m20:34:57.866995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB888EB60>]}
[0m20:34:57.866995 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:34:58.849577 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:34:58.850713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB8C9A3E0>]}
[0m20:34:58.898264 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:34:59.462060 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:34:59.462060 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:34:59.462060 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:34:59.578201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFCCF35ED0>]}
[0m20:34:59.795196 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:34:59.812098 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:34:59.845471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFCCE8D810>]}
[0m20:34:59.845471 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:34:59.845471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFCCE73F10>]}
[0m20:34:59.845471 [info ] [MainThread]: 
[0m20:34:59.845471 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:34:59.861873 [info ] [MainThread]: 
[0m20:34:59.864297 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:34:59.865632 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:34:59.880139 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:34:59.880139 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:34:59.912657 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:34:59.912657 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:34:59.912657 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:35:02.956015 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-37b0-1077-a7a2-2d0721a10917) - Created
[0m20:35:03.563406 [debug] [ThreadPool]: SQL status: OK in 3.650 seconds
[0m20:35:03.572805 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cdb-37b0-1077-a7a2-2d0721a10917, command-id=01f10cdb-37d7-1acb-b87c-0b180d373abb) - Closing
[0m20:35:03.573805 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:35:03.574992 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-37b0-1077-a7a2-2d0721a10917) - Closing
[0m20:35:03.854383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ebbd45c1-6390-4feb-9948-78d03dd2c12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFCCE8DFC0>]}
[0m20:35:03.863174 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:35:03.864191 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_support_tickets_account_id ...................... [RUN]
[0m20:35:03.866438 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:35:03.867439 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:35:03.868456 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:35:03.895804 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:35:03.911939 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:35:03.945714 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:35:03.945714 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:35:03.945714 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:35:03.945714 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:07.224745 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-3a3c-188c-bebd-1f8dc951481b) - Created
[0m20:35:07.656563 [debug] [Thread-2 (]: SQL status: OK in 3.710 seconds
[0m20:35:07.661727 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cdb-3a3c-188c-bebd-1f8dc951481b, command-id=01f10cdb-3a63-1d1c-b5b2-e6a40cf1e572) - Closing
[0m20:35:07.669253 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:35:07.669253 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-3a3c-188c-bebd-1f8dc951481b) - Closing
[0m20:35:07.955336 [info ] [Thread-2 (]: 1 of 4 PASS not_null_stg_support_tickets_account_id ............................ [[32mPASS[0m in 4.09s]
[0m20:35:07.959044 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:35:07.959044 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:35:07.961052 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:35:07.963452 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:35:07.965896 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:35:07.966904 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:35:07.982743 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:35:07.984750 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:35:07.985821 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:35:07.995977 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:35:07.995977 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:35:07.995977 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:11.136954 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-3c91-18ca-8db6-d31391ce813b) - Created
[0m20:35:11.526543 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cdb-3cb8-170e-addf-6732feb4ac0b
[0m20:35:11.528550 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:35:11.530558 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-3c91-18ca-8db6-d31391ce813b) - Closing
[0m20:35:11.817269 [debug] [Thread-2 (]: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:35:11.820994 [error] [Thread-2 (]: 2 of 4 ERROR not_null_stg_support_tickets_ticket_id ............................ [[31mERROR[0m in 3.86s]
[0m20:35:11.823001 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:35:11.825377 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:35:11.826887 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2' to be skipped because of status 'error'.  Reason: Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql.
[0m20:35:11.827842 [info ] [Thread-2 (]: 3 of 4 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:35:11.832682 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:35:11.833755 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:35:11.833755 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:35:11.862830 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:35:11.864830 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:35:11.872366 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:35:11.874374 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:35:11.877516 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:35:11.878516 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:14.997834 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-3edc-1d0d-bd13-92b5649725ff) - Created
[0m20:35:15.468816 [debug] [Thread-2 (]: SQL status: OK in 3.590 seconds
[0m20:35:15.478799 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cdb-3edc-1d0d-bd13-92b5649725ff, command-id=01f10cdb-3f06-1e9c-8a8d-eeeb7d9feda7) - Closing
[0m20:35:15.478799 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:35:15.478799 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-3edc-1d0d-bd13-92b5649725ff) - Closing
[0m20:35:15.779236 [error] [Thread-2 (]: 3 of 4 FAIL 2000 relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[31mFAIL 2000[0m in 3.95s]
[0m20:35:15.782238 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:35:15.783545 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:35:15.784551 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_support_tickets_ticket_id ......................... [RUN]
[0m20:35:15.787154 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:35:15.788155 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:35:15.789160 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:35:15.811305 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:35:15.813902 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:35:15.818386 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:35:15.818386 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:35:15.826998 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:35:15.828301 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:35:18.814620 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-4126-1ca3-b14d-15c4a85cc113) - Created
[0m20:35:19.244839 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10cdb-414d-144b-8ac2-5c7d81e19e38
[0m20:35:19.246878 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:35:19.248916 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-4126-1ca3-b14d-15c4a85cc113) - Closing
[0m20:35:19.527925 [debug] [Thread-2 (]: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:35:19.528936 [error] [Thread-2 (]: 4 of 4 ERROR unique_stg_support_tickets_ticket_id .............................. [[31mERROR[0m in 3.74s]
[0m20:35:19.531028 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:35:19.533049 [debug] [Thread-5 (]: Marking all children of 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a' to be skipped because of status 'error'.  Reason: Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql.
[0m20:35:19.536947 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:35:19.538196 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:35:19.540296 [info ] [MainThread]: 
[0m20:35:19.541301 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 19.68 seconds (19.68s).
[0m20:35:19.543158 [debug] [MainThread]: Command end result
[0m20:35:19.632627 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:35:19.639265 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:35:19.655418 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:35:19.656757 [info ] [MainThread]: 
[0m20:35:19.658802 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m20:35:19.659877 [info ] [MainThread]: 
[0m20:35:19.659877 [error] [MainThread]: [31mFailure in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:35:19.664587 [error] [MainThread]:   Database Error in test not_null_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 18 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:35:19.666715 [info ] [MainThread]: 
[0m20:35:19.667800 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\not_null_stg_support_tickets_ticket_id.sql
[0m20:35:19.668953 [info ] [MainThread]: 
[0m20:35:19.669958 [error] [MainThread]: [31mFailure in test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_ (models\silver\schema.yml)[0m
[0m20:35:19.669958 [error] [MainThread]:   Got 2000 results, configured to fail if != 0
[0m20:35:19.674117 [info ] [MainThread]: 
[0m20:35:19.675430 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\relationships_stg_support_tick_85461c61406a90cbb08cbe8405118ed1.sql
[0m20:35:19.676919 [info ] [MainThread]: 
[0m20:35:19.679269 [error] [MainThread]: [31mFailure in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)[0m
[0m20:35:19.680520 [error] [MainThread]:   Database Error in test unique_stg_support_tickets_ticket_id (models\silver\schema.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `ticket_id` cannot be resolved. Did you mean one of the following? [`account_id`, `closed_at`, `priority`, `ingestion_ts`, `submitted_at`]. SQLSTATE: 42703; line 19 pos 6
  compiled code at target\run\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:35:19.681533 [info ] [MainThread]: 
[0m20:35:19.682538 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\silver\schema.yml\unique_stg_support_tickets_ticket_id.sql
[0m20:35:19.683735 [info ] [MainThread]: 
[0m20:35:19.685042 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=4
[0m20:35:19.688232 [debug] [MainThread]: Command `dbt test` failed at 20:35:19.687230 after 25.86 seconds
[0m20:35:19.688232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB7E3EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFB818C340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AFCD169CF0>]}
[0m20:35:19.689508 [debug] [MainThread]: Flushing usage events
[0m20:35:21.325864 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:39:20.631548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC661DEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC675283A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC675281F0>]}


============================== 20:39:20.631548 | 3f151b8b-a7a7-4b9e-b68f-ac83a00c757c ==============================
[0m20:39:20.631548 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:39:20.647924 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_support_tickets', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:39:22.540662 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:39:22.540662 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:39:22.540662 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:39:24.252808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC6361BDF0>]}
[0m20:39:24.406126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC6734D000>]}
[0m20:39:24.406126 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:39:25.307140 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:39:25.307140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC67005ED0>]}
[0m20:39:25.360069 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:39:25.904957 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:39:25.904957 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\silver\stg_support_tickets.sql
[0m20:39:26.839884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC7B7513F0>]}
[0m20:39:27.182180 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:39:27.182180 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:39:27.214379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC7B4507F0>]}
[0m20:39:27.214379 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:39:27.214379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC7B451480>]}
[0m20:39:27.214379 [info ] [MainThread]: 
[0m20:39:27.214379 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:39:27.228708 [info ] [MainThread]: 
[0m20:39:27.231068 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:39:27.232130 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:39:27.232130 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:39:27.232130 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:39:27.262417 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:39:27.263419 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:39:27.264662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:39:30.387587 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-d717-1ad8-a027-128c6842b915) - Created
[0m20:39:30.992743 [debug] [ThreadPool]: SQL status: OK in 3.730 seconds
[0m20:39:30.992743 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cdb-d717-1ad8-a027-128c6842b915, command-id=01f10cdb-d740-18ca-b22b-6d8fe3264e91) - Closing
[0m20:39:31.006821 [debug] [ThreadPool]: On list_workspace: Close
[0m20:39:31.008566 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-d717-1ad8-a027-128c6842b915) - Closing
[0m20:39:31.307866 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:39:31.311575 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:39:31.321155 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:39:31.321155 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:39:31.321155 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:39:34.607191 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-d998-18ad-8fc3-8a1c2ef9e814) - Created
[0m20:39:35.257971 [debug] [ThreadPool]: SQL status: OK in 3.940 seconds
[0m20:39:35.270089 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cdb-d998-18ad-8fc3-8a1c2ef9e814, command-id=01f10cdb-d9c2-1b63-b0b9-9c0dd343bcd1) - Closing
[0m20:39:35.272095 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:39:35.272095 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-d998-18ad-8fc3-8a1c2ef9e814) - Closing
[0m20:39:35.564402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC7B453760>]}
[0m20:39:35.570443 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:39:35.570443 [info ] [Thread-3 (]: 1 of 1 START sql view model analytics.stg_support_tickets ...................... [RUN]
[0m20:39:35.570443 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:39:35.570443 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:39:35.570443 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:39:35.592723 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:39:35.592723 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:39:35.639001 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:39:35.643327 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:39:35.645332 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC66C134F0>]}
[0m20:39:35.675355 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:39:35.690758 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:39:35.690758 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:39:35.690758 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_support_tickets

),

cleaned as (

    select
        ticket_id,
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:39:35.707509 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:39:38.817863 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cdb-dc1f-179d-9007-ca216b0b889b) - Created
[0m20:39:39.663166 [debug] [Thread-3 (]: SQL status: OK in 3.960 seconds
[0m20:39:39.663166 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10cdb-dc1f-179d-9007-ca216b0b889b, command-id=01f10cdb-dc48-1f5e-ab37-980ab3575b62) - Closing
[0m20:39:39.679036 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:39:39.694739 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:39:39.694739 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10cdb-dc1f-179d-9007-ca216b0b889b) - Closing
[0m20:39:39.960155 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f151b8b-a7a7-4b9e-b68f-ac83a00c757c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC6361AEF0>]}
[0m20:39:39.963618 [info ] [Thread-3 (]: 1 of 1 OK created sql view model analytics.stg_support_tickets ................. [[32mOK[0m in 4.39s]
[0m20:39:39.963618 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:39:39.963618 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:39:39.963618 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:39:39.963618 [info ] [MainThread]: 
[0m20:39:39.974984 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 12.73 seconds (12.73s).
[0m20:39:39.978108 [debug] [MainThread]: Command end result
[0m20:39:40.072492 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:39:40.073352 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:39:40.093514 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:39:40.093514 [info ] [MainThread]: 
[0m20:39:40.093514 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:39:40.093514 [info ] [MainThread]: 
[0m20:39:40.093514 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:39:40.102724 [debug] [MainThread]: Command `dbt run` succeeded at 20:39:40.102724 after 19.63 seconds
[0m20:39:40.103765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC661DEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC7AF35690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC66579ED0>]}
[0m20:39:40.105186 [debug] [MainThread]: Flushing usage events
[0m20:39:41.622412 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:40:05.016567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53A83EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53CB583A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53CB581F0>]}


============================== 20:40:05.032653 | e3473b88-1981-4d70-8815-ba54b9c9edc8 ==============================
[0m20:40:05.032653 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:40:05.032653 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt test --select stg_support_tickets', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:40:07.293818 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:40:07.294819 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:40:07.295818 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:40:09.134635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53CB58460>]}
[0m20:40:09.249039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D54EC87280>]}
[0m20:40:09.249039 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:40:10.135548 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:40:10.135548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53C674A30>]}
[0m20:40:10.186012 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:40:10.701993 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:40:10.701993 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:40:10.701993 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:40:10.801982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D550926320>]}
[0m20:40:11.035232 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:40:11.035232 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:40:11.085525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D55087D1B0>]}
[0m20:40:11.085525 [info ] [MainThread]: Found 5 models, 19 data tests, 845 macros
[0m20:40:11.085525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D55087FBE0>]}
[0m20:40:11.085525 [info ] [MainThread]: 
[0m20:40:11.085525 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:40:11.085525 [info ] [MainThread]: 
[0m20:40:11.100946 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:40:11.102012 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:40:11.121569 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:40:11.122597 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:40:11.137342 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:40:11.151535 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:40:11.153142 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:40:14.116682 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-f129-1f82-a6b8-fa939d5d7dd4) - Created
[0m20:40:14.720555 [debug] [ThreadPool]: SQL status: OK in 3.570 seconds
[0m20:40:14.730789 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10cdb-f129-1f82-a6b8-fa939d5d7dd4, command-id=01f10cdb-f14f-1304-8519-52d2b14ff0cf) - Closing
[0m20:40:14.732830 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:40:14.734343 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10cdb-f129-1f82-a6b8-fa939d5d7dd4) - Closing
[0m20:40:15.007228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e3473b88-1981-4d70-8815-ba54b9c9edc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D55087E140>]}
[0m20:40:15.011990 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:40:15.011990 [info ] [Thread-2 (]: 1 of 4 START test not_null_stg_support_tickets_account_id ...................... [RUN]
[0m20:40:15.011990 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:40:15.021774 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:40:15.023299 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:40:15.051962 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:40:15.051962 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:40:15.102113 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:40:15.102113 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:40:15.102113 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:40:15.102113 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:40:17.997591 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-f37d-1073-a03e-f0f4cef255f7) - Created
[0m20:40:18.710251 [debug] [Thread-2 (]: SQL status: OK in 3.590 seconds
[0m20:40:18.710251 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cdb-f37d-1073-a03e-f0f4cef255f7, command-id=01f10cdb-f3a1-1ac9-9ba8-c3718ea42b99) - Closing
[0m20:40:18.710251 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:40:18.710251 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-f37d-1073-a03e-f0f4cef255f7) - Closing
[0m20:40:18.973444 [info ] [Thread-2 (]: 1 of 4 PASS not_null_stg_support_tickets_account_id ............................ [[32mPASS[0m in 3.96s]
[0m20:40:18.976603 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:40:18.977623 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:40:18.978641 [info ] [Thread-2 (]: 2 of 4 START test not_null_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:40:18.981144 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:40:18.983369 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:40:18.985481 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:40:18.988607 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:40:19.002405 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:40:19.008846 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:40:19.008846 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:40:19.008846 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:40:19.008846 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:40:21.924935 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-f5d1-1218-b3d1-26da52e8754c) - Created
[0m20:40:22.628686 [debug] [Thread-2 (]: SQL status: OK in 3.620 seconds
[0m20:40:22.628686 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cdb-f5d1-1218-b3d1-26da52e8754c, command-id=01f10cdb-f5f9-1c23-8d76-a59a17c48a6c) - Closing
[0m20:40:22.628686 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:40:22.628686 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-f5d1-1218-b3d1-26da52e8754c) - Closing
[0m20:40:22.913772 [info ] [Thread-2 (]: 2 of 4 PASS not_null_stg_support_tickets_ticket_id ............................. [[32mPASS[0m in 3.93s]
[0m20:40:22.913772 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:40:22.913772 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:40:22.931004 [info ] [Thread-2 (]: 3 of 4 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:40:22.933909 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:40:22.934953 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:40:22.936060 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:40:22.956711 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:40:22.956711 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:40:22.970477 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:40:22.971904 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:40:22.973067 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:40:22.974228 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:40:26.307975 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-f86c-1c5b-a1cd-e8365c1bebf0) - Created
[0m20:40:27.535309 [debug] [Thread-2 (]: SQL status: OK in 4.560 seconds
[0m20:40:27.540309 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cdb-f86c-1c5b-a1cd-e8365c1bebf0, command-id=01f10cdb-f894-192f-8de6-41b2aff447aa) - Closing
[0m20:40:27.542605 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:40:27.544608 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-f86c-1c5b-a1cd-e8365c1bebf0) - Closing
[0m20:40:27.822465 [info ] [Thread-2 (]: 3 of 4 PASS relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 4.89s]
[0m20:40:27.824664 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:40:27.825802 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:40:27.827885 [info ] [Thread-2 (]: 4 of 4 START test unique_stg_support_tickets_ticket_id ......................... [RUN]
[0m20:40:27.830403 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:40:27.832653 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:40:27.835657 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:40:27.862073 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:40:27.864145 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:40:27.875563 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:40:27.877891 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:40:27.879398 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:40:27.881425 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:40:31.620773 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-fb94-1058-b930-750b2c9df50b) - Created
[0m20:40:32.343301 [debug] [Thread-2 (]: SQL status: OK in 4.460 seconds
[0m20:40:32.343301 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10cdb-fb94-1058-b930-750b2c9df50b, command-id=01f10cdb-fbbe-1e33-8dc5-6da8b5bfc4c9) - Closing
[0m20:40:32.343301 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:40:32.353737 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10cdb-fb94-1058-b930-750b2c9df50b) - Closing
[0m20:40:32.619415 [info ] [Thread-2 (]: 4 of 4 PASS unique_stg_support_tickets_ticket_id ............................... [[32mPASS[0m in 4.79s]
[0m20:40:32.629789 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:40:32.629789 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:40:32.629789 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:40:32.629789 [info ] [MainThread]: 
[0m20:40:32.629789 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 21.53 seconds (21.53s).
[0m20:40:32.640111 [debug] [MainThread]: Command end result
[0m20:40:32.770024 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:40:32.776483 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:40:32.798121 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:40:32.798121 [info ] [MainThread]: 
[0m20:40:32.798121 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:40:32.798121 [info ] [MainThread]: 
[0m20:40:32.798121 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m20:40:32.808163 [debug] [MainThread]: Command `dbt test` succeeded at 20:40:32.808163 after 27.96 seconds
[0m20:40:32.810712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53A83EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D54EC87280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D53BBADB40>]}
[0m20:40:32.810712 [debug] [MainThread]: Flushing usage events
[0m20:40:34.621863 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:37:26.677840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA997AEBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9BACC370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9BACC1C0>]}


============================== 20:37:26.970831 | f13af7e6-cd20-4b0e-b9bc-8d6b9a885f3f ==============================
[0m20:37:26.970831 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:37:26.970831 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select int_account_support.sql', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:37:29.619761 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:37:29.619761 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:37:29.619761 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:37:32.238088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f13af7e6-cd20-4b0e-b9bc-8d6b9a885f3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9E4D0160>]}
[0m20:37:32.383766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f13af7e6-cd20-4b0e-b9bc-8d6b9a885f3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9E4BA770>]}
[0m20:37:32.383766 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:37:33.352150 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:37:33.368111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'f13af7e6-cd20-4b0e-b9bc-8d6b9a885f3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA9B5DC670>]}
[0m20:37:33.472201 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:37:35.200736 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 7 files added, 0 files changed.
[0m20:37:35.200736 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\schema.yml
[0m20:37:35.200736 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_feature_usage.sql
[0m20:37:35.200736 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_churn.sql
[0m20:37:35.200736 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_360.sql
[0m20:37:35.200736 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\gold\gold_mrr_trends.sql
[0m20:37:35.200736 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_subsriptions.sql
[0m20:37:35.216725 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_support.sql
[0m20:37:35.833571 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'int_account_subscriptions' in the 'models' section of file 'models\intermediate\schema.yml'
[0m20:37:36.063025 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.saas_dbt_analytics.int_account_360' (models\intermediate\int_account_360.sql) depends on a node named 'int_account_subscriptions' which was not found
[0m20:37:36.063025 [debug] [MainThread]: Command `dbt run` failed at 20:37:36.063025 after 9.67 seconds
[0m20:37:36.063025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA997AEBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAAFB82590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAAFB82890>]}
[0m20:37:36.063025 [debug] [MainThread]: Flushing usage events
[0m20:37:38.284570 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:38:16.922745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAD2CBEBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAD4FD0370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAD4FD01C0>]}


============================== 20:38:16.922745 | c59ace56-d60f-4439-a387-d960e28a9303 ==============================
[0m20:38:16.922745 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:38:16.922745 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select int_account_subscriptions.sql', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:38:19.098811 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:38:19.098811 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:38:19.098811 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:38:20.982278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c59ace56-d60f-4439-a387-d960e28a9303', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAD4FD0430>]}
[0m20:38:21.125111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c59ace56-d60f-4439-a387-d960e28a9303', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAD75F15A0>]}
[0m20:38:21.125111 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:38:22.096310 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:38:22.096310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'c59ace56-d60f-4439-a387-d960e28a9303', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAD4AECC10>]}
[0m20:38:22.132097 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:38:22.754603 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 7 files added, 0 files changed.
[0m20:38:22.756432 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_churn.sql
[0m20:38:22.758670 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\schema.yml
[0m20:38:22.759673 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\gold\gold_mrr_trends.sql
[0m20:38:22.760702 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_feature_usage.sql
[0m20:38:22.762684 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_support.sql
[0m20:38:22.763890 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_subsriptions.sql
[0m20:38:22.765489 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_360.sql
[0m20:38:23.428137 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'int_account_subscriptions' in the 'models' section of file 'models\intermediate\schema.yml'
[0m20:38:23.680890 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.saas_dbt_analytics.int_account_360' (models\intermediate\int_account_360.sql) depends on a node named 'int_account_subscriptions' which was not found
[0m20:38:23.682899 [debug] [MainThread]: Command `dbt run` failed at 20:38:23.682899 after 6.94 seconds
[0m20:38:23.685811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAD2CBEBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAE9122D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAE9122BC0>]}
[0m20:38:23.685811 [debug] [MainThread]: Flushing usage events
[0m20:38:26.561134 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:39:03.655282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000132A752EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000132A983C400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000132A983C250>]}


============================== 20:39:03.655282 | 49e2d711-2e34-4737-917f-154b6a619d82 ==============================
[0m20:39:03.655282 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:39:03.655282 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:39:05.827041 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:39:05.827041 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:39:05.827041 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:39:07.683164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '49e2d711-2e34-4737-917f-154b6a619d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000132A593BE20>]}
[0m20:39:07.794380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '49e2d711-2e34-4737-917f-154b6a619d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000132AC2496C0>]}
[0m20:39:07.794380 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:39:08.718855 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:39:08.718855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '49e2d711-2e34-4737-917f-154b6a619d82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000132A91CA020>]}
[0m20:39:08.769286 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:39:09.361413 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 7 files added, 0 files changed.
[0m20:39:09.366962 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\gold\gold_mrr_trends.sql
[0m20:39:09.366962 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_subsriptions.sql
[0m20:39:09.366962 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_support.sql
[0m20:39:09.366962 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\schema.yml
[0m20:39:09.366962 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_churn.sql
[0m20:39:09.366962 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_360.sql
[0m20:39:09.366962 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_feature_usage.sql
[0m20:39:09.996758 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'int_account_subscriptions' in the 'models' section of file 'models\intermediate\schema.yml'
[0m20:39:10.253653 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.saas_dbt_analytics.int_account_360' (models\intermediate\int_account_360.sql) depends on a node named 'int_account_subscriptions' which was not found
[0m20:39:10.258693 [debug] [MainThread]: Command `dbt run` failed at 20:39:10.258693 after 6.80 seconds
[0m20:39:10.258693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000132A752EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000132BD6F7C70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000132BD6F4910>]}
[0m20:39:10.260700 [debug] [MainThread]: Flushing usage events
[0m20:39:12.609154 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:40:04.765168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204E9FEEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204EC2FC400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204EC2FC250>]}


============================== 20:40:04.765168 | b4bafed9-52da-4e48-9ea5-261daac8a786 ==============================
[0m20:40:04.765168 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:40:04.765168 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:40:06.998418 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:40:06.998418 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:40:06.998418 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:40:08.826876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204EED0B4F0>]}
[0m20:40:08.969410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204FE44B280>]}
[0m20:40:08.969410 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:40:10.060196 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:40:10.060196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204EBE1ED70>]}
[0m20:40:10.101095 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:40:10.714003 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 7 files added, 0 files changed.
[0m20:40:10.714003 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_360.sql
[0m20:40:10.714003 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\gold\gold_mrr_trends.sql
[0m20:40:10.714003 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_subscriptions.sql
[0m20:40:10.723489 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_feature_usage.sql
[0m20:40:10.725001 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_support.sql
[0m20:40:10.725001 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\schema.yml
[0m20:40:10.725001 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\intermediate\int_account_churn.sql
[0m20:40:11.945620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204805A4190>]}
[0m20:40:12.416873 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:40:12.462486 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:40:12.510123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020480499180>]}
[0m20:40:12.510123 [info ] [MainThread]: Found 11 models, 32 data tests, 845 macros
[0m20:40:12.525926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204803AC4F0>]}
[0m20:40:12.525926 [info ] [MainThread]: 
[0m20:40:12.525926 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:40:12.534865 [info ] [MainThread]: 
[0m20:40:12.537414 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:40:12.538427 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:40:12.558324 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:40:12.561229 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:40:12.584775 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:40:12.584775 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:40:12.584775 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:40:16.089441 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da5-1cd7-12d1-b560-0d2122d1d8e8) - Created
[0m20:40:30.760187 [debug] [ThreadPool]: SQL status: OK in 18.180 seconds
[0m20:40:30.777287 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10da5-1cd7-12d1-b560-0d2122d1d8e8, command-id=01f10da5-1d12-1728-af28-f861caeddcf8) - Closing
[0m20:40:31.102418 [debug] [ThreadPool]: On list_workspace: Close
[0m20:40:31.102418 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da5-1cd7-12d1-b560-0d2122d1d8e8) - Closing
[0m20:40:31.386686 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:40:31.386686 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:40:31.415641 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:40:31.418586 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:40:31.418586 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:40:34.662068 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da5-27f8-1745-917e-b02f8abae776) - Created
[0m20:40:38.312648 [debug] [ThreadPool]: SQL status: OK in 6.890 seconds
[0m20:40:38.326455 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10da5-27f8-1745-917e-b02f8abae776, command-id=01f10da5-2825-1645-8c03-cf0b63254db8) - Closing
[0m20:40:38.328464 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:40:38.328464 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da5-27f8-1745-917e-b02f8abae776) - Closing
[0m20:40:38.620126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204E5457D30>]}
[0m20:40:38.635912 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_accounts
[0m20:40:38.646491 [info ] [Thread-3 (]: 1 of 10 START sql view model analytics.stg_accounts ............................ [RUN]
[0m20:40:38.646491 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_accounts) - Creating connection
[0m20:40:38.649848 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_accounts'
[0m20:40:38.651591 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_accounts
[0m20:40:38.670751 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_accounts"
[0m20:40:38.707287 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_accounts
[0m20:40:38.743696 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:40:38.746458 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:40:38.746458 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204EBE1C430>]}
[0m20:40:38.780499 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_accounts`
[0m20:40:38.802406 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_accounts"
[0m20:40:38.804513 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_accounts"
[0m20:40:38.806511 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_accounts: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_accounts"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_accounts`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_accounts

),

cleaned as (

    select
        account_id,
        account_name,
        industry,
        country,
        signup_date,
        referral_source,
        plan_tier,
        seats,
        is_trial,
        churn_flag,
        ingestion_ts

    from source

    where account_id is not null

)

select *
from cleaned
  )

[0m20:40:38.807512 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:40:42.005379 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-2c5b-11a3-b1b2-1911f3adaa01) - Created
[0m20:40:45.828149 [debug] [Thread-3 (]: SQL status: OK in 7.020 seconds
[0m20:40:45.841960 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da5-2c5b-11a3-b1b2-1911f3adaa01, command-id=01f10da5-2c86-1c69-b683-51bc6ecbcf20) - Closing
[0m20:40:45.857680 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:40:45.857680 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_accounts: Close
[0m20:40:45.873614 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-2c5b-11a3-b1b2-1911f3adaa01) - Closing
[0m20:40:46.158644 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048057CA30>]}
[0m20:40:46.158644 [info ] [Thread-3 (]: 1 of 10 OK created sql view model analytics.stg_accounts ....................... [[32mOK[0m in 7.51s]
[0m20:40:46.174382 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_accounts
[0m20:40:46.174382 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_churn_events
[0m20:40:46.174382 [info ] [Thread-3 (]: 2 of 10 START sql view model analytics.stg_churn_events ........................ [RUN]
[0m20:40:46.181246 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_churn_events) - Creating connection
[0m20:40:46.183374 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_churn_events'
[0m20:40:46.185129 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_churn_events
[0m20:40:46.202903 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_churn_events"
[0m20:40:46.202903 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_churn_events
[0m20:40:46.214928 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:40:46.217110 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_churn_events`
[0m20:40:46.217110 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_churn_events"
[0m20:40:46.217110 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_churn_events"
[0m20:40:46.217110 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_churn_events: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_churn_events"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_churn_events`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_churn_events

),

cleaned as (

    select
        churn_event_id,
        account_id,
        churn_date,
        reason_code,
        refund_amount_usd,
        preceding_upgrade_flag,
        preceding_downgrade_flag,
        is_reactivation,
        feedback_text,
        ingestion_ts

    from source

    where churn_event_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:40:46.229422 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:40:49.325038 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-30be-1209-bfb0-edd7ef1c254f) - Created
[0m20:40:51.204993 [debug] [Thread-3 (]: SQL status: OK in 4.980 seconds
[0m20:40:51.204993 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da5-30be-1209-bfb0-edd7ef1c254f, command-id=01f10da5-30e2-1682-a52d-abc02c7cd542) - Closing
[0m20:40:51.217125 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:40:51.219712 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_churn_events: Close
[0m20:40:51.219712 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-30be-1209-bfb0-edd7ef1c254f) - Closing
[0m20:40:51.485095 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204E9E122C0>]}
[0m20:40:51.490333 [info ] [Thread-3 (]: 2 of 10 OK created sql view model analytics.stg_churn_events ................... [[32mOK[0m in 5.31s]
[0m20:40:51.490333 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_churn_events
[0m20:40:51.490333 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_feature_usage
[0m20:40:51.490333 [info ] [Thread-3 (]: 3 of 10 START sql view model analytics.stg_feature_usage ....................... [RUN]
[0m20:40:51.498745 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_feature_usage) - Creating connection
[0m20:40:51.500600 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_feature_usage'
[0m20:40:51.502801 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_feature_usage
[0m20:40:51.516594 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_feature_usage"
[0m20:40:51.545350 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_feature_usage
[0m20:40:51.564242 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:40:51.568275 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_feature_usage`
[0m20:40:51.571319 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_feature_usage"
[0m20:40:51.573407 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_feature_usage"
[0m20:40:51.575428 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_feature_usage: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_feature_usage"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_feature_usage`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_feature_usage
    where subscription_id is not null

),

aggregated as (

    select
        subscription_id,
        usage_date,
        feature_name,

        sum(usage_count)              as total_usage_count,
        sum(usage_duration_secs)      as total_usage_duration_secs,
        sum(error_count)              as total_error_count,

        max(is_beta_feature)          as is_beta_feature

    from source
    group by
        subscription_id,
        usage_date,
        feature_name

)

select *
from aggregated
  )

[0m20:40:51.576437 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:40:54.731434 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-33f5-15b8-97b9-d9ac2af5b0fa) - Created
[0m20:40:56.821484 [debug] [Thread-3 (]: SQL status: OK in 5.250 seconds
[0m20:40:56.837596 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da5-33f5-15b8-97b9-d9ac2af5b0fa, command-id=01f10da5-341c-1306-9357-9722ec0b20dc) - Closing
[0m20:40:56.837596 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:40:56.837596 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_feature_usage: Close
[0m20:40:56.837596 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-33f5-15b8-97b9-d9ac2af5b0fa) - Closing
[0m20:40:57.109475 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048031EFB0>]}
[0m20:40:57.116009 [info ] [Thread-3 (]: 3 of 10 OK created sql view model analytics.stg_feature_usage .................. [[32mOK[0m in 5.62s]
[0m20:40:57.116009 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_feature_usage
[0m20:40:57.116009 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_subscriptions
[0m20:40:57.116009 [info ] [Thread-3 (]: 4 of 10 START sql view model analytics.stg_subscriptions ....................... [RUN]
[0m20:40:57.125107 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_subscriptions) - Creating connection
[0m20:40:57.126903 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_subscriptions'
[0m20:40:57.128070 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_subscriptions
[0m20:40:57.145051 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_subscriptions"
[0m20:40:57.148452 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_subscriptions
[0m20:40:57.150924 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:40:57.150924 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_subscriptions`
[0m20:40:57.161779 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_subscriptions"
[0m20:40:57.164698 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_subscriptions"
[0m20:40:57.164698 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_subscriptions"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_subscriptions`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_subscriptions

),

cleaned as (

    select
        subscription_id,
        account_id,
        start_date,
        end_date,
        plan_tier,
        seats,
        mrr_amount,
        arr_amount,
        is_trial,
        upgrade_flag,
        downgrade_flag,
        churn_flag,
        billing_frequency,
        auto_renew_flag,
        ingestion_ts

    from source

    where subscription_id is not null
      and account_id is not null

)

select *
from cleaned
  )

[0m20:40:57.164698 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:41:00.296844 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-3746-1eef-87f0-8902013bd39e) - Created
[0m20:41:01.991400 [debug] [Thread-3 (]: SQL status: OK in 4.830 seconds
[0m20:41:01.991400 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da5-3746-1eef-87f0-8902013bd39e, command-id=01f10da5-376e-13e0-a419-15f4fc2d9daf) - Closing
[0m20:41:01.991400 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:41:01.991400 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_subscriptions: Close
[0m20:41:01.991400 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-3746-1eef-87f0-8902013bd39e) - Closing
[0m20:41:02.271606 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048031C0D0>]}
[0m20:41:02.271606 [info ] [Thread-3 (]: 4 of 10 OK created sql view model analytics.stg_subscriptions .................. [[32mOK[0m in 5.16s]
[0m20:41:02.277463 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_subscriptions
[0m20:41:02.277463 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:41:02.280621 [info ] [Thread-3 (]: 5 of 10 START sql view model analytics.stg_support_tickets ..................... [RUN]
[0m20:41:02.284080 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:41:02.285336 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:41:02.287824 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:41:02.296056 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:41:02.298112 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:41:02.300198 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:41:02.300198 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:41:02.315252 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:41:02.315252 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:41:02.315252 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_support_tickets

),

cleaned as (

    select
        ticket_id,
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:41:02.315252 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:41:05.482921 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-3a5e-1c7b-b397-709bc2d6f6d6) - Created
[0m20:41:07.396249 [debug] [Thread-3 (]: SQL status: OK in 5.080 seconds
[0m20:41:07.398257 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da5-3a5e-1c7b-b397-709bc2d6f6d6, command-id=01f10da5-3a84-19e3-90b7-8407ec2a1317) - Closing
[0m20:41:07.400265 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:41:07.404018 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:41:07.404018 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-3a5e-1c7b-b397-709bc2d6f6d6) - Closing
[0m20:41:07.685117 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204803B4BB0>]}
[0m20:41:07.685117 [info ] [Thread-3 (]: 5 of 10 OK created sql view model analytics.stg_support_tickets ................ [[32mOK[0m in 5.40s]
[0m20:41:07.685117 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:41:07.685117 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.int_account_churn
[0m20:41:07.685117 [info ] [Thread-3 (]: 6 of 10 START sql view model analytics.int_account_churn ....................... [RUN]
[0m20:41:07.685117 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.int_account_churn) - Creating connection
[0m20:41:07.697675 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.int_account_churn'
[0m20:41:07.698806 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.int_account_churn
[0m20:41:07.712848 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.int_account_churn"
[0m20:41:07.715389 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.int_account_churn
[0m20:41:07.716896 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:41:07.726888 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`int_account_churn`
[0m20:41:07.726888 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.int_account_churn"
[0m20:41:07.743904 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.int_account_churn"
[0m20:41:07.743904 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_churn: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.int_account_churn"} */

  
  
  create or replace view `workspace`.`analytics`.`int_account_churn`
  
  as (
    /*
    Grain: one row per churn event
    Enriches churn events with account and subscription context (plan at churn, MRR lost).
    Used by: gold_churn_analysis, int_account_360
*/

with churn as (

    select
        churn_event_id,
        account_id,
        churn_date,
        reason_code,
        refund_amount_usd,
        preceding_upgrade_flag,
        preceding_downgrade_flag,
        is_reactivation,
        feedback_text

    from `workspace`.`analytics`.`stg_churn_events`

),

accounts as (

    select
        account_id,
        account_name,
        industry,
        country,
        plan_tier     as account_plan_tier,
        seats,
        signup_date

    from `workspace`.`analytics`.`stg_accounts`

),

-- Get the most recent subscription per account to capture MRR lost at churn
latest_subscriptions as (

    select
        account_id,
        plan_tier     as subscription_plan_tier,
        mrr_amount    as mrr_at_churn,
        arr_amount    as arr_at_churn,
        billing_frequency,
        row_number() over (
            partition by account_id
            order by start_date desc
        ) as rn

    from `workspace`.`analytics`.`stg_subscriptions`

),

enriched as (

    select
        c.churn_event_id,
        c.account_id,
        a.account_name,
        a.industry,
        a.country,
        a.signup_date,

        -- Tenure at churn (days from signup to churn)
        datediff(c.churn_date, a.signup_date) as days_to_churn,

        c.churn_date,
        c.reason_code,
        c.refund_amount_usd,
        c.preceding_upgrade_flag,
        c.preceding_downgrade_flag,
        c.is_reactivation,
        c.feedback_text,

        -- Revenue context
        coalesce(s.subscription_plan_tier, a.account_plan_tier) as plan_tier_at_churn,
        s.mrr_at_churn,
        s.arr_at_churn,
        s.billing_frequency,

        -- Segment churn risk signal
        case
            when c.preceding_downgrade_flag = true then 'downgrade_to_churn'
            when c.preceding_upgrade_flag = true  then 'upgrade_to_churn'
            else 'direct_churn'
        end as churn_path

    from churn c
    left join accounts a using (account_id)
    left join latest_subscriptions s
        on c.account_id = s.account_id and s.rn = 1

)

select * from enriched
  )

[0m20:41:07.743904 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:41:10.900330 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-3d97-116a-a0a4-b7b7c6d3cb7b) - Created
[0m20:41:13.287277 [debug] [Thread-3 (]: SQL status: OK in 5.540 seconds
[0m20:41:13.289284 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da5-3d97-116a-a0a4-b7b7c6d3cb7b, command-id=01f10da5-3e14-1893-8c06-d5343ba09066) - Closing
[0m20:41:13.291291 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:41:13.295306 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_churn: Close
[0m20:41:13.295306 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-3d97-116a-a0a4-b7b7c6d3cb7b) - Closing
[0m20:41:13.594589 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020480327070>]}
[0m20:41:13.596599 [info ] [Thread-3 (]: 6 of 10 OK created sql view model analytics.int_account_churn .................. [[32mOK[0m in 5.91s]
[0m20:41:13.598606 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.int_account_churn
[0m20:41:13.598606 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.int_account_feature_usage
[0m20:41:13.600613 [info ] [Thread-3 (]: 7 of 10 START sql view model analytics.int_account_feature_usage ............... [RUN]
[0m20:41:13.603793 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.int_account_feature_usage) - Creating connection
[0m20:41:13.606152 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.int_account_feature_usage'
[0m20:41:13.607917 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.int_account_feature_usage
[0m20:41:13.621011 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.int_account_feature_usage"
[0m20:41:13.625922 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.int_account_feature_usage
[0m20:41:13.631229 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:41:13.638796 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`int_account_feature_usage`
[0m20:41:13.643328 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.int_account_feature_usage"
[0m20:41:13.645675 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.int_account_feature_usage"
[0m20:41:13.647683 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_feature_usage: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.int_account_feature_usage"} */

  
  
  create or replace view `workspace`.`analytics`.`int_account_feature_usage`
  
  as (
    /*
    Grain: one row per account
    Aggregates feature usage across all subscriptions per account.
    Used by: gold_feature_adoption, int_account_360
*/

with usage as (

    select
        subscription_id,
        usage_date,
        feature_name,
        total_usage_count,
        total_usage_duration_secs,
        total_error_count,
        is_beta_feature

    from `workspace`.`analytics`.`stg_feature_usage`

),

subscriptions as (

    select
        subscription_id,
        account_id

    from `workspace`.`analytics`.`stg_subscriptions`

),

-- Join usage to accounts via subscriptions
usage_with_account as (

    select
        s.account_id,
        u.subscription_id,
        u.usage_date,
        u.feature_name,
        u.total_usage_count,
        u.total_usage_duration_secs,
        u.total_error_count,
        u.is_beta_feature

    from usage u
    inner join subscriptions s using (subscription_id)

),

aggregated as (

    select
        account_id,

        -- Overall usage
        count(distinct feature_name)                            as distinct_features_used,
        sum(total_usage_count)                                  as total_usage_count,
        round(sum(total_usage_duration_secs) / 3600.0, 2)      as total_usage_hours,
        sum(total_error_count)                                  as total_error_count,
        round(
            sum(total_error_count) * 100.0
            / nullif(sum(total_usage_count), 0), 2
        )                                                       as error_rate_pct,

        -- Beta adoption
        count(distinct case when is_beta_feature = true
            then feature_name end)                              as beta_features_used,

        -- Most used feature
        max_by(feature_name, total_usage_count)                 as top_feature,

        -- Recency
        max(usage_date)                                         as last_usage_date,
        min(usage_date)                                         as first_usage_date,
        count(distinct usage_date)                              as active_days

    from usage_with_account
    group by account_id

)

select * from aggregated
  )

[0m20:41:13.649690 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:41:16.902625 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-412c-1152-969b-83112270bd43) - Created
[0m20:41:18.049037 [debug] [Thread-3 (]: SQL status: OK in 4.400 seconds
[0m20:41:18.059756 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da5-412c-1152-969b-83112270bd43, command-id=01f10da5-4154-1c3f-8a2b-ed64d0d2264c) - Closing
[0m20:41:18.061768 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:41:18.064677 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_feature_usage: Close
[0m20:41:18.064677 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-412c-1152-969b-83112270bd43) - Closing
[0m20:41:18.362900 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048053FD00>]}
[0m20:41:18.362900 [info ] [Thread-3 (]: 7 of 10 OK created sql view model analytics.int_account_feature_usage .......... [[32mOK[0m in 4.76s]
[0m20:41:18.362900 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.int_account_feature_usage
[0m20:41:18.362900 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.int_account_subscriptions
[0m20:41:18.362900 [info ] [Thread-3 (]: 8 of 10 START sql view model analytics.int_account_subscriptions ............... [RUN]
[0m20:41:18.374269 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.int_account_subscriptions) - Creating connection
[0m20:41:18.376533 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.int_account_subscriptions'
[0m20:41:18.379250 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.int_account_subscriptions
[0m20:41:18.393474 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.int_account_subscriptions"
[0m20:41:18.395950 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.int_account_subscriptions
[0m20:41:18.399352 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:41:18.408349 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`int_account_subscriptions`
[0m20:41:18.409321 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.int_account_subscriptions"
[0m20:41:18.409321 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.int_account_subscriptions"
[0m20:41:18.409321 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.int_account_subscriptions"} */

  
  
  create or replace view `workspace`.`analytics`.`int_account_subscriptions`
  
  as (
    /*
    Grain: one row per subscription
    Joins accounts with subscriptions to build subscription-level revenue context.
    Used by: gold_mrr_trends, int_account_360
*/

with accounts as (

    select
        account_id,
        account_name,
        industry,
        country,
        signup_date,
        referral_source

    from `workspace`.`analytics`.`stg_accounts`

),

subscriptions as (

    select
        subscription_id,
        account_id,
        start_date,
        end_date,
        plan_tier,
        seats,
        mrr_amount,
        arr_amount,
        is_trial,
        upgrade_flag,
        downgrade_flag,
        churn_flag,
        billing_frequency,
        auto_renew_flag,

        -- Derived fields
        case
            when end_date is null then true
            else false
        end as is_active,

        datediff(
            coalesce(end_date, current_date()),
            start_date
        ) as subscription_duration_days

    from `workspace`.`analytics`.`stg_subscriptions`

),

joined as (

    select
        s.subscription_id,
        s.account_id,
        a.account_name,
        a.industry,
        a.country,
        a.signup_date,
        a.referral_source,
        s.start_date,
        s.end_date,
        s.plan_tier,
        s.seats,
        s.mrr_amount,
        s.arr_amount,
        s.is_trial,
        s.upgrade_flag,
        s.downgrade_flag,
        s.churn_flag,
        s.billing_frequency,
        s.auto_renew_flag,
        s.is_active,
        s.subscription_duration_days

    from subscriptions s
    left join accounts a using (account_id)

)

select * from joined
  )

[0m20:41:18.409321 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:41:21.748486 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-440e-14d2-baf6-015f7dfb69d2) - Created
[0m20:41:23.328676 [debug] [Thread-3 (]: SQL status: OK in 4.920 seconds
[0m20:41:23.332428 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da5-440e-14d2-baf6-015f7dfb69d2, command-id=01f10da5-4437-1585-a039-fb325bc554ed) - Closing
[0m20:41:23.334438 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:41:23.336447 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_subscriptions: Close
[0m20:41:23.336447 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-440e-14d2-baf6-015f7dfb69d2) - Closing
[0m20:41:23.629119 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020480083A30>]}
[0m20:41:23.629119 [info ] [Thread-3 (]: 8 of 10 OK created sql view model analytics.int_account_subscriptions .......... [[32mOK[0m in 5.27s]
[0m20:41:23.629119 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.int_account_subscriptions
[0m20:41:23.637094 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.int_account_support
[0m20:41:23.639102 [info ] [Thread-3 (]: 9 of 10 START sql view model analytics.int_account_support ..................... [RUN]
[0m20:41:23.642315 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.int_account_support) - Creating connection
[0m20:41:23.644772 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.int_account_support'
[0m20:41:23.645830 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.int_account_support
[0m20:41:23.661841 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.int_account_support"
[0m20:41:23.664116 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.int_account_support
[0m20:41:23.675101 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:41:23.675101 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`int_account_support`
[0m20:41:23.683766 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.int_account_support"
[0m20:41:23.687005 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.int_account_support"
[0m20:41:23.687005 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_support: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.int_account_support"} */

  
  
  create or replace view `workspace`.`analytics`.`int_account_support`
  
  as (
    /*
    Grain: one row per account
    Aggregates support ticket metrics per account.
 */

with tickets as (

    select
        ticket_id,
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag

    from `workspace`.`analytics`.`stg_support_tickets`

),

aggregated as (

    select
        account_id,

        -- Volume
        count(ticket_id)                                        as total_tickets,
        count(case when priority = 'urgent' then 1 end)         as urgent_tickets,
        count(case when priority = 'high' then 1 end)           as high_priority_tickets,
        count(case when escalation_flag = true then 1 end)      as escalated_tickets,

        -- Resolution quality
        round(avg(resolution_time_hours), 2)                    as avg_resolution_time_hours,
        round(avg(first_response_time_minutes), 2)              as avg_first_response_time_minutes,
        round(avg(satisfaction_score), 2)                       as avg_satisfaction_score,
        min(satisfaction_score)                                 as min_satisfaction_score,

        -- Rates
        round(
            count(case when escalation_flag = true then 1 end) * 100.0
            / nullif(count(ticket_id), 0), 2
        )                                                       as escalation_rate_pct,

        -- Recency
        max(submitted_at)                                       as last_ticket_date,
        min(submitted_at)                                       as first_ticket_date

    from tickets
    group by account_id

)

select * from aggregated
  )

[0m20:41:23.689805 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:41:27.079024 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-473a-1973-b2a7-e7117ba1bcb7) - Created
[0m20:41:28.219186 [debug] [Thread-3 (]: SQL status: OK in 4.530 seconds
[0m20:41:28.219186 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da5-473a-1973-b2a7-e7117ba1bcb7, command-id=01f10da5-4764-1e8b-88e3-2292b49babfa) - Closing
[0m20:41:28.219186 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:41:28.219186 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_support: Close
[0m20:41:28.219186 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-473a-1973-b2a7-e7117ba1bcb7) - Closing
[0m20:41:28.504626 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204E9E122C0>]}
[0m20:41:28.504626 [info ] [Thread-3 (]: 9 of 10 OK created sql view model analytics.int_account_support ................ [[32mOK[0m in 4.87s]
[0m20:41:28.504626 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.int_account_support
[0m20:41:28.504626 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.int_account_360
[0m20:41:28.504626 [info ] [Thread-3 (]: 10 of 10 START sql table model analytics.int_account_360 ....................... [RUN]
[0m20:41:28.519269 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.int_account_360) - Creating connection
[0m20:41:28.522076 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.int_account_360'
[0m20:41:28.524221 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.int_account_360
[0m20:41:28.543993 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.int_account_360"
[0m20:41:28.546165 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.int_account_360
[0m20:41:28.596773 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m20:41:28.686105 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.int_account_360"
[0m20:41:28.689208 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.int_account_360"
[0m20:41:28.691248 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_360: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.int_account_360"} */

  
    
        create or replace table `workspace`.`analytics`.`int_account_360`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      /*
    Grain: one row per account
    The central account spine joining all intermediate models.
    Materialized as table because it's an expensive join used by multiple gold models.
    Used by: gold_account_health_summary, gold_customer_segments
*/

with accounts as (

    select
        account_id,
        account_name,
        industry,
        country,
        signup_date,
        referral_source,
        plan_tier,
        seats,
        is_trial,
        churn_flag

    from `workspace`.`analytics`.`stg_accounts`

),

-- Aggregate subscriptions to account level
subscription_summary as (

    select
        account_id,
        count(subscription_id)                                          as total_subscriptions,
        sum(mrr_amount)                                                 as total_mrr,
        sum(arr_amount)                                                 as total_arr,
        max(mrr_amount)                                                 as peak_mrr,
        count(case when is_active = true then 1 end)                    as active_subscriptions,
        max(case when is_active = true then plan_tier end)              as current_plan_tier,
        max(case when is_active = true then mrr_amount end)             as current_mrr,
        bool_or(upgrade_flag)                                           as ever_upgraded,
        bool_or(downgrade_flag)                                         as ever_downgraded,
        max(start_date)                                                 as latest_subscription_start

    from `workspace`.`analytics`.`int_account_subscriptions`
    group by account_id

),

-- Latest churn event per account
churn_summary as (

    select
        account_id,
        churn_date,
        reason_code                                                     as churn_reason,
        refund_amount_usd,
        churn_path,
        days_to_churn,
        mrr_at_churn

    from `workspace`.`analytics`.`int_account_churn`
    qualify row_number() over (partition by account_id order by churn_date desc) = 1

),

support as (

    select * from `workspace`.`analytics`.`int_account_support`

),

feature_usage as (

    select * from `workspace`.`analytics`.`int_account_feature_usage`

),

joined as (

    select
        -- Account core
        a.account_id,
        a.account_name,
        a.industry,
        a.country,
        a.signup_date,
        a.referral_source,
        a.plan_tier          as original_plan_tier,
        a.seats,
        a.is_trial,
        a.churn_flag,

        -- Subscription summary
        s.total_subscriptions,
        s.total_mrr,
        s.total_arr,
        s.peak_mrr,
        s.active_subscriptions,
        s.current_plan_tier,
        s.current_mrr,
        s.ever_upgraded,
        s.ever_downgraded,
        s.latest_subscription_start,

        -- Churn context (null if not churned)
        c.churn_date,
        c.churn_reason,
        c.refund_amount_usd,
        c.churn_path,
        c.days_to_churn,
        c.mrr_at_churn,

        -- Support metrics
        coalesce(t.total_tickets, 0)                                    as total_tickets,
        t.avg_resolution_time_hours,
        t.avg_satisfaction_score,
        coalesce(t.escalation_rate_pct, 0)                             as escalation_rate_pct,
        t.last_ticket_date,

        -- Feature usage metrics
        coalesce(u.distinct_features_used, 0)                          as distinct_features_used,
        coalesce(u.total_usage_count, 0)                               as total_usage_count,
        u.total_usage_hours,
        coalesce(u.error_rate_pct, 0)                                  as error_rate_pct,
        coalesce(u.beta_features_used, 0)                              as beta_features_used,
        u.top_feature,
        u.last_usage_date,
        coalesce(u.active_days, 0)                                     as active_days,

        -- Computed health score (0–100)
        -- Higher is healthier. Weighted across usage, support satisfaction, and retention signals.
        round(
            least(100,
                -- Usage engagement (40 pts max)
                least(40, coalesce(u.active_days, 0) * 0.5)
                -- Satisfaction (30 pts max)
                + coalesce(t.avg_satisfaction_score, 3) / 5.0 * 30
                -- No escalations (15 pts)
                + case when coalesce(t.escalation_rate_pct, 0) = 0 then 15 else 0 end
                -- Feature breadth (15 pts max)
                + least(15, coalesce(u.distinct_features_used, 0) * 1.5)
            ), 2
        )                                                               as health_score

    from accounts a
    left join subscription_summary s using (account_id)
    left join churn_summary c using (account_id)
    left join support t using (account_id)
    left join feature_usage u using (account_id)

)

select * from joined
  
[0m20:41:28.693250 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:41:32.110916 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-4a39-1921-9890-8e3014a091cc) - Created
[0m20:41:49.701130 [debug] [Thread-3 (]: SQL status: OK in 21.010 seconds
[0m20:41:49.701130 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da5-4a39-1921-9890-8e3014a091cc, command-id=01f10da5-4a65-11e1-aec8-486fdf7a3f45) - Closing
[0m20:41:50.029817 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:41:50.060869 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_360: Close
[0m20:41:50.060869 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da5-4a39-1921-9890-8e3014a091cc) - Closing
[0m20:41:50.352739 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4bafed9-52da-4e48-9ea5-261daac8a786', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204EC2B0B20>]}
[0m20:41:50.361470 [info ] [Thread-3 (]: 10 of 10 OK created sql table model analytics.int_account_360 .................. [[32mOK[0m in 21.83s]
[0m20:41:50.364079 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.int_account_360
[0m20:41:50.366085 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:41:50.366085 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:41:50.369849 [info ] [MainThread]: 
[0m20:41:50.371857 [info ] [MainThread]: Finished running 1 table model, 9 view models in 0 hours 1 minutes and 37.83 seconds (97.83s).
[0m20:41:50.379306 [debug] [MainThread]: Command end result
[0m20:41:50.486287 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:41:50.494145 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:41:50.509201 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:41:50.511207 [info ] [MainThread]: 
[0m20:41:50.511207 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:41:50.511207 [info ] [MainThread]: 
[0m20:41:50.511207 [info ] [MainThread]: Done. PASS=10 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=10
[0m20:41:50.518096 [debug] [MainThread]: Command `dbt run` succeeded at 20:41:50.518096 after 105.94 seconds
[0m20:41:50.520606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204E9FEEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204E9DF66E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048049B0A0>]}
[0m20:41:50.522451 [debug] [MainThread]: Flushing usage events
[0m20:41:52.422503 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:42:11.492500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A57AFEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A59E183A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A59E181F0>]}


============================== 20:42:11.492500 | dfac5b36-3977-4524-97cd-1cac122c1075 ==============================
[0m20:42:11.492500 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:42:11.492500 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'use_colors': 'True', 'invocation_command': 'dbt test', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:42:14.092510 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:42:14.092510 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:42:14.092510 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:42:16.131567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dfac5b36-3977-4524-97cd-1cac122c1075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A59E181C0>]}
[0m20:42:16.286476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dfac5b36-3977-4524-97cd-1cac122c1075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A59E1A9E0>]}
[0m20:42:16.286476 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:42:17.385400 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:42:17.385400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'dfac5b36-3977-4524-97cd-1cac122c1075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A599299F0>]}
[0m20:42:17.447098 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:42:18.054289 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:42:18.056295 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:42:18.056295 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:42:18.174965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dfac5b36-3977-4524-97cd-1cac122c1075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A6D923EB0>]}
[0m20:42:18.593301 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:42:18.600051 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:42:18.684783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dfac5b36-3977-4524-97cd-1cac122c1075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A6D9205E0>]}
[0m20:42:18.685800 [info ] [MainThread]: Found 11 models, 32 data tests, 845 macros
[0m20:42:18.687977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dfac5b36-3977-4524-97cd-1cac122c1075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A6D921210>]}
[0m20:42:18.694036 [info ] [MainThread]: 
[0m20:42:18.696708 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:42:18.698723 [info ] [MainThread]: 
[0m20:42:18.701574 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:42:18.701574 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:42:18.722651 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:42:18.722651 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:42:18.750674 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:42:18.750674 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:42:18.750674 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:42:22.022366 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da5-67fa-19e1-a7f8-fd5c541a2288) - Created
[0m20:42:22.751735 [debug] [ThreadPool]: SQL status: OK in 4.000 seconds
[0m20:42:22.769278 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10da5-67fa-19e1-a7f8-fd5c541a2288, command-id=01f10da5-6825-1eac-9a70-a425c287bc1f) - Closing
[0m20:42:22.771286 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:42:22.773294 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da5-67fa-19e1-a7f8-fd5c541a2288) - Closing
[0m20:42:23.070821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dfac5b36-3977-4524-97cd-1cac122c1075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A6D920970>]}
[0m20:42:23.079578 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:42:23.079578 [info ] [Thread-2 (]: 1 of 32 START test dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [RUN]
[0m20:42:23.079578 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77) - Creating connection
[0m20:42:23.085934 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77'
[0m20:42:23.085934 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:42:23.130176 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:42:23.132764 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:42:23.180483 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:42:23.214091 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m20:42:23.218109 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  





with validation_errors as (

    select
        subscription_id, usage_date, feature_name
    from `workspace`.`analytics`.`stg_feature_usage`
    group by subscription_id, usage_date, feature_name
    having count(*) > 1

)

select *
from validation_errors



  
  
      
    ) dbt_internal_test
[0m20:42:23.218109 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:42:26.356451 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-6a92-161e-a2d7-e723038c450f) - Created
[0m20:42:27.721856 [debug] [Thread-2 (]: SQL status: OK in 4.500 seconds
[0m20:42:27.729583 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-6a92-161e-a2d7-e723038c450f, command-id=01f10da5-6ab8-1d96-8c53-7965eeabf5ee) - Closing
[0m20:42:27.736541 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: Close
[0m20:42:27.736541 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-6a92-161e-a2d7-e723038c450f) - Closing
[0m20:42:28.031066 [info ] [Thread-2 (]: 1 of 32 PASS dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [[32mPASS[0m in 4.95s]
[0m20:42:28.033073 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m20:42:28.035080 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73
[0m20:42:28.037085 [info ] [Thread-2 (]: 2 of 32 START test not_null_int_account_360_account_id ......................... [RUN]
[0m20:42:28.037085 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73) - Creating connection
[0m20:42:28.041354 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73'
[0m20:42:28.043378 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73
[0m20:42:28.066848 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73"
[0m20:42:28.066848 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73
[0m20:42:28.080886 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73"
[0m20:42:28.083754 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73"
[0m20:42:28.083754 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`int_account_360`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:42:28.083754 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:42:31.492735 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-6d8e-13b5-b2fa-84e9f6e38627) - Created
[0m20:42:32.311611 [debug] [Thread-2 (]: SQL status: OK in 4.230 seconds
[0m20:42:32.327468 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-6d8e-13b5-b2fa-84e9f6e38627, command-id=01f10da5-6dc8-1732-93ae-475ffd38462e) - Closing
[0m20:42:32.327468 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73: Close
[0m20:42:32.327468 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-6d8e-13b5-b2fa-84e9f6e38627) - Closing
[0m20:42:32.606925 [info ] [Thread-2 (]: 2 of 32 PASS not_null_int_account_360_account_id ............................... [[32mPASS[0m in 4.57s]
[0m20:42:32.606925 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73
[0m20:42:32.606925 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a
[0m20:42:32.622616 [info ] [Thread-2 (]: 3 of 32 START test not_null_int_account_360_health_score ....................... [RUN]
[0m20:42:32.625247 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a) - Creating connection
[0m20:42:32.627055 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a'
[0m20:42:32.627962 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a
[0m20:42:32.631186 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a"
[0m20:42:32.646931 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a
[0m20:42:32.654108 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a"
[0m20:42:32.654108 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a"
[0m20:42:32.664621 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select health_score
from `workspace`.`analytics`.`int_account_360`
where health_score is null



  
  
      
    ) dbt_internal_test
[0m20:42:32.664621 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:42:35.689953 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-7024-167a-9fb0-064c218726a2) - Created
[0m20:42:36.646895 [debug] [Thread-2 (]: SQL status: OK in 3.980 seconds
[0m20:42:36.650618 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-7024-167a-9fb0-064c218726a2, command-id=01f10da5-704a-117f-ba90-dbcd8eb1a91f) - Closing
[0m20:42:36.652628 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a: Close
[0m20:42:36.654374 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-7024-167a-9fb0-064c218726a2) - Closing
[0m20:42:36.918803 [info ] [Thread-2 (]: 3 of 32 PASS not_null_int_account_360_health_score ............................. [[32mPASS[0m in 4.30s]
[0m20:42:36.934512 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a
[0m20:42:36.936527 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb
[0m20:42:36.936527 [info ] [Thread-2 (]: 4 of 32 START test not_null_int_account_churn_account_id ....................... [RUN]
[0m20:42:36.941379 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb) - Creating connection
[0m20:42:36.943997 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb'
[0m20:42:36.945681 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb
[0m20:42:36.964041 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb"
[0m20:42:36.967478 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb
[0m20:42:36.970379 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb"
[0m20:42:36.970379 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb"
[0m20:42:36.981835 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`int_account_churn`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:42:36.981835 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:42:40.119970 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-72c4-1f73-af71-8559a1003097) - Created
[0m20:42:41.296033 [debug] [Thread-2 (]: SQL status: OK in 4.310 seconds
[0m20:42:41.296033 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-72c4-1f73-af71-8559a1003097, command-id=01f10da5-72f0-1ab7-be82-b59d525524c1) - Closing
[0m20:42:41.296033 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb: Close
[0m20:42:41.296033 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-72c4-1f73-af71-8559a1003097) - Closing
[0m20:42:41.574268 [info ] [Thread-2 (]: 4 of 32 PASS not_null_int_account_churn_account_id ............................. [[32mPASS[0m in 4.64s]
[0m20:42:41.574268 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb
[0m20:42:41.574268 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c
[0m20:42:41.574268 [info ] [Thread-2 (]: 5 of 32 START test not_null_int_account_churn_churn_event_id ................... [RUN]
[0m20:42:41.588596 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c) - Creating connection
[0m20:42:41.591049 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c'
[0m20:42:41.592433 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c
[0m20:42:41.596902 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c"
[0m20:42:41.611409 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c
[0m20:42:41.624743 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c"
[0m20:42:41.626751 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c"
[0m20:42:41.632812 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select churn_event_id
from `workspace`.`analytics`.`int_account_churn`
where churn_event_id is null



  
  
      
    ) dbt_internal_test
[0m20:42:41.634820 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:42:44.676809 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-757f-15de-be32-6dfe40864b17) - Created
[0m20:42:45.892415 [debug] [Thread-2 (]: SQL status: OK in 4.260 seconds
[0m20:42:45.896428 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-757f-15de-be32-6dfe40864b17, command-id=01f10da5-75a5-10da-a990-72fcd05040d6) - Closing
[0m20:42:45.898436 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c: Close
[0m20:42:45.899946 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-757f-15de-be32-6dfe40864b17) - Closing
[0m20:42:46.172722 [info ] [Thread-2 (]: 5 of 32 PASS not_null_int_account_churn_churn_event_id ......................... [[32mPASS[0m in 4.60s]
[0m20:42:46.175028 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c
[0m20:42:46.175028 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71
[0m20:42:46.175028 [info ] [Thread-2 (]: 6 of 32 START test not_null_int_account_feature_usage_account_id ............... [RUN]
[0m20:42:46.182064 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71) - Creating connection
[0m20:42:46.183201 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71'
[0m20:42:46.185608 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71
[0m20:42:46.198301 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71"
[0m20:42:46.202518 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71
[0m20:42:46.202518 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71"
[0m20:42:46.214102 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71"
[0m20:42:46.214102 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`int_account_feature_usage`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:42:46.214102 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:42:49.275411 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-783a-136f-aaea-8e62dcef4689) - Created
[0m20:42:50.375710 [debug] [Thread-2 (]: SQL status: OK in 4.160 seconds
[0m20:42:50.379086 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-783a-136f-aaea-8e62dcef4689, command-id=01f10da5-7864-1943-86dc-c37846e1563d) - Closing
[0m20:42:50.379086 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71: Close
[0m20:42:50.379086 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-783a-136f-aaea-8e62dcef4689) - Closing
[0m20:42:50.667252 [info ] [Thread-2 (]: 6 of 32 PASS not_null_int_account_feature_usage_account_id ..................... [[32mPASS[0m in 4.49s]
[0m20:42:50.667252 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71
[0m20:42:50.667252 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5
[0m20:42:50.667252 [info ] [Thread-2 (]: 7 of 32 START test not_null_int_account_subscriptions_account_id ............... [RUN]
[0m20:42:50.684028 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5) - Creating connection
[0m20:42:50.686196 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5'
[0m20:42:50.687281 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5
[0m20:42:50.699596 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5"
[0m20:42:50.701604 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5
[0m20:42:50.711065 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5"
[0m20:42:50.713110 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5"
[0m20:42:50.713110 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`int_account_subscriptions`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:42:50.716377 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:42:53.944116 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-7b06-1169-8252-fdba691c7246) - Created
[0m20:42:55.297790 [debug] [Thread-2 (]: SQL status: OK in 4.580 seconds
[0m20:42:55.300614 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-7b06-1169-8252-fdba691c7246, command-id=01f10da5-7b2a-11cb-be57-ca6e07c775cc) - Closing
[0m20:42:55.306714 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5: Close
[0m20:42:55.306714 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-7b06-1169-8252-fdba691c7246) - Closing
[0m20:42:55.579765 [info ] [Thread-2 (]: 7 of 32 PASS not_null_int_account_subscriptions_account_id ..................... [[32mPASS[0m in 4.89s]
[0m20:42:55.579765 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5
[0m20:42:55.579765 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71
[0m20:42:55.579765 [info ] [Thread-2 (]: 8 of 32 START test not_null_int_account_subscriptions_subscription_id .......... [RUN]
[0m20:42:55.579765 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71) - Creating connection
[0m20:42:55.587899 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71'
[0m20:42:55.589897 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71
[0m20:42:55.594697 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71"
[0m20:42:55.594697 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71
[0m20:42:55.612779 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71"
[0m20:42:55.613910 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71"
[0m20:42:55.613910 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`int_account_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:42:55.613910 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:42:58.728957 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-7dde-1de9-b107-2145fee331b9) - Created
[0m20:42:59.635710 [debug] [Thread-2 (]: SQL status: OK in 4.020 seconds
[0m20:42:59.635710 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-7dde-1de9-b107-2145fee331b9, command-id=01f10da5-7e07-1ad8-a82a-a7061fd19f0a) - Closing
[0m20:42:59.635710 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71: Close
[0m20:42:59.635710 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-7dde-1de9-b107-2145fee331b9) - Closing
[0m20:42:59.921116 [info ] [Thread-2 (]: 8 of 32 PASS not_null_int_account_subscriptions_subscription_id ................ [[32mPASS[0m in 4.34s]
[0m20:42:59.937070 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71
[0m20:42:59.937070 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59
[0m20:42:59.937070 [info ] [Thread-2 (]: 9 of 32 START test not_null_int_account_support_account_id ..................... [RUN]
[0m20:42:59.943477 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59) - Creating connection
[0m20:42:59.944976 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59'
[0m20:42:59.947508 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59
[0m20:42:59.948599 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59"
[0m20:42:59.963064 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59
[0m20:42:59.967549 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59"
[0m20:42:59.967549 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59"
[0m20:42:59.967549 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`int_account_support`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:42:59.976437 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:03.127311 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-807e-1a55-8f7b-64e48819e093) - Created
[0m20:43:03.995354 [debug] [Thread-2 (]: SQL status: OK in 4.020 seconds
[0m20:43:03.995354 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-807e-1a55-8f7b-64e48819e093, command-id=01f10da5-80a5-16b6-8528-b74fff4a6bf2) - Closing
[0m20:43:03.995354 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59: Close
[0m20:43:03.995354 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-807e-1a55-8f7b-64e48819e093) - Closing
[0m20:43:04.275845 [info ] [Thread-2 (]: 9 of 32 PASS not_null_int_account_support_account_id ........................... [[32mPASS[0m in 4.34s]
[0m20:43:04.275845 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59
[0m20:43:04.275845 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:43:04.275845 [info ] [Thread-2 (]: 10 of 32 START test not_null_stg_accounts_account_id ........................... [RUN]
[0m20:43:04.292216 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m20:43:04.294468 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108'
[0m20:43:04.296132 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:43:04.298447 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:43:04.310233 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:43:04.317008 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:43:04.343579 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m20:43:04.359177 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:43:04.360494 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:07.555006 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-831f-1793-a91e-93b7790af6a0) - Created
[0m20:43:08.380956 [debug] [Thread-2 (]: SQL status: OK in 4.020 seconds
[0m20:43:08.380956 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-831f-1793-a91e-93b7790af6a0, command-id=01f10da5-8348-1bf6-8fe7-a19590bc6782) - Closing
[0m20:43:08.380956 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: Close
[0m20:43:08.380956 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-831f-1793-a91e-93b7790af6a0) - Closing
[0m20:43:08.670951 [info ] [Thread-2 (]: 10 of 32 PASS not_null_stg_accounts_account_id ................................. [[32mPASS[0m in 4.36s]
[0m20:43:08.670951 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m20:43:08.670951 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:43:08.670951 [info ] [Thread-2 (]: 11 of 32 START test not_null_stg_accounts_seats ................................ [RUN]
[0m20:43:08.680290 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39) - Creating connection
[0m20:43:08.681735 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39'
[0m20:43:08.683059 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:43:08.695416 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:43:08.696901 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:43:08.700113 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:43:08.700113 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m20:43:08.708558 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select seats
from `workspace`.`analytics`.`stg_accounts`
where seats is null



  
  
      
    ) dbt_internal_test
[0m20:43:08.710505 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:11.767201 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-85a4-1d12-a0c7-a6e13bb98c4e) - Created
[0m20:43:12.476702 [debug] [Thread-2 (]: SQL status: OK in 3.770 seconds
[0m20:43:12.480719 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-85a4-1d12-a0c7-a6e13bb98c4e, command-id=01f10da5-85c9-1ec0-a324-aa60847d6bf9) - Closing
[0m20:43:12.480719 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: Close
[0m20:43:12.480719 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-85a4-1d12-a0c7-a6e13bb98c4e) - Closing
[0m20:43:12.737235 [info ] [Thread-2 (]: 11 of 32 PASS not_null_stg_accounts_seats ...................................... [[32mPASS[0m in 4.07s]
[0m20:43:12.739245 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m20:43:12.742481 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:43:12.743603 [info ] [Thread-2 (]: 12 of 32 START test not_null_stg_accounts_signup_date .......................... [RUN]
[0m20:43:12.746259 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a) - Creating connection
[0m20:43:12.748766 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a'
[0m20:43:12.750002 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:43:12.765495 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:43:12.810378 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:43:12.819648 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:43:12.821760 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m20:43:12.823032 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signup_date
from `workspace`.`analytics`.`stg_accounts`
where signup_date is null



  
  
      
    ) dbt_internal_test
[0m20:43:12.824858 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:15.897825 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-881a-1949-92a3-4dee502c41e0) - Created
[0m20:43:16.600712 [debug] [Thread-2 (]: SQL status: OK in 3.780 seconds
[0m20:43:16.600712 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-881a-1949-92a3-4dee502c41e0, command-id=01f10da5-8843-12b1-985b-19374d98fc7b) - Closing
[0m20:43:16.600712 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: Close
[0m20:43:16.600712 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-881a-1949-92a3-4dee502c41e0) - Closing
[0m20:43:16.886031 [info ] [Thread-2 (]: 12 of 32 PASS not_null_stg_accounts_signup_date ................................ [[32mPASS[0m in 4.14s]
[0m20:43:16.886031 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m20:43:16.901727 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:43:16.901727 [info ] [Thread-2 (]: 13 of 32 START test not_null_stg_churn_events_account_id ....................... [RUN]
[0m20:43:16.905481 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d) - Creating connection
[0m20:43:16.906987 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d'
[0m20:43:16.908050 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:43:16.914584 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:43:16.960845 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:43:16.969943 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:43:16.971351 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m20:43:16.973680 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_churn_events`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:43:16.975045 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:20.948840 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-8b20-1219-8e7e-588f8eac64a8) - Created
[0m20:43:21.658661 [debug] [Thread-2 (]: SQL status: OK in 4.680 seconds
[0m20:43:21.662391 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-8b20-1219-8e7e-588f8eac64a8, command-id=01f10da5-8b44-19aa-86a1-27f40dff625e) - Closing
[0m20:43:21.662391 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: Close
[0m20:43:21.662391 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-8b20-1219-8e7e-588f8eac64a8) - Closing
[0m20:43:21.931317 [info ] [Thread-2 (]: 13 of 32 PASS not_null_stg_churn_events_account_id ............................. [[32mPASS[0m in 5.03s]
[0m20:43:21.931317 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m20:43:21.937347 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:43:21.937347 [info ] [Thread-2 (]: 14 of 32 START test not_null_stg_churn_events_churn_event_id ................... [RUN]
[0m20:43:21.941570 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc) - Creating connection
[0m20:43:21.942912 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc'
[0m20:43:21.945038 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:43:21.946038 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:43:21.959076 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:43:21.964578 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:43:21.964578 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m20:43:21.964578 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select churn_event_id
from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is null



  
  
      
    ) dbt_internal_test
[0m20:43:21.973410 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:25.096877 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-8d95-1ef4-969d-620c62685f2e) - Created
[0m20:43:25.823050 [debug] [Thread-2 (]: SQL status: OK in 3.860 seconds
[0m20:43:25.826996 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-8d95-1ef4-969d-620c62685f2e, command-id=01f10da5-8dbb-192b-b097-38f07a466848) - Closing
[0m20:43:25.829975 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: Close
[0m20:43:25.829975 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-8d95-1ef4-969d-620c62685f2e) - Closing
[0m20:43:26.108576 [info ] [Thread-2 (]: 14 of 32 PASS not_null_stg_churn_events_churn_event_id ......................... [[32mPASS[0m in 4.17s]
[0m20:43:26.110585 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m20:43:26.112594 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:43:26.112594 [info ] [Thread-2 (]: 15 of 32 START test not_null_stg_feature_usage_subscription_id ................. [RUN]
[0m20:43:26.116726 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3) - Creating connection
[0m20:43:26.118861 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3'
[0m20:43:26.120601 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:43:26.130624 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:43:26.130624 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:43:26.141613 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:43:26.151998 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m20:43:26.154007 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_feature_usage`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:43:26.156340 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:29.363149 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-9023-111d-a8b3-96a96826d28e) - Created
[0m20:43:30.168544 [debug] [Thread-2 (]: SQL status: OK in 4.010 seconds
[0m20:43:30.172544 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-9023-111d-a8b3-96a96826d28e, command-id=01f10da5-9049-13f6-8918-5f28445d44c0) - Closing
[0m20:43:30.174552 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: Close
[0m20:43:30.176562 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-9023-111d-a8b3-96a96826d28e) - Closing
[0m20:43:30.429533 [info ] [Thread-2 (]: 15 of 32 PASS not_null_stg_feature_usage_subscription_id ....................... [[32mPASS[0m in 4.31s]
[0m20:43:30.437896 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m20:43:30.437896 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:43:30.437896 [info ] [Thread-2 (]: 16 of 32 START test not_null_stg_subscriptions_account_id ...................... [RUN]
[0m20:43:30.444591 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9) - Creating connection
[0m20:43:30.447338 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9'
[0m20:43:30.449521 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:43:30.651305 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:43:30.651305 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:43:30.666654 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:43:30.668661 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m20:43:30.670518 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_subscriptions`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:43:30.672527 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:34.804628 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-9361-1102-87c1-02cfbc8de962) - Created
[0m20:43:35.518624 [debug] [Thread-2 (]: SQL status: OK in 4.850 seconds
[0m20:43:35.522836 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-9361-1102-87c1-02cfbc8de962, command-id=01f10da5-9385-1b41-b6bb-542c1edc29c8) - Closing
[0m20:43:35.524843 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: Close
[0m20:43:35.524843 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-9361-1102-87c1-02cfbc8de962) - Closing
[0m20:43:35.792813 [info ] [Thread-2 (]: 16 of 32 PASS not_null_stg_subscriptions_account_id ............................ [[32mPASS[0m in 5.35s]
[0m20:43:35.792813 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m20:43:35.792813 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:43:35.792813 [info ] [Thread-2 (]: 17 of 32 START test not_null_stg_subscriptions_subscription_id ................. [RUN]
[0m20:43:35.804503 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91) - Creating connection
[0m20:43:35.805749 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m20:43:35.807776 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:43:35.809782 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:43:35.825702 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:43:35.825702 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:43:35.856886 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m20:43:35.856886 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m20:43:35.871473 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:38.879561 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-95cf-152d-bfc5-42dbec022703) - Created
[0m20:43:39.580153 [debug] [Thread-2 (]: SQL status: OK in 3.710 seconds
[0m20:43:39.586251 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-95cf-152d-bfc5-42dbec022703, command-id=01f10da5-95f5-132e-b1b7-3c359ef21e23) - Closing
[0m20:43:39.586251 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m20:43:39.589212 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-95cf-152d-bfc5-42dbec022703) - Closing
[0m20:43:39.857477 [info ] [Thread-2 (]: 17 of 32 PASS not_null_stg_subscriptions_subscription_id ....................... [[32mPASS[0m in 4.06s]
[0m20:43:39.857477 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m20:43:39.857477 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:43:39.857477 [info ] [Thread-2 (]: 18 of 32 START test not_null_stg_support_tickets_account_id .................... [RUN]
[0m20:43:39.867887 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m20:43:39.869328 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m20:43:39.870959 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:43:39.890804 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:43:39.893915 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:43:39.896124 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:43:39.939846 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m20:43:39.940842 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m20:43:39.943401 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:43.107567 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-984e-11d8-abab-ed06ff41c133) - Created
[0m20:43:43.886135 [debug] [Thread-2 (]: SQL status: OK in 3.940 seconds
[0m20:43:43.886135 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-984e-11d8-abab-ed06ff41c133, command-id=01f10da5-9878-1eaa-9b19-edfbed6e4c8d) - Closing
[0m20:43:43.886135 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m20:43:43.898447 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-984e-11d8-abab-ed06ff41c133) - Closing
[0m20:43:44.179192 [info ] [Thread-2 (]: 18 of 32 PASS not_null_stg_support_tickets_account_id .......................... [[32mPASS[0m in 4.31s]
[0m20:43:44.179192 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m20:43:44.179192 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:43:44.185664 [info ] [Thread-2 (]: 19 of 32 START test not_null_stg_support_tickets_ticket_id ..................... [RUN]
[0m20:43:44.188631 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m20:43:44.190466 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m20:43:44.192082 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:43:44.208223 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:43:44.210334 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:43:44.211425 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:43:44.221776 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m20:43:44.222780 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m20:43:44.222780 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:47.210098 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-9ac3-1984-be9a-740f2b337057) - Created
[0m20:43:47.846586 [debug] [Thread-2 (]: SQL status: OK in 3.620 seconds
[0m20:43:47.846586 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-9ac3-1984-be9a-740f2b337057, command-id=01f10da5-9aea-1e6c-bb3b-03463b671177) - Closing
[0m20:43:47.846586 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m20:43:47.846586 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-9ac3-1984-be9a-740f2b337057) - Closing
[0m20:43:48.121596 [info ] [Thread-2 (]: 19 of 32 PASS not_null_stg_support_tickets_ticket_id ........................... [[32mPASS[0m in 3.94s]
[0m20:43:48.121596 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m20:43:48.121596 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:43:48.121596 [info ] [Thread-2 (]: 20 of 32 START test relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:43:48.134746 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b) - Creating connection
[0m20:43:48.136175 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b'
[0m20:43:48.137973 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:43:48.164525 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:43:48.206029 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:43:48.216062 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:43:48.218221 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m20:43:48.220474 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_churn_events`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:43:48.220474 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:51.266290 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-9d2f-17b1-afd5-b6710730f011) - Created
[0m20:43:52.565400 [debug] [Thread-2 (]: SQL status: OK in 4.340 seconds
[0m20:43:52.570430 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-9d2f-17b1-afd5-b6710730f011, command-id=01f10da5-9d56-1834-9a0f-de2c081ce551) - Closing
[0m20:43:52.572590 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: Close
[0m20:43:52.574612 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-9d2f-17b1-afd5-b6710730f011) - Closing
[0m20:43:52.847139 [info ] [Thread-2 (]: 20 of 32 PASS relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 4.71s]
[0m20:43:52.859219 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m20:43:52.859219 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:43:52.859219 [info ] [Thread-2 (]: 21 of 32 START test relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [RUN]
[0m20:43:52.865357 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850) - Creating connection
[0m20:43:52.867763 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850'
[0m20:43:52.868828 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:43:52.888566 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:43:52.891197 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:43:52.892206 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:43:52.892206 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m20:43:52.904580 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select subscription_id as from_field
    from `workspace`.`analytics`.`stg_feature_usage`
    where subscription_id is not null
),

parent as (
    select subscription_id as to_field
    from `workspace`.`analytics`.`stg_subscriptions`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:43:52.905603 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:43:55.922893 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-9ff6-1245-ab51-774db6172c81) - Created
[0m20:43:57.195981 [debug] [Thread-2 (]: SQL status: OK in 4.290 seconds
[0m20:43:57.211866 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-9ff6-1245-ab51-774db6172c81, command-id=01f10da5-a028-1edf-940a-48b0821c1d53) - Closing
[0m20:43:57.211866 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: Close
[0m20:43:57.211866 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-9ff6-1245-ab51-774db6172c81) - Closing
[0m20:43:57.495743 [info ] [Thread-2 (]: 21 of 32 PASS relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [[32mPASS[0m in 4.64s]
[0m20:43:57.500364 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m20:43:57.502373 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:43:57.504383 [info ] [Thread-2 (]: 22 of 32 START test relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:43:57.507226 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90) - Creating connection
[0m20:43:57.509394 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90'
[0m20:43:57.511180 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:43:57.544180 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:43:57.546188 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:43:57.557119 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:43:57.563158 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m20:43:57.566004 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_subscriptions`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:43:57.566004 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:44:00.726818 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-a2d3-116a-9d2c-cea4c62ead02) - Created
[0m20:44:01.818427 [debug] [Thread-2 (]: SQL status: OK in 4.250 seconds
[0m20:44:01.836177 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-a2d3-116a-9d2c-cea4c62ead02, command-id=01f10da5-a2f9-1928-ae5c-187e11251cf2) - Closing
[0m20:44:01.836177 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: Close
[0m20:44:01.836177 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-a2d3-116a-9d2c-cea4c62ead02) - Closing
[0m20:44:02.114107 [info ] [Thread-2 (]: 22 of 32 PASS relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 4.61s]
[0m20:44:02.114107 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m20:44:02.114107 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:44:02.114107 [info ] [Thread-2 (]: 23 of 32 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m20:44:02.129234 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m20:44:02.131365 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m20:44:02.132439 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:44:02.151138 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:44:02.155123 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:44:02.164327 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:44:02.166334 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m20:44:02.167863 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m20:44:02.168885 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:44:05.479233 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-a5a0-13da-98ba-8d753cd0b5ab) - Created
[0m20:44:06.779597 [debug] [Thread-2 (]: SQL status: OK in 4.610 seconds
[0m20:44:06.779597 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-a5a0-13da-98ba-8d753cd0b5ab, command-id=01f10da5-a5cf-1388-afe7-e0e31e3b566c) - Closing
[0m20:44:06.779597 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m20:44:06.779597 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-a5a0-13da-98ba-8d753cd0b5ab) - Closing
[0m20:44:07.079149 [info ] [Thread-2 (]: 23 of 32 PASS relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 4.97s]
[0m20:44:07.079149 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m20:44:07.079149 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376
[0m20:44:07.085806 [info ] [Thread-2 (]: 24 of 32 START test unique_int_account_360_account_id .......................... [RUN]
[0m20:44:07.089410 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376) - Creating connection
[0m20:44:07.091316 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376'
[0m20:44:07.092925 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376
[0m20:44:07.119124 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376"
[0m20:44:07.119124 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376
[0m20:44:07.119124 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376"
[0m20:44:07.119124 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376"
[0m20:44:07.133806 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`int_account_360`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:44:07.134820 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:44:10.173808 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-a876-1e43-a28b-071ef576a2e0) - Created
[0m20:44:10.948651 [debug] [Thread-2 (]: SQL status: OK in 3.810 seconds
[0m20:44:10.952528 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-a876-1e43-a28b-071ef576a2e0, command-id=01f10da5-a89a-1c0d-a836-7a30721b35b2) - Closing
[0m20:44:10.952528 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376: Close
[0m20:44:10.952528 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-a876-1e43-a28b-071ef576a2e0) - Closing
[0m20:44:11.225337 [info ] [Thread-2 (]: 24 of 32 PASS unique_int_account_360_account_id ................................ [[32mPASS[0m in 4.13s]
[0m20:44:11.225337 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376
[0m20:44:11.228034 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a
[0m20:44:11.230040 [info ] [Thread-2 (]: 25 of 32 START test unique_int_account_churn_churn_event_id .................... [RUN]
[0m20:44:11.232365 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a) - Creating connection
[0m20:44:11.234459 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a'
[0m20:44:11.235467 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a
[0m20:44:11.250297 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a"
[0m20:44:11.266998 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a
[0m20:44:11.281062 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a"
[0m20:44:11.285625 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a"
[0m20:44:11.286066 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    churn_event_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`int_account_churn`
where churn_event_id is not null
group by churn_event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:44:11.290354 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:44:15.515942 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-ab9e-1f8b-b053-622b393a50a7) - Created
[0m20:44:17.017642 [debug] [Thread-2 (]: SQL status: OK in 5.730 seconds
[0m20:44:17.028977 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-ab9e-1f8b-b053-622b393a50a7, command-id=01f10da5-abc9-1c46-b0b8-b2a34a240fe0) - Closing
[0m20:44:17.028977 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a: Close
[0m20:44:17.031584 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-ab9e-1f8b-b053-622b393a50a7) - Closing
[0m20:44:17.327579 [info ] [Thread-2 (]: 25 of 32 PASS unique_int_account_churn_churn_event_id .......................... [[32mPASS[0m in 6.10s]
[0m20:44:17.329583 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a
[0m20:44:17.331590 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe
[0m20:44:17.333599 [info ] [Thread-2 (]: 26 of 32 START test unique_int_account_feature_usage_account_id ................ [RUN]
[0m20:44:17.336302 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe) - Creating connection
[0m20:44:17.338850 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe'
[0m20:44:17.340344 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe
[0m20:44:17.367954 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe"
[0m20:44:17.374085 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe
[0m20:44:17.385487 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe"
[0m20:44:17.389500 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe"
[0m20:44:17.389500 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`int_account_feature_usage`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:44:17.391506 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:44:20.508249 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-ae9f-1fb7-825e-a0079108dea0) - Created
[0m20:44:21.315956 [debug] [Thread-2 (]: SQL status: OK in 3.920 seconds
[0m20:44:21.321912 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-ae9f-1fb7-825e-a0079108dea0, command-id=01f10da5-aec3-15bf-86e2-1ab79d33ac40) - Closing
[0m20:44:21.323918 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe: Close
[0m20:44:21.323918 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-ae9f-1fb7-825e-a0079108dea0) - Closing
[0m20:44:21.584445 [info ] [Thread-2 (]: 26 of 32 PASS unique_int_account_feature_usage_account_id ...................... [[32mPASS[0m in 4.25s]
[0m20:44:21.586453 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe
[0m20:44:21.588460 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b
[0m20:44:21.590468 [info ] [Thread-2 (]: 27 of 32 START test unique_int_account_subscriptions_subscription_id ........... [RUN]
[0m20:44:21.592644 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b) - Creating connection
[0m20:44:21.594982 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b'
[0m20:44:21.597066 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b
[0m20:44:21.612313 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b"
[0m20:44:21.616128 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b
[0m20:44:21.625205 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b"
[0m20:44:21.625205 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b"
[0m20:44:21.625205 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`int_account_subscriptions`
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:44:21.625205 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:44:25.132415 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-b15b-1814-8f32-7509cc55255f) - Created
[0m20:44:26.340533 [debug] [Thread-2 (]: SQL status: OK in 4.720 seconds
[0m20:44:26.340533 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-b15b-1814-8f32-7509cc55255f, command-id=01f10da5-b185-196b-a0bf-4678a66abac2) - Closing
[0m20:44:26.356540 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b: Close
[0m20:44:26.356540 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-b15b-1814-8f32-7509cc55255f) - Closing
[0m20:44:26.661785 [info ] [Thread-2 (]: 27 of 32 PASS unique_int_account_subscriptions_subscription_id ................. [[32mPASS[0m in 5.07s]
[0m20:44:26.663791 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b
[0m20:44:26.663791 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab
[0m20:44:26.667781 [info ] [Thread-2 (]: 28 of 32 START test unique_int_account_support_account_id ...................... [RUN]
[0m20:44:26.670957 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab) - Creating connection
[0m20:44:26.673158 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab'
[0m20:44:26.674239 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab
[0m20:44:26.699861 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab"
[0m20:44:26.699861 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab
[0m20:44:26.711969 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab"
[0m20:44:26.715436 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab"
[0m20:44:26.715436 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`int_account_support`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:44:26.719196 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:44:29.894066 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-b432-1917-9488-60e85d07bd39) - Created
[0m20:44:30.651365 [debug] [Thread-2 (]: SQL status: OK in 3.930 seconds
[0m20:44:30.651365 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-b432-1917-9488-60e85d07bd39, command-id=01f10da5-b45b-10e8-9fca-310f59d3d8a7) - Closing
[0m20:44:30.651365 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab: Close
[0m20:44:30.651365 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-b432-1917-9488-60e85d07bd39) - Closing
[0m20:44:30.969158 [info ] [Thread-2 (]: 28 of 32 PASS unique_int_account_support_account_id ............................ [[32mPASS[0m in 4.30s]
[0m20:44:30.969158 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab
[0m20:44:30.973078 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:44:30.973078 [info ] [Thread-2 (]: 29 of 32 START test unique_stg_accounts_account_id ............................. [RUN]
[0m20:44:30.977341 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m20:44:30.979024 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79'
[0m20:44:30.980659 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:44:30.999662 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:44:31.001260 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:44:31.003261 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:44:31.003261 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m20:44:31.014875 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:44:31.014875 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:44:34.163779 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-b6c3-125f-8d00-4ecc363752c1) - Created
[0m20:44:35.011434 [debug] [Thread-2 (]: SQL status: OK in 4.000 seconds
[0m20:44:35.011434 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-b6c3-125f-8d00-4ecc363752c1, command-id=01f10da5-b6e8-1eae-b4d9-5a96a2f41efd) - Closing
[0m20:44:35.024569 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: Close
[0m20:44:35.024569 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-b6c3-125f-8d00-4ecc363752c1) - Closing
[0m20:44:35.302472 [info ] [Thread-2 (]: 29 of 32 PASS unique_stg_accounts_account_id ................................... [[32mPASS[0m in 4.33s]
[0m20:44:35.302472 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m20:44:35.302472 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:44:35.302472 [info ] [Thread-2 (]: 30 of 32 START test unique_stg_churn_events_churn_event_id ..................... [RUN]
[0m20:44:35.310319 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947) - Creating connection
[0m20:44:35.312563 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947'
[0m20:44:35.314744 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:44:35.333002 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:44:35.335702 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:44:35.342865 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:44:35.346546 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m20:44:35.347988 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    churn_event_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is not null
group by churn_event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:44:35.347988 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:44:38.348898 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-b93e-15bc-a744-456f3f8f0ceb) - Created
[0m20:44:39.097609 [debug] [Thread-2 (]: SQL status: OK in 3.750 seconds
[0m20:44:39.097609 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-b93e-15bc-a744-456f3f8f0ceb, command-id=01f10da5-b965-1640-a459-6b7a8974321a) - Closing
[0m20:44:39.104840 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: Close
[0m20:44:39.106588 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-b93e-15bc-a744-456f3f8f0ceb) - Closing
[0m20:44:39.368966 [info ] [Thread-2 (]: 30 of 32 PASS unique_stg_churn_events_churn_event_id ........................... [[32mPASS[0m in 4.07s]
[0m20:44:39.368966 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m20:44:39.368966 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:44:39.368966 [info ] [Thread-2 (]: 31 of 32 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m20:44:39.385276 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e) - Creating connection
[0m20:44:39.386956 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m20:44:39.388600 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:44:39.405399 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:44:39.405399 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:44:39.414265 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:44:39.414265 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m20:44:39.414265 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:44:39.414265 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:44:42.557482 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-bbc0-146a-a913-7ce47c43b19b) - Created
[0m20:44:43.343427 [debug] [Thread-2 (]: SQL status: OK in 3.930 seconds
[0m20:44:43.343427 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-bbc0-146a-a913-7ce47c43b19b, command-id=01f10da5-bbe8-1574-93d1-e91b8b53ff44) - Closing
[0m20:44:43.343427 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m20:44:43.343427 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-bbc0-146a-a913-7ce47c43b19b) - Closing
[0m20:44:43.623968 [info ] [Thread-2 (]: 31 of 32 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 4.26s]
[0m20:44:43.635588 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m20:44:43.638076 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:44:43.640085 [info ] [Thread-2 (]: 32 of 32 START test unique_stg_support_tickets_ticket_id ....................... [RUN]
[0m20:44:43.642939 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m20:44:43.645935 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m20:44:43.648084 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:44:43.666486 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:44:43.668619 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:44:43.676714 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:44:43.679745 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m20:44:43.679745 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m20:44:43.679745 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:44:46.786411 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-be45-1a41-bf74-50789f124138) - Created
[0m20:44:47.580198 [debug] [Thread-2 (]: SQL status: OK in 3.900 seconds
[0m20:44:47.584409 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da5-be45-1a41-bf74-50789f124138, command-id=01f10da5-be70-12b8-8d83-f2c11ae39465) - Closing
[0m20:44:47.584409 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m20:44:47.584409 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da5-be45-1a41-bf74-50789f124138) - Closing
[0m20:44:47.908832 [info ] [Thread-2 (]: 32 of 32 PASS unique_stg_support_tickets_ticket_id ............................. [[32mPASS[0m in 4.27s]
[0m20:44:47.911638 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m20:44:47.913386 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:44:47.913386 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:44:47.913386 [info ] [MainThread]: 
[0m20:44:47.919964 [info ] [MainThread]: Finished running 32 data tests in 0 hours 2 minutes and 29.21 seconds (149.21s).
[0m20:44:47.932480 [debug] [MainThread]: Command end result
[0m20:44:48.034287 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:44:48.034287 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:44:48.060519 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:44:48.060519 [info ] [MainThread]: 
[0m20:44:48.060519 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:44:48.060519 [info ] [MainThread]: 
[0m20:44:48.060519 [info ] [MainThread]: Done. PASS=32 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=32
[0m20:44:48.069883 [debug] [MainThread]: Command `dbt test` succeeded at 20:44:48.069883 after 156.78 seconds
[0m20:44:48.071505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A57AFEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A58E6B910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A6D8FFD00>]}
[0m20:44:48.072578 [debug] [MainThread]: Flushing usage events
[0m20:44:49.822191 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:54:14.034763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229AF0DEB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B13F8370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B13F81C0>]}


============================== 20:54:14.034763 | 0faf1c93-8d38-4572-99c4-747aacb6de36 ==============================
[0m20:54:14.034763 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:54:14.034763 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:54:16.359500 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:54:16.359500 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:54:16.359500 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:54:18.331408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B394B970>]}
[0m20:54:18.479193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B0DAE530>]}
[0m20:54:18.479193 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:54:19.460189 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:54:19.460189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B0F08B50>]}
[0m20:54:19.518564 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:54:20.209980 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 5 files added, 1 files changed.
[0m20:54:20.209980 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\gold\gold_feature_adoption.sql
[0m20:54:20.226070 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\gold\gold_support_performance.sql
[0m20:54:20.226070 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\gold\gold_churn_analysis.sql
[0m20:54:20.226070 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\gold\gold_account_health_summary.sql
[0m20:54:20.226070 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\gold\gold_customer_segments.sql
[0m20:54:20.226070 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\gold\gold_mrr_trends.sql
[0m20:54:20.895284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C54821D0>]}
[0m20:54:21.181115 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:54:21.181799 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:54:21.212713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C4BCB250>]}
[0m20:54:21.212713 [info ] [MainThread]: Found 16 models, 32 data tests, 845 macros
[0m20:54:21.212713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C525D090>]}
[0m20:54:21.230874 [info ] [MainThread]: 
[0m20:54:21.233318 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:54:21.234423 [info ] [MainThread]: 
[0m20:54:21.235432 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:54:21.235432 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:54:21.255853 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:54:21.255853 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:54:21.283213 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:54:21.283213 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:54:21.283213 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:54:25.729904 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da7-1753-1dca-8bad-f8208ef199ac) - Created
[0m20:54:27.109023 [debug] [ThreadPool]: SQL status: OK in 5.830 seconds
[0m20:54:27.125110 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10da7-1753-1dca-8bad-f8208ef199ac, command-id=01f10da7-1782-1ae8-8918-f6e5b1ff200a) - Closing
[0m20:54:27.125110 [debug] [ThreadPool]: On list_workspace: Close
[0m20:54:27.125110 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da7-1753-1dca-8bad-f8208ef199ac) - Closing
[0m20:54:28.150432 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:54:28.150432 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:54:28.158795 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:54:28.158795 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:54:28.158795 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:54:31.897985 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da7-1aee-172d-b353-74035e7cec59) - Created
[0m20:54:33.240564 [debug] [ThreadPool]: SQL status: OK in 5.080 seconds
[0m20:54:33.243264 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10da7-1aee-172d-b353-74035e7cec59, command-id=01f10da7-1b2e-1c60-b7a8-e92aa3b5ab23) - Closing
[0m20:54:33.250217 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:54:33.250217 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da7-1aee-172d-b353-74035e7cec59) - Closing
[0m20:54:33.649515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C5308AC0>]}
[0m20:54:33.656072 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_accounts
[0m20:54:33.662941 [info ] [Thread-3 (]: 1 of 16 START sql view model analytics.stg_accounts ............................ [RUN]
[0m20:54:33.662941 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_accounts) - Creating connection
[0m20:54:33.662941 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_accounts'
[0m20:54:33.662941 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_accounts
[0m20:54:33.694443 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_accounts"
[0m20:54:33.697805 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_accounts
[0m20:54:33.733269 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:54:33.737726 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:54:33.739741 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C55915D0>]}
[0m20:54:33.763753 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_accounts`
[0m20:54:33.788627 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_accounts"
[0m20:54:33.790803 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_accounts"
[0m20:54:33.791813 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_accounts: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_accounts"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_accounts`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_accounts

),

cleaned as (

    select
        account_id,
        account_name,
        industry,
        country,
        signup_date,
        referral_source,
        plan_tier,
        seats,
        is_trial,
        churn_flag,
        ingestion_ts

    from source

    where account_id is not null

)

select *
from cleaned
  )

[0m20:54:33.793886 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:54:37.295524 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-1e2e-1a97-80d8-f423a50ca920) - Created
[0m20:54:38.247889 [debug] [Thread-3 (]: SQL status: OK in 4.460 seconds
[0m20:54:38.247889 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-1e2e-1a97-80d8-f423a50ca920, command-id=01f10da7-1e68-15a9-8d6a-ddea37fffe22) - Closing
[0m20:54:38.275548 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:54:38.275548 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_accounts: Close
[0m20:54:38.275548 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-1e2e-1a97-80d8-f423a50ca920) - Closing
[0m20:54:38.663917 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C5547E50>]}
[0m20:54:38.663917 [info ] [Thread-3 (]: 1 of 16 OK created sql view model analytics.stg_accounts ....................... [[32mOK[0m in 5.00s]
[0m20:54:38.663917 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_accounts
[0m20:54:38.663917 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_churn_events
[0m20:54:38.663917 [info ] [Thread-3 (]: 2 of 16 START sql view model analytics.stg_churn_events ........................ [RUN]
[0m20:54:38.663917 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_churn_events) - Creating connection
[0m20:54:38.679535 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_churn_events'
[0m20:54:38.680757 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_churn_events
[0m20:54:38.683352 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_churn_events"
[0m20:54:38.683352 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_churn_events
[0m20:54:38.703954 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:54:38.703954 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_churn_events`
[0m20:54:38.703954 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_churn_events"
[0m20:54:38.712680 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_churn_events"
[0m20:54:38.714674 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_churn_events: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_churn_events"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_churn_events`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_churn_events

),

cleaned as (

    select
        churn_event_id,
        account_id,
        churn_date,
        reason_code,
        refund_amount_usd,
        preceding_upgrade_flag,
        preceding_downgrade_flag,
        is_reactivation,
        feedback_text,
        ingestion_ts

    from source

    where churn_event_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:54:38.714674 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:54:43.075931 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-219f-1b1a-bcef-a11810758007) - Created
[0m20:54:43.977077 [debug] [Thread-3 (]: SQL status: OK in 5.260 seconds
[0m20:54:43.977077 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-219f-1b1a-bcef-a11810758007, command-id=01f10da7-21d9-157d-8bf5-963a9f5887be) - Closing
[0m20:54:43.977077 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:54:43.977077 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_churn_events: Close
[0m20:54:43.977077 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-219f-1b1a-bcef-a11810758007) - Closing
[0m20:54:44.372296 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229AEF35C00>]}
[0m20:54:44.372296 [info ] [Thread-3 (]: 2 of 16 OK created sql view model analytics.stg_churn_events ................... [[32mOK[0m in 5.71s]
[0m20:54:44.372296 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_churn_events
[0m20:54:44.379867 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_feature_usage
[0m20:54:44.381873 [info ] [Thread-3 (]: 3 of 16 START sql view model analytics.stg_feature_usage ....................... [RUN]
[0m20:54:44.384679 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_feature_usage) - Creating connection
[0m20:54:44.386133 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_feature_usage'
[0m20:54:44.387812 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_feature_usage
[0m20:54:44.402650 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_feature_usage"
[0m20:54:44.405210 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_feature_usage
[0m20:54:44.413786 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:54:44.413786 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_feature_usage`
[0m20:54:44.413786 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_feature_usage"
[0m20:54:44.413786 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_feature_usage"
[0m20:54:44.413786 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_feature_usage: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_feature_usage"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_feature_usage`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_feature_usage
    where subscription_id is not null

),

aggregated as (

    select
        subscription_id,
        usage_date,
        feature_name,

        sum(usage_count)              as total_usage_count,
        sum(usage_duration_secs)      as total_usage_duration_secs,
        sum(error_count)              as total_error_count,

        max(is_beta_feature)          as is_beta_feature

    from source
    group by
        subscription_id,
        usage_date,
        feature_name

)

select *
from aggregated
  )

[0m20:54:44.413786 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:54:50.961352 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-2657-1bf1-b4b4-f11476168b42) - Created
[0m20:54:51.847124 [debug] [Thread-3 (]: SQL status: OK in 7.430 seconds
[0m20:54:51.849133 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-2657-1bf1-b4b4-f11476168b42, command-id=01f10da7-268f-11af-b8c0-343e63173d73) - Closing
[0m20:54:51.849133 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:54:51.853300 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_feature_usage: Close
[0m20:54:51.855308 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-2657-1bf1-b4b4-f11476168b42) - Closing
[0m20:54:53.124293 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C54C0910>]}
[0m20:54:53.126301 [info ] [Thread-3 (]: 3 of 16 OK created sql view model analytics.stg_feature_usage .................. [[32mOK[0m in 8.74s]
[0m20:54:53.126301 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_feature_usage
[0m20:54:53.129898 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_subscriptions
[0m20:54:53.131904 [info ] [Thread-3 (]: 4 of 16 START sql view model analytics.stg_subscriptions ....................... [RUN]
[0m20:54:53.134674 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_subscriptions) - Creating connection
[0m20:54:53.137447 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_subscriptions'
[0m20:54:53.139041 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_subscriptions
[0m20:54:53.153642 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_subscriptions"
[0m20:54:53.153642 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_subscriptions
[0m20:54:53.301638 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:54:53.301638 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_subscriptions`
[0m20:54:53.308546 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_subscriptions"
[0m20:54:53.308546 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_subscriptions"
[0m20:54:53.308546 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_subscriptions"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_subscriptions`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_subscriptions

),

cleaned as (

    select
        subscription_id,
        account_id,
        start_date,
        end_date,
        plan_tier,
        seats,
        mrr_amount,
        arr_amount,
        is_trial,
        upgrade_flag,
        downgrade_flag,
        churn_flag,
        billing_frequency,
        auto_renew_flag,
        ingestion_ts

    from source

    where subscription_id is not null
      and account_id is not null

)

select *
from cleaned
  )

[0m20:54:53.314456 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:55:00.194013 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-2bd8-181f-bfec-759ca4ec0131) - Created
[0m20:55:01.099565 [debug] [Thread-3 (]: SQL status: OK in 7.790 seconds
[0m20:55:01.099565 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-2bd8-181f-bfec-759ca4ec0131, command-id=01f10da7-2c0f-1d43-a7bd-b9b059353889) - Closing
[0m20:55:01.108945 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:55:01.108945 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_subscriptions: Close
[0m20:55:01.112830 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-2bd8-181f-bfec-759ca4ec0131) - Closing
[0m20:55:01.501893 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229AEF35C00>]}
[0m20:55:01.505575 [info ] [Thread-3 (]: 4 of 16 OK created sql view model analytics.stg_subscriptions .................. [[32mOK[0m in 8.37s]
[0m20:55:01.505575 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_subscriptions
[0m20:55:01.508922 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.stg_support_tickets
[0m20:55:01.508922 [info ] [Thread-3 (]: 5 of 16 START sql view model analytics.stg_support_tickets ..................... [RUN]
[0m20:55:01.513543 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.stg_support_tickets) - Creating connection
[0m20:55:01.515806 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.stg_support_tickets'
[0m20:55:01.516798 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.stg_support_tickets
[0m20:55:01.531447 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:55:01.533631 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.stg_support_tickets
[0m20:55:01.533631 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:55:01.544202 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`stg_support_tickets`
[0m20:55:01.546434 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.stg_support_tickets"
[0m20:55:01.548886 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.stg_support_tickets"
[0m20:55:01.548886 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.stg_support_tickets"} */

  
  
  create or replace view `workspace`.`analytics`.`stg_support_tickets`
  
  as (
    with source as (

    select *
    from workspace.default.bronze_support_tickets

),

cleaned as (

    select
        ticket_id,
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        ingestion_ts

    from source

    where ticket_id is not null
        and account_id is not null

)

select *
from cleaned
  )

[0m20:55:01.548886 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:55:04.927141 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-2ead-1db1-b07e-9fcf42fc2e9f) - Created
[0m20:55:06.905997 [debug] [Thread-3 (]: SQL status: OK in 5.360 seconds
[0m20:55:06.905997 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-2ead-1db1-b07e-9fcf42fc2e9f, command-id=01f10da7-2ee1-151e-8e04-0aec8334d676) - Closing
[0m20:55:06.905997 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:55:06.905997 [debug] [Thread-3 (]: On model.saas_dbt_analytics.stg_support_tickets: Close
[0m20:55:06.905997 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-2ead-1db1-b07e-9fcf42fc2e9f) - Closing
[0m20:55:07.286773 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B0AFDF30>]}
[0m20:55:07.288777 [info ] [Thread-3 (]: 5 of 16 OK created sql view model analytics.stg_support_tickets ................ [[32mOK[0m in 5.77s]
[0m20:55:07.291050 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.stg_support_tickets
[0m20:55:07.292056 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.gold_feature_adoption
[0m20:55:07.292056 [info ] [Thread-3 (]: 6 of 16 START sql table model analytics.gold_feature_adoption .................. [RUN]
[0m20:55:07.296315 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.gold_feature_adoption) - Creating connection
[0m20:55:07.297358 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.gold_feature_adoption'
[0m20:55:07.298433 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.gold_feature_adoption
[0m20:55:07.301614 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.gold_feature_adoption"
[0m20:55:07.309833 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.gold_feature_adoption
[0m20:55:07.344942 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m20:55:07.427140 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.gold_feature_adoption"
[0m20:55:07.428650 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.gold_feature_adoption"
[0m20:55:07.428650 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.gold_feature_adoption"} */

  
    
        create or replace table `workspace`.`analytics`.`gold_feature_adoption`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      /*
    Grain: one row per feature per month
    Tracks feature adoption trends, error rates, and engagement by plan tier.
    Used for product analytics and feature prioritization.
*/

with usage as (

    select
        u.subscription_id,
        u.usage_date,
        u.feature_name,
        u.total_usage_count,
        u.total_usage_duration_secs,
        u.total_error_count,
        u.is_beta_feature

    from `workspace`.`analytics`.`stg_feature_usage` u

),

subscriptions as (

    select
        subscription_id,
        account_id,
        plan_tier

    from `workspace`.`analytics`.`stg_subscriptions`

),

-- Join usage to account/plan context
enriched as (

    select
        u.feature_name,
        date_trunc('month', u.usage_date)                       as usage_month,
        s.plan_tier,
        s.account_id,
        u.total_usage_count,
        u.total_usage_duration_secs,
        u.total_error_count,
        u.is_beta_feature

    from usage u
    inner join subscriptions s using (subscription_id)

),

aggregated as (

    select
        feature_name,
        usage_month,
        plan_tier,
        is_beta_feature,

        -- Reach
        count(distinct account_id)                              as active_accounts,

        -- Volume
        sum(total_usage_count)                                  as total_usage_count,
        round(avg(total_usage_count), 2)                        as avg_usage_per_account,

        -- Duration
        round(sum(total_usage_duration_secs) / 3600.0, 2)      as total_usage_hours,
        round(avg(total_usage_duration_secs) / 60.0, 2)        as avg_session_minutes,

        -- Quality
        sum(total_error_count)                                  as total_errors,
        round(
            sum(total_error_count) * 100.0
            / nullif(sum(total_usage_count), 0), 2
        )                                                       as error_rate_pct,

        current_timestamp()                                     as updated_at

    from enriched
    group by
        feature_name,
        usage_month,
        plan_tier,
        is_beta_feature

)

select * from aggregated
order by usage_month desc, active_accounts desc
  
[0m20:55:07.428650 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:55:10.665091 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-321d-11c3-bf38-1dbcc3f75043) - Created
[0m20:55:18.162221 [debug] [Thread-3 (]: SQL status: OK in 10.730 seconds
[0m20:55:18.162221 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-321d-11c3-bf38-1dbcc3f75043, command-id=01f10da7-32fb-161b-be6b-ce1e5fcb9ac7) - Closing
[0m20:55:18.536522 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:55:18.582565 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_feature_adoption: Close
[0m20:55:18.582565 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-321d-11c3-bf38-1dbcc3f75043) - Closing
[0m20:55:18.927154 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B3D85960>]}
[0m20:55:18.929155 [info ] [Thread-3 (]: 6 of 16 OK created sql table model analytics.gold_feature_adoption ............. [[32mOK[0m in 11.64s]
[0m20:55:18.931489 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.gold_feature_adoption
[0m20:55:18.932720 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.int_account_churn
[0m20:55:18.934816 [info ] [Thread-3 (]: 7 of 16 START sql view model analytics.int_account_churn ....................... [RUN]
[0m20:55:18.937217 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.int_account_churn) - Creating connection
[0m20:55:18.938649 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.int_account_churn'
[0m20:55:18.940637 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.int_account_churn
[0m20:55:18.941908 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.int_account_churn"
[0m20:55:18.941908 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.int_account_churn
[0m20:55:18.961203 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:55:18.969260 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`int_account_churn`
[0m20:55:18.974862 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.int_account_churn"
[0m20:55:18.976870 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.int_account_churn"
[0m20:55:18.980889 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_churn: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.int_account_churn"} */

  
  
  create or replace view `workspace`.`analytics`.`int_account_churn`
  
  as (
    /*
    Grain: one row per churn event
    Enriches churn events with account and subscription context (plan at churn, MRR lost).
    Used by: gold_churn_analysis, int_account_360
*/

with churn as (

    select
        churn_event_id,
        account_id,
        churn_date,
        reason_code,
        refund_amount_usd,
        preceding_upgrade_flag,
        preceding_downgrade_flag,
        is_reactivation,
        feedback_text

    from `workspace`.`analytics`.`stg_churn_events`

),

accounts as (

    select
        account_id,
        account_name,
        industry,
        country,
        plan_tier     as account_plan_tier,
        seats,
        signup_date

    from `workspace`.`analytics`.`stg_accounts`

),

-- Get the most recent subscription per account to capture MRR lost at churn
latest_subscriptions as (

    select
        account_id,
        plan_tier     as subscription_plan_tier,
        mrr_amount    as mrr_at_churn,
        arr_amount    as arr_at_churn,
        billing_frequency,
        row_number() over (
            partition by account_id
            order by start_date desc
        ) as rn

    from `workspace`.`analytics`.`stg_subscriptions`

),

enriched as (

    select
        c.churn_event_id,
        c.account_id,
        a.account_name,
        a.industry,
        a.country,
        a.signup_date,

        -- Tenure at churn (days from signup to churn)
        datediff(c.churn_date, a.signup_date) as days_to_churn,

        c.churn_date,
        c.reason_code,
        c.refund_amount_usd,
        c.preceding_upgrade_flag,
        c.preceding_downgrade_flag,
        c.is_reactivation,
        c.feedback_text,

        -- Revenue context
        coalesce(s.subscription_plan_tier, a.account_plan_tier) as plan_tier_at_churn,
        s.mrr_at_churn,
        s.arr_at_churn,
        s.billing_frequency,

        -- Segment churn risk signal
        case
            when c.preceding_downgrade_flag = true then 'downgrade_to_churn'
            when c.preceding_upgrade_flag = true  then 'upgrade_to_churn'
            else 'direct_churn'
        end as churn_path

    from churn c
    left join accounts a using (account_id)
    left join latest_subscriptions s
        on c.account_id = s.account_id and s.rn = 1

)

select * from enriched
  )

[0m20:55:18.982899 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:55:23.179087 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-3983-1d20-a78c-1e7ac54ba0ba) - Created
[0m20:55:24.641754 [debug] [Thread-3 (]: SQL status: OK in 5.660 seconds
[0m20:55:24.647015 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-3983-1d20-a78c-1e7ac54ba0ba, command-id=01f10da7-39c0-1da8-b21d-385a5715662c) - Closing
[0m20:55:24.650045 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:55:24.653644 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_churn: Close
[0m20:55:24.655654 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-3983-1d20-a78c-1e7ac54ba0ba) - Closing
[0m20:55:25.034343 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B3D85960>]}
[0m20:55:25.041655 [info ] [Thread-3 (]: 7 of 16 OK created sql view model analytics.int_account_churn .................. [[32mOK[0m in 6.10s]
[0m20:55:25.043862 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.int_account_churn
[0m20:55:25.043862 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.int_account_feature_usage
[0m20:55:25.043862 [info ] [Thread-3 (]: 8 of 16 START sql view model analytics.int_account_feature_usage ............... [RUN]
[0m20:55:25.050401 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.int_account_feature_usage) - Creating connection
[0m20:55:25.051237 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.int_account_feature_usage'
[0m20:55:25.052959 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.int_account_feature_usage
[0m20:55:25.077018 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.int_account_feature_usage"
[0m20:55:25.079392 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.int_account_feature_usage
[0m20:55:25.086627 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:55:25.090600 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`int_account_feature_usage`
[0m20:55:25.090600 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.int_account_feature_usage"
[0m20:55:25.090600 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.int_account_feature_usage"
[0m20:55:25.090600 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_feature_usage: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.int_account_feature_usage"} */

  
  
  create or replace view `workspace`.`analytics`.`int_account_feature_usage`
  
  as (
    /*
    Grain: one row per account
    Aggregates feature usage across all subscriptions per account.
    Used by: gold_feature_adoption, int_account_360
*/

with usage as (

    select
        subscription_id,
        usage_date,
        feature_name,
        total_usage_count,
        total_usage_duration_secs,
        total_error_count,
        is_beta_feature

    from `workspace`.`analytics`.`stg_feature_usage`

),

subscriptions as (

    select
        subscription_id,
        account_id

    from `workspace`.`analytics`.`stg_subscriptions`

),

-- Join usage to accounts via subscriptions
usage_with_account as (

    select
        s.account_id,
        u.subscription_id,
        u.usage_date,
        u.feature_name,
        u.total_usage_count,
        u.total_usage_duration_secs,
        u.total_error_count,
        u.is_beta_feature

    from usage u
    inner join subscriptions s using (subscription_id)

),

aggregated as (

    select
        account_id,

        -- Overall usage
        count(distinct feature_name)                            as distinct_features_used,
        sum(total_usage_count)                                  as total_usage_count,
        round(sum(total_usage_duration_secs) / 3600.0, 2)      as total_usage_hours,
        sum(total_error_count)                                  as total_error_count,
        round(
            sum(total_error_count) * 100.0
            / nullif(sum(total_usage_count), 0), 2
        )                                                       as error_rate_pct,

        -- Beta adoption
        count(distinct case when is_beta_feature = true
            then feature_name end)                              as beta_features_used,

        -- Most used feature
        max_by(feature_name, total_usage_count)                 as top_feature,

        -- Recency
        max(usage_date)                                         as last_usage_date,
        min(usage_date)                                         as first_usage_date,
        count(distinct usage_date)                              as active_days

    from usage_with_account
    group by account_id

)

select * from aggregated
  )

[0m20:55:25.090600 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:55:28.757337 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-3cdb-1f2d-8d2d-241c11fefee2) - Created
[0m20:55:29.889068 [debug] [Thread-3 (]: SQL status: OK in 4.800 seconds
[0m20:55:29.889068 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-3cdb-1f2d-8d2d-241c11fefee2, command-id=01f10da7-3d14-127d-a710-3b66f8670348) - Closing
[0m20:55:29.889068 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:55:29.889068 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_feature_usage: Close
[0m20:55:29.889068 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-3cdb-1f2d-8d2d-241c11fefee2) - Closing
[0m20:55:30.340889 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B3D85960>]}
[0m20:55:30.340889 [info ] [Thread-3 (]: 8 of 16 OK created sql view model analytics.int_account_feature_usage .......... [[32mOK[0m in 5.30s]
[0m20:55:30.345645 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.int_account_feature_usage
[0m20:55:30.345645 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.int_account_subscriptions
[0m20:55:30.347652 [info ] [Thread-3 (]: 9 of 16 START sql view model analytics.int_account_subscriptions ............... [RUN]
[0m20:55:30.350919 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.int_account_subscriptions) - Creating connection
[0m20:55:30.353610 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.int_account_subscriptions'
[0m20:55:30.355538 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.int_account_subscriptions
[0m20:55:30.368092 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.int_account_subscriptions"
[0m20:55:30.373652 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.int_account_subscriptions
[0m20:55:30.379192 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:55:30.383243 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`int_account_subscriptions`
[0m20:55:30.389860 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.int_account_subscriptions"
[0m20:55:30.389860 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.int_account_subscriptions"
[0m20:55:30.389860 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.int_account_subscriptions"} */

  
  
  create or replace view `workspace`.`analytics`.`int_account_subscriptions`
  
  as (
    /*
    Grain: one row per subscription
    Joins accounts with subscriptions to build subscription-level revenue context.
    Used by: gold_mrr_trends, int_account_360
*/

with accounts as (

    select
        account_id,
        account_name,
        industry,
        country,
        signup_date,
        referral_source

    from `workspace`.`analytics`.`stg_accounts`

),

subscriptions as (

    select
        subscription_id,
        account_id,
        start_date,
        end_date,
        plan_tier,
        seats,
        mrr_amount,
        arr_amount,
        is_trial,
        upgrade_flag,
        downgrade_flag,
        churn_flag,
        billing_frequency,
        auto_renew_flag,

        -- Derived fields
        case
            when end_date is null then true
            else false
        end as is_active,

        datediff(
            coalesce(end_date, current_date()),
            start_date
        ) as subscription_duration_days

    from `workspace`.`analytics`.`stg_subscriptions`

),

joined as (

    select
        s.subscription_id,
        s.account_id,
        a.account_name,
        a.industry,
        a.country,
        a.signup_date,
        a.referral_source,
        s.start_date,
        s.end_date,
        s.plan_tier,
        s.seats,
        s.mrr_amount,
        s.arr_amount,
        s.is_trial,
        s.upgrade_flag,
        s.downgrade_flag,
        s.churn_flag,
        s.billing_frequency,
        s.auto_renew_flag,
        s.is_active,
        s.subscription_duration_days

    from subscriptions s
    left join accounts a using (account_id)

)

select * from joined
  )

[0m20:55:30.389860 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:55:36.080978 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-4146-1c48-a643-3c2b29dec085) - Created
[0m20:55:37.065923 [debug] [Thread-3 (]: SQL status: OK in 6.680 seconds
[0m20:55:37.072633 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-4146-1c48-a643-3c2b29dec085, command-id=01f10da7-4172-12f0-ba19-472cc1f7a34f) - Closing
[0m20:55:37.072633 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:55:37.072633 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_subscriptions: Close
[0m20:55:37.072633 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-4146-1c48-a643-3c2b29dec085) - Closing
[0m20:55:37.414755 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C4F0BD90>]}
[0m20:55:37.414755 [info ] [Thread-3 (]: 9 of 16 OK created sql view model analytics.int_account_subscriptions .......... [[32mOK[0m in 7.07s]
[0m20:55:37.430723 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.int_account_subscriptions
[0m20:55:37.430723 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.gold_support_performance
[0m20:55:37.430723 [info ] [Thread-3 (]: 10 of 16 START sql table model analytics.gold_support_performance .............. [RUN]
[0m20:55:37.437716 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.gold_support_performance) - Creating connection
[0m20:55:37.438738 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.gold_support_performance'
[0m20:55:37.438738 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.gold_support_performance
[0m20:55:37.451842 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.gold_support_performance"
[0m20:55:37.451842 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.gold_support_performance
[0m20:55:37.464401 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m20:55:37.465403 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.gold_support_performance"
[0m20:55:37.471716 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.gold_support_performance"
[0m20:55:37.474433 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_support_performance: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.gold_support_performance"} */

  
    
        create or replace table `workspace`.`analytics`.`gold_support_performance`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      /*
    Grain: one row per month per priority level
    SLA and support quality metrics for operational dashboards.
    Used by customer success and support team leadership.
*/

with tickets as (

    select
        ticket_id,
        account_id,
        submitted_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag,
        date_trunc('month', submitted_at)                       as ticket_month

    from `workspace`.`analytics`.`stg_support_tickets`

),

final as (

    select
        ticket_month,
        priority,

        -- Volume
        count(ticket_id)                                        as total_tickets,
        count(distinct account_id)                              as accounts_with_tickets,
        count(case when escalation_flag = true then 1 end)      as escalated_tickets,

        -- SLA metrics
        round(avg(resolution_time_hours), 2)                    as avg_resolution_time_hours,
        round(percentile_approx(
            resolution_time_hours, 0.5), 2)                     as median_resolution_time_hours,
        round(percentile_approx(
            resolution_time_hours, 0.95), 2)                    as p95_resolution_time_hours,

        round(avg(first_response_time_minutes), 2)              as avg_first_response_minutes,
        round(percentile_approx(
            first_response_time_minutes, 0.5), 2)               as median_first_response_minutes,

        -- Satisfaction
        round(avg(satisfaction_score), 2)                       as avg_satisfaction_score,
        count(case when satisfaction_score >= 4 then 1 end)     as satisfied_tickets,
        count(case when satisfaction_score <= 2 then 1 end)     as dissatisfied_tickets,

        -- Rates
        round(
            count(case when escalation_flag = true then 1 end) * 100.0
            / nullif(count(ticket_id), 0), 2
        )                                                       as escalation_rate_pct,

        round(
            count(case when satisfaction_score is not null then 1 end) * 100.0
            / nullif(count(ticket_id), 0), 2
        )                                                       as csat_response_rate_pct,

        current_timestamp()                                     as updated_at

    from tickets
    group by ticket_month, priority

)

select * from final
order by ticket_month desc, priority
  
[0m20:55:37.474433 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:55:43.104014 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-4563-144f-8205-40e5aee11d50) - Created
[0m20:55:48.530225 [debug] [Thread-3 (]: SQL status: OK in 11.060 seconds
[0m20:55:48.530225 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-4563-144f-8205-40e5aee11d50, command-id=01f10da7-45ee-1030-9c18-cffb4d8407ff) - Closing
[0m20:55:48.546098 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:55:48.546098 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_support_performance: Close
[0m20:55:48.546098 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-4563-144f-8205-40e5aee11d50) - Closing
[0m20:55:48.998025 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B3D85960>]}
[0m20:55:49.014075 [info ] [Thread-3 (]: 10 of 16 OK created sql table model analytics.gold_support_performance ......... [[32mOK[0m in 11.57s]
[0m20:55:49.014075 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.gold_support_performance
[0m20:55:49.014075 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.int_account_support
[0m20:55:49.014075 [info ] [Thread-3 (]: 11 of 16 START sql view model analytics.int_account_support .................... [RUN]
[0m20:55:49.022032 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.int_account_support) - Creating connection
[0m20:55:49.023892 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.int_account_support'
[0m20:55:49.025140 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.int_account_support
[0m20:55:49.034727 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.int_account_support"
[0m20:55:49.037999 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.int_account_support
[0m20:55:49.037999 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m20:55:49.037999 [debug] [Thread-3 (]: Creating view `workspace`.`analytics`.`int_account_support`
[0m20:55:49.037999 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.int_account_support"
[0m20:55:49.054162 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.int_account_support"
[0m20:55:49.056484 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_support: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.int_account_support"} */

  
  
  create or replace view `workspace`.`analytics`.`int_account_support`
  
  as (
    /*
    Grain: one row per account
    Aggregates support ticket metrics per account.
 */

with tickets as (

    select
        ticket_id,
        account_id,
        submitted_at,
        closed_at,
        resolution_time_hours,
        priority,
        first_response_time_minutes,
        satisfaction_score,
        escalation_flag

    from `workspace`.`analytics`.`stg_support_tickets`

),

aggregated as (

    select
        account_id,

        -- Volume
        count(ticket_id)                                        as total_tickets,
        count(case when priority = 'urgent' then 1 end)         as urgent_tickets,
        count(case when priority = 'high' then 1 end)           as high_priority_tickets,
        count(case when escalation_flag = true then 1 end)      as escalated_tickets,

        -- Resolution quality
        round(avg(resolution_time_hours), 2)                    as avg_resolution_time_hours,
        round(avg(first_response_time_minutes), 2)              as avg_first_response_time_minutes,
        round(avg(satisfaction_score), 2)                       as avg_satisfaction_score,
        min(satisfaction_score)                                 as min_satisfaction_score,

        -- Rates
        round(
            count(case when escalation_flag = true then 1 end) * 100.0
            / nullif(count(ticket_id), 0), 2
        )                                                       as escalation_rate_pct,

        -- Recency
        max(submitted_at)                                       as last_ticket_date,
        min(submitted_at)                                       as first_ticket_date

    from tickets
    group by account_id

)

select * from aggregated
  )

[0m20:55:49.057548 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:55:53.153073 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-4b5e-1437-893c-4a8e37140237) - Created
[0m20:55:54.414910 [debug] [Thread-3 (]: SQL status: OK in 5.360 seconds
[0m20:55:54.414910 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-4b5e-1437-893c-4a8e37140237, command-id=01f10da7-4b9f-197f-ba3f-f670e82d2645) - Closing
[0m20:55:54.425216 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:55:54.425216 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_support: Close
[0m20:55:54.425216 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-4b5e-1437-893c-4a8e37140237) - Closing
[0m20:55:54.847525 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B0E84970>]}
[0m20:55:54.847525 [info ] [Thread-3 (]: 11 of 16 OK created sql view model analytics.int_account_support ............... [[32mOK[0m in 5.83s]
[0m20:55:54.847525 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.int_account_support
[0m20:55:54.847525 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.gold_churn_analysis
[0m20:55:54.847525 [info ] [Thread-3 (]: 12 of 16 START sql table model analytics.gold_churn_analysis ................... [RUN]
[0m20:55:54.861707 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.gold_churn_analysis) - Creating connection
[0m20:55:54.863575 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.gold_churn_analysis'
[0m20:55:54.865235 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.gold_churn_analysis
[0m20:55:54.876310 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.gold_churn_analysis"
[0m20:55:54.876310 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.gold_churn_analysis
[0m20:55:54.889431 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m20:55:54.889431 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.gold_churn_analysis"
[0m20:55:54.889431 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.gold_churn_analysis"
[0m20:55:54.889431 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_churn_analysis: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.gold_churn_analysis"} */

  
    
        create or replace table `workspace`.`analytics`.`gold_churn_analysis`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      /*
    Grain: one row per churn event
    Used for churn root cause analysis, cohort analysis, and revenue leakage reporting.
*/

with churn as (

    select * from `workspace`.`analytics`.`int_account_churn`

),

final as (

    select
        -- Event identity
        churn_event_id,
        account_id,
        account_name,

        -- Segmentation
        industry,
        country,
        plan_tier_at_churn,
        billing_frequency,

        -- Timeline
        signup_date,
        churn_date,
        days_to_churn,
        date_trunc('month', churn_date)                         as churn_month,

        -- Tenure bucket
        case
            when days_to_churn < 30     then '< 1 month'
            when days_to_churn < 90     then '1–3 months'
            when days_to_churn < 180    then '3–6 months'
            when days_to_churn < 365    then '6–12 months'
            else                             '12+ months'
        end                                                     as tenure_bucket,

        -- Reason
        reason_code                                             as churn_reason,
        churn_path,
        preceding_upgrade_flag,
        preceding_downgrade_flag,
        is_reactivation,
        feedback_text,

        -- Revenue impact
        coalesce(mrr_at_churn, 0)                               as mrr_lost,
        coalesce(arr_at_churn, 0)                               as arr_lost,
        coalesce(refund_amount_usd, 0)                          as refund_amount_usd,

        -- Metadata
        current_timestamp()                                     as updated_at

    from churn

)

select * from final
  
[0m20:55:54.903123 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:55:58.586733 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-4e9e-12c2-acca-fb38cb006361) - Created
[0m20:56:03.578968 [debug] [Thread-3 (]: SQL status: OK in 8.680 seconds
[0m20:56:03.578968 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-4e9e-12c2-acca-fb38cb006361, command-id=01f10da7-4edc-1b3e-8fdd-8d4f9129e295) - Closing
[0m20:56:03.594083 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:56:03.594083 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_churn_analysis: Close
[0m20:56:03.600959 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-4e9e-12c2-acca-fb38cb006361) - Closing
[0m20:56:04.902554 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B0E86AA0>]}
[0m20:56:04.902554 [info ] [Thread-3 (]: 12 of 16 OK created sql table model analytics.gold_churn_analysis .............. [[32mOK[0m in 10.06s]
[0m20:56:04.902554 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.gold_churn_analysis
[0m20:56:04.902554 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.gold_mrr_trends
[0m20:56:04.913581 [info ] [Thread-3 (]: 13 of 16 START sql table model analytics.gold_mrr_trends ....................... [RUN]
[0m20:56:04.916143 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.gold_mrr_trends) - Creating connection
[0m20:56:04.918233 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.gold_mrr_trends'
[0m20:56:04.919677 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.gold_mrr_trends
[0m20:56:04.929519 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.gold_mrr_trends"
[0m20:56:04.936136 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.gold_mrr_trends
[0m20:56:04.940517 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m20:56:04.952817 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.gold_mrr_trends"
[0m20:56:04.955067 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.gold_mrr_trends"
[0m20:56:04.955067 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_mrr_trends: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.gold_mrr_trends"} */

  
    
        create or replace table `workspace`.`analytics`.`gold_mrr_trends`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      /*
    Grain: one row per month
    Core SaaS revenue metrics: new MRR, churned MRR, net MRR, active accounts.
    Used for executive revenue dashboards and board reporting.
*/

with subscriptions as (

    select
        account_id,
        subscription_id,
        start_date,
        end_date,
        plan_tier,
        mrr_amount,
        arr_amount,
        churn_flag,
        upgrade_flag,
        downgrade_flag,
        is_trial

    from `workspace`.`analytics`.`int_account_subscriptions`

    where is_trial = false  -- exclude trials from revenue metrics

),

-- Generate a month spine from the earliest start_date to today
months as (

    select
        date_trunc('month', add_months(
            (select min(start_date) from subscriptions),
            pos
        )) as month_start

    from (
        select explode(sequence(0,
            months_between(current_date(), (select min(start_date) from subscriptions))
        )) as pos
    )

),

-- Classify each subscription per month
monthly_subs as (

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        s.mrr_amount,
        'new'                                                   as mrr_type

    from subscriptions s

    union all

    select
        date_trunc('month', s.end_date)                         as month_start,
        s.plan_tier,
        -s.mrr_amount                                           as mrr_amount,
        'churned'                                               as mrr_type

    from subscriptions s
    where s.churn_flag = true and s.end_date is not null

    union all

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        s.mrr_amount,
        'expansion'                                             as mrr_type

    from subscriptions s
    where s.upgrade_flag = true

    union all

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        -s.mrr_amount                                           as mrr_amount,
        'contraction'                                           as mrr_type

    from subscriptions s
    where s.downgrade_flag = true

),

aggregated as (

    select
        month_start,
        plan_tier,
        mrr_type,
        sum(mrr_amount)                                         as mrr_movement,
        count(*)                                                as subscription_count

    from monthly_subs
    where month_start is not null
    group by month_start, plan_tier, mrr_type

),

final as (

    select
        month_start,
        plan_tier,

        -- MRR by type
        sum(case when mrr_type = 'new'         then mrr_movement else 0 end) as new_mrr,
        sum(case when mrr_type = 'churned'     then mrr_movement else 0 end) as churned_mrr,
        sum(case when mrr_type = 'expansion'   then mrr_movement else 0 end) as expansion_mrr,
        sum(case when mrr_type = 'contraction' then mrr_movement else 0 end) as contraction_mrr,
        sum(mrr_movement)                                                     as net_mrr_change,

        -- Subscription counts
        sum(case when mrr_type = 'new'     then subscription_count else 0 end) as new_subscriptions,
        sum(case when mrr_type = 'churned' then subscription_count else 0 end) as churned_subscriptions,

        current_timestamp()                                                   as updated_at

    from aggregated
    group by month_start, plan_tier

)

select * from final
order by month_start, plan_tier
  
[0m20:56:04.955067 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:56:08.800123 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-54c6-14fa-b655-2fed34cb62f8) - Created
[0m20:56:10.605944 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.gold_mrr_trends"} */

  
    
        create or replace table `workspace`.`analytics`.`gold_mrr_trends`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      /*
    Grain: one row per month
    Core SaaS revenue metrics: new MRR, churned MRR, net MRR, active accounts.
    Used for executive revenue dashboards and board reporting.
*/

with subscriptions as (

    select
        account_id,
        subscription_id,
        start_date,
        end_date,
        plan_tier,
        mrr_amount,
        arr_amount,
        churn_flag,
        upgrade_flag,
        downgrade_flag,
        is_trial

    from `workspace`.`analytics`.`int_account_subscriptions`

    where is_trial = false  -- exclude trials from revenue metrics

),

-- Generate a month spine from the earliest start_date to today
months as (

    select
        date_trunc('month', add_months(
            (select min(start_date) from subscriptions),
            pos
        )) as month_start

    from (
        select explode(sequence(0,
            months_between(current_date(), (select min(start_date) from subscriptions))
        )) as pos
    )

),

-- Classify each subscription per month
monthly_subs as (

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        s.mrr_amount,
        'new'                                                   as mrr_type

    from subscriptions s

    union all

    select
        date_trunc('month', s.end_date)                         as month_start,
        s.plan_tier,
        -s.mrr_amount                                           as mrr_amount,
        'churned'                                               as mrr_type

    from subscriptions s
    where s.churn_flag = true and s.end_date is not null

    union all

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        s.mrr_amount,
        'expansion'                                             as mrr_type

    from subscriptions s
    where s.upgrade_flag = true

    union all

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        -s.mrr_amount                                           as mrr_amount,
        'contraction'                                           as mrr_type

    from subscriptions s
    where s.downgrade_flag = true

),

aggregated as (

    select
        month_start,
        plan_tier,
        mrr_type,
        sum(mrr_amount)                                         as mrr_movement,
        count(*)                                                as subscription_count

    from monthly_subs
    where month_start is not null
    group by month_start, plan_tier, mrr_type

),

final as (

    select
        month_start,
        plan_tier,

        -- MRR by type
        sum(case when mrr_type = 'new'         then mrr_movement else 0 end) as new_mrr,
        sum(case when mrr_type = 'churned'     then mrr_movement else 0 end) as churned_mrr,
        sum(case when mrr_type = 'expansion'   then mrr_movement else 0 end) as expansion_mrr,
        sum(case when mrr_type = 'contraction' then mrr_movement else 0 end) as contraction_mrr,
        sum(mrr_movement)                                                     as net_mrr_change,

        -- Subscription counts
        sum(case when mrr_type = 'new'     then subscription_count else 0 end) as new_subscriptions,
        sum(case when mrr_type = 'churned' then subscription_count else 0 end) as churned_subscriptions,

        current_timestamp()                                                   as updated_at

    from aggregated
    group by month_start, plan_tier

)

select * from final
order by month_start, plan_tier
  
: [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] Cannot resolve "sequence(0, months_between(current_date(), scalarsubquery(), true))" due to data type mismatch: `sequence` uses the wrong parameter type. The parameter type must conform to:
1. The start and stop expressions must resolve to the same type.
2. If start and stop expressions resolve to the ("TIMESTAMP" or "TIMESTAMP_NTZ" or "DATE") type, then the step expression must resolve to the ("INTERVAL" or "INTERVAL YEAR TO MONTH" or "INTERVAL DAY TO SECOND") type.
3. Otherwise, if start and stop expressions resolve to the "INTEGRAL" type, then the step expression must resolve to the same type. SQLSTATE: 42K09; line 55 pos 23
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] Cannot resolve "sequence(0, months_between(current_date(), scalarsubquery(), true))" due to data type mismatch: `sequence` uses the wrong parameter type. The parameter type must conform to:
1. The start and stop expressions must resolve to the same type.
2. If start and stop expressions resolve to the ("TIMESTAMP" or "TIMESTAMP_NTZ" or "DATE") type, then the step expression must resolve to the ("INTERVAL" or "INTERVAL YEAR TO MONTH" or "INTERVAL DAY TO SECOND") type.
3. Otherwise, if start and stop expressions resolve to the "INTEGRAL" type, then the step expression must resolve to the same type. SQLSTATE: 42K09; line 55 pos 23
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] Cannot resolve "sequence(0, months_between(current_date(), scalarsubquery(), true))" due to data type mismatch: `sequence` uses the wrong parameter type. The parameter type must conform to:
1. The start and stop expressions must resolve to the same type.
2. If start and stop expressions resolve to the ("TIMESTAMP" or "TIMESTAMP_NTZ" or "DATE") type, then the step expression must resolve to the ("INTERVAL" or "INTERVAL YEAR TO MONTH" or "INTERVAL DAY TO SECOND") type.
3. Otherwise, if start and stop expressions resolve to the "INTEGRAL" type, then the step expression must resolve to the same type. SQLSTATE: 42K09; line 55 pos 23
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10da7-54f2-1155-aa09-5f13040fe700
[0m20:56:10.607952 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_mrr_trends: Close
[0m20:56:10.609961 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-54c6-14fa-b655-2fed34cb62f8) - Closing
[0m20:56:10.984279 [debug] [Thread-3 (]: Database Error in model gold_mrr_trends (models\gold\gold_mrr_trends.sql)
  [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] Cannot resolve "sequence(0, months_between(current_date(), scalarsubquery(), true))" due to data type mismatch: `sequence` uses the wrong parameter type. The parameter type must conform to:
  1. The start and stop expressions must resolve to the same type.
  2. If start and stop expressions resolve to the ("TIMESTAMP" or "TIMESTAMP_NTZ" or "DATE") type, then the step expression must resolve to the ("INTERVAL" or "INTERVAL YEAR TO MONTH" or "INTERVAL DAY TO SECOND") type.
  3. Otherwise, if start and stop expressions resolve to the "INTEGRAL" type, then the step expression must resolve to the same type. SQLSTATE: 42K09; line 55 pos 23
  compiled code at target\run\saas_dbt_analytics\models\gold\gold_mrr_trends.sql
[0m20:56:10.990648 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B3D85960>]}
[0m20:56:10.993664 [error] [Thread-3 (]: 13 of 16 ERROR creating sql table model analytics.gold_mrr_trends .............. [[31mERROR[0m in 6.08s]
[0m20:56:10.993664 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.gold_mrr_trends
[0m20:56:10.993664 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.int_account_360
[0m20:56:10.998532 [debug] [Thread-6 (]: Marking all children of 'model.saas_dbt_analytics.gold_mrr_trends' to be skipped because of status 'error'.  Reason: Database Error in model gold_mrr_trends (models\gold\gold_mrr_trends.sql)
  [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] Cannot resolve "sequence(0, months_between(current_date(), scalarsubquery(), true))" due to data type mismatch: `sequence` uses the wrong parameter type. The parameter type must conform to:
  1. The start and stop expressions must resolve to the same type.
  2. If start and stop expressions resolve to the ("TIMESTAMP" or "TIMESTAMP_NTZ" or "DATE") type, then the step expression must resolve to the ("INTERVAL" or "INTERVAL YEAR TO MONTH" or "INTERVAL DAY TO SECOND") type.
  3. Otherwise, if start and stop expressions resolve to the "INTEGRAL" type, then the step expression must resolve to the same type. SQLSTATE: 42K09; line 55 pos 23
  compiled code at target\run\saas_dbt_analytics\models\gold\gold_mrr_trends.sql.
[0m20:56:10.998532 [info ] [Thread-3 (]: 14 of 16 START sql table model analytics.int_account_360 ....................... [RUN]
[0m20:56:11.002846 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.int_account_360) - Creating connection
[0m20:56:11.005575 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.int_account_360'
[0m20:56:11.006641 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.int_account_360
[0m20:56:11.022950 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.int_account_360"
[0m20:56:11.023966 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.int_account_360
[0m20:56:11.023966 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m20:56:11.034818 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.int_account_360"
[0m20:56:11.043220 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.int_account_360"
[0m20:56:11.043220 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_360: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.int_account_360"} */

  
    
        create or replace table `workspace`.`analytics`.`int_account_360`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      /*
    Grain: one row per account
    The central account spine joining all intermediate models.
    Materialized as table because it's an expensive join used by multiple gold models.
    Used by: gold_account_health_summary, gold_customer_segments
*/

with accounts as (

    select
        account_id,
        account_name,
        industry,
        country,
        signup_date,
        referral_source,
        plan_tier,
        seats,
        is_trial,
        churn_flag

    from `workspace`.`analytics`.`stg_accounts`

),

-- Aggregate subscriptions to account level
subscription_summary as (

    select
        account_id,
        count(subscription_id)                                          as total_subscriptions,
        sum(mrr_amount)                                                 as total_mrr,
        sum(arr_amount)                                                 as total_arr,
        max(mrr_amount)                                                 as peak_mrr,
        count(case when is_active = true then 1 end)                    as active_subscriptions,
        max(case when is_active = true then plan_tier end)              as current_plan_tier,
        max(case when is_active = true then mrr_amount end)             as current_mrr,
        bool_or(upgrade_flag)                                           as ever_upgraded,
        bool_or(downgrade_flag)                                         as ever_downgraded,
        max(start_date)                                                 as latest_subscription_start

    from `workspace`.`analytics`.`int_account_subscriptions`
    group by account_id

),

-- Latest churn event per account
churn_summary as (

    select
        account_id,
        churn_date,
        reason_code                                                     as churn_reason,
        refund_amount_usd,
        churn_path,
        days_to_churn,
        mrr_at_churn

    from `workspace`.`analytics`.`int_account_churn`
    qualify row_number() over (partition by account_id order by churn_date desc) = 1

),

support as (

    select * from `workspace`.`analytics`.`int_account_support`

),

feature_usage as (

    select * from `workspace`.`analytics`.`int_account_feature_usage`

),

joined as (

    select
        -- Account core
        a.account_id,
        a.account_name,
        a.industry,
        a.country,
        a.signup_date,
        a.referral_source,
        a.plan_tier          as original_plan_tier,
        a.seats,
        a.is_trial,
        a.churn_flag,

        -- Subscription summary
        s.total_subscriptions,
        s.total_mrr,
        s.total_arr,
        s.peak_mrr,
        s.active_subscriptions,
        s.current_plan_tier,
        s.current_mrr,
        s.ever_upgraded,
        s.ever_downgraded,
        s.latest_subscription_start,

        -- Churn context (null if not churned)
        c.churn_date,
        c.churn_reason,
        c.refund_amount_usd,
        c.churn_path,
        c.days_to_churn,
        c.mrr_at_churn,

        -- Support metrics
        coalesce(t.total_tickets, 0)                                    as total_tickets,
        t.avg_resolution_time_hours,
        t.avg_satisfaction_score,
        coalesce(t.escalation_rate_pct, 0)                             as escalation_rate_pct,
        t.last_ticket_date,

        -- Feature usage metrics
        coalesce(u.distinct_features_used, 0)                          as distinct_features_used,
        coalesce(u.total_usage_count, 0)                               as total_usage_count,
        u.total_usage_hours,
        coalesce(u.error_rate_pct, 0)                                  as error_rate_pct,
        coalesce(u.beta_features_used, 0)                              as beta_features_used,
        u.top_feature,
        u.last_usage_date,
        coalesce(u.active_days, 0)                                     as active_days,

        -- Computed health score (0–100)
        -- Higher is healthier. Weighted across usage, support satisfaction, and retention signals.
        round(
            least(100,
                -- Usage engagement (40 pts max)
                least(40, coalesce(u.active_days, 0) * 0.5)
                -- Satisfaction (30 pts max)
                + coalesce(t.avg_satisfaction_score, 3) / 5.0 * 30
                -- No escalations (15 pts)
                + case when coalesce(t.escalation_rate_pct, 0) = 0 then 15 else 0 end
                -- Feature breadth (15 pts max)
                + least(15, coalesce(u.distinct_features_used, 0) * 1.5)
            ), 2
        )                                                               as health_score

    from accounts a
    left join subscription_summary s using (account_id)
    left join churn_summary c using (account_id)
    left join support t using (account_id)
    left join feature_usage u using (account_id)

)

select * from joined
  
[0m20:56:11.043220 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:56:14.678230 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-5849-1d04-9c50-bc55c7e034c9) - Created
[0m20:56:22.692915 [debug] [Thread-3 (]: SQL status: OK in 11.650 seconds
[0m20:56:22.696436 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-5849-1d04-9c50-bc55c7e034c9, command-id=01f10da7-5873-1def-a314-6d656b8fcdfe) - Closing
[0m20:56:23.064925 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:56:23.069173 [debug] [Thread-3 (]: On model.saas_dbt_analytics.int_account_360: Close
[0m20:56:23.070521 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-5849-1d04-9c50-bc55c7e034c9) - Closing
[0m20:56:23.394761 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C4EC66E0>]}
[0m20:56:23.397273 [info ] [Thread-3 (]: 14 of 16 OK created sql table model analytics.int_account_360 .................. [[32mOK[0m in 12.39s]
[0m20:56:23.397273 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.int_account_360
[0m20:56:23.397273 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.gold_account_health_summary
[0m20:56:23.404134 [info ] [Thread-3 (]: 15 of 16 START sql table model analytics.gold_account_health_summary ........... [RUN]
[0m20:56:23.409127 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.gold_account_health_summary) - Creating connection
[0m20:56:23.411324 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.gold_account_health_summary'
[0m20:56:23.413203 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.gold_account_health_summary
[0m20:56:23.422379 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.gold_account_health_summary"
[0m20:56:23.422379 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.gold_account_health_summary
[0m20:56:23.434111 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m20:56:23.434111 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.gold_account_health_summary"
[0m20:56:23.487318 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.gold_account_health_summary"
[0m20:56:23.488376 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_account_health_summary: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.gold_account_health_summary"} */

  
    
        create or replace table `workspace`.`analytics`.`gold_account_health_summary`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      /*
    Grain: one row per account
    Primary gold model for account health dashboards and CSM tooling.
    Source of truth for account-level KPIs.
*/

with base as (

    select * from `workspace`.`analytics`.`int_account_360`

),

final as (

    select
        -- Identity
        account_id,
        account_name,
        industry,
        country,
        referral_source,
        signup_date,

        -- Current state
        coalesce(current_plan_tier, original_plan_tier)         as plan_tier,
        seats,
        is_trial,
        churn_flag,
        active_subscriptions > 0                                as is_active_customer,

        -- Revenue
        coalesce(current_mrr, 0)                                as current_mrr,
        coalesce(total_arr, 0)                                  as total_arr,
        coalesce(peak_mrr, 0)                                   as peak_mrr,
        ever_upgraded,
        ever_downgraded,

        -- Engagement
        distinct_features_used,
        total_usage_count,
        total_usage_hours,
        active_days,
        top_feature,
        last_usage_date,
        beta_features_used,
        error_rate_pct,

        -- Support
        total_tickets,
        avg_resolution_time_hours,
        avg_satisfaction_score,
        escalation_rate_pct,
        last_ticket_date,

        -- Churn context
        churn_date,
        churn_reason,
        churn_path,
        days_to_churn,
        mrr_at_churn,
        refund_amount_usd,

        -- Health
        health_score,
        case
            when churn_flag = true                          then 'Churned'
            when health_score >= 75                         then 'Healthy'
            when health_score >= 50                         then 'At Risk'
            else                                                'Critical'
        end                                                     as health_status,

        -- Metadata
        current_timestamp()                                     as updated_at

    from base

)

select * from final
  
[0m20:56:23.490393 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:56:27.695795 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-6003-17d8-9c5b-ed50c0124fbc) - Created
[0m20:56:31.814926 [debug] [Thread-3 (]: SQL status: OK in 8.320 seconds
[0m20:56:31.814926 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-6003-17d8-9c5b-ed50c0124fbc, command-id=01f10da7-6037-1bb9-abc0-7f02f661b0f4) - Closing
[0m20:56:31.814926 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:56:31.814926 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_account_health_summary: Close
[0m20:56:31.830696 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-6003-17d8-9c5b-ed50c0124fbc) - Closing
[0m20:56:32.174907 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C5144160>]}
[0m20:56:32.174907 [info ] [Thread-3 (]: 15 of 16 OK created sql table model analytics.gold_account_health_summary ...... [[32mOK[0m in 8.77s]
[0m20:56:32.174907 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.gold_account_health_summary
[0m20:56:32.174907 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.gold_customer_segments
[0m20:56:32.174907 [info ] [Thread-3 (]: 16 of 16 START sql table model analytics.gold_customer_segments ................ [RUN]
[0m20:56:32.188680 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.gold_customer_segments) - Creating connection
[0m20:56:32.190157 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.gold_customer_segments'
[0m20:56:32.192359 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.gold_customer_segments
[0m20:56:32.205919 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.gold_customer_segments"
[0m20:56:32.205919 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.gold_customer_segments
[0m20:56:32.216341 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m20:56:32.216341 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.gold_customer_segments"
[0m20:56:32.216341 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.gold_customer_segments"
[0m20:56:32.216341 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_customer_segments: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.gold_customer_segments"} */

  
    
        create or replace table `workspace`.`analytics`.`gold_customer_segments`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      /*
    Grain: one row per account
    Rule-based customer segmentation for sales, marketing, and CS targeting.
    Segments are derived from health score, MRR, churn risk, and engagement signals.
*/

with base as (

    select * from `workspace`.`analytics`.`int_account_360`

    where churn_flag = false  -- active accounts only

),

segmented as (

    select
        account_id,
        account_name,
        industry,
        country,
        coalesce(current_plan_tier, original_plan_tier)         as plan_tier,
        coalesce(current_mrr, 0)                                as current_mrr,
        health_score,
        distinct_features_used,
        active_days,
        total_tickets,
        avg_satisfaction_score,
        escalation_rate_pct,
        ever_upgraded,
        ever_downgraded,
        signup_date,
        last_usage_date,

        -- Days since last activity
        datediff(current_date(), last_usage_date)               as days_since_last_activity,

        -- Primary segment
        case
            when health_score >= 80
                and coalesce(current_mrr, 0) >= 1000
                and ever_upgraded = true                        then 'Champion'

            when health_score >= 75
                and coalesce(current_mrr, 0) >= 500             then 'Loyal'

            when health_score >= 60
                and datediff(current_date(), signup_date) <= 90  then 'Promising'

            when health_score >= 50
                and datediff(current_date(), signup_date) <= 30  then 'New Customer'

            when health_score < 50
                and ever_downgraded = true                      then 'At Risk – Downgraded'

            when health_score < 40
                and datediff(current_date(), last_usage_date) > 30
                                                                then 'Hibernating'

            when health_score < 30                              then 'Critical – Likely to Churn'

            else                                                     'Needs Attention'
        end                                                     as customer_segment,

        -- Recommended action
        case
            when health_score >= 80
                and coalesce(current_mrr, 0) >= 1000
                and ever_upgraded = true                        then 'Upsell / Advocacy program'

            when health_score >= 75
                and coalesce(current_mrr, 0) >= 500             then 'Nurture and expand'

            when health_score >= 60
                and datediff(current_date(), signup_date) <= 90  then 'Onboarding check-in'

            when health_score >= 50
                and datediff(current_date(), signup_date) <= 30  then 'Send welcome sequence'

            when health_score < 50
                and ever_downgraded = true                      then 'CSM outreach – retention offer'

            when health_score < 40
                and datediff(current_date(), last_usage_date) > 30
                                                                then 'Re-engagement campaign'

            when health_score < 30                              then 'Urgent CSM intervention'

            else                                                     'Monitor and check in'
        end                                                     as recommended_action,

        current_timestamp()                                     as updated_at

    from base

)

select * from segmented
order by health_score asc  -- surface at-risk accounts first
  
[0m20:56:32.230228 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:56:35.668330 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-64bf-1d4e-b940-746ee262d059) - Created
[0m20:56:39.872165 [debug] [Thread-3 (]: SQL status: OK in 7.660 seconds
[0m20:56:39.874172 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da7-64bf-1d4e-b940-746ee262d059, command-id=01f10da7-64f6-165b-b402-c517be968730) - Closing
[0m20:56:39.877689 [debug] [Thread-3 (]: Applying tags to relation None
[0m20:56:39.881708 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_customer_segments: Close
[0m20:56:39.881708 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-64bf-1d4e-b940-746ee262d059) - Closing
[0m20:56:40.235683 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0faf1c93-8d38-4572-99c4-747aacb6de36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229C5144160>]}
[0m20:56:40.235683 [info ] [Thread-3 (]: 16 of 16 OK created sql table model analytics.gold_customer_segments ........... [[32mOK[0m in 8.06s]
[0m20:56:40.251408 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.gold_customer_segments
[0m20:56:40.253417 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:56:40.253417 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:56:40.253417 [info ] [MainThread]: 
[0m20:56:40.253417 [info ] [MainThread]: Finished running 7 table models, 9 view models in 0 hours 2 minutes and 19.02 seconds (139.02s).
[0m20:56:40.265053 [debug] [MainThread]: Command end result
[0m20:56:40.372740 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:56:40.379357 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:56:40.396358 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:56:40.396358 [info ] [MainThread]: 
[0m20:56:40.396358 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m20:56:40.396358 [info ] [MainThread]: 
[0m20:56:40.396358 [error] [MainThread]: [31mFailure in model gold_mrr_trends (models\gold\gold_mrr_trends.sql)[0m
[0m20:56:40.404460 [error] [MainThread]:   Database Error in model gold_mrr_trends (models\gold\gold_mrr_trends.sql)
  [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] Cannot resolve "sequence(0, months_between(current_date(), scalarsubquery(), true))" due to data type mismatch: `sequence` uses the wrong parameter type. The parameter type must conform to:
  1. The start and stop expressions must resolve to the same type.
  2. If start and stop expressions resolve to the ("TIMESTAMP" or "TIMESTAMP_NTZ" or "DATE") type, then the step expression must resolve to the ("INTERVAL" or "INTERVAL YEAR TO MONTH" or "INTERVAL DAY TO SECOND") type.
  3. Otherwise, if start and stop expressions resolve to the "INTEGRAL" type, then the step expression must resolve to the same type. SQLSTATE: 42K09; line 55 pos 23
  compiled code at target\run\saas_dbt_analytics\models\gold\gold_mrr_trends.sql
[0m20:56:40.405497 [info ] [MainThread]: 
[0m20:56:40.407697 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\gold\gold_mrr_trends.sql
[0m20:56:40.409947 [info ] [MainThread]: 
[0m20:56:40.412389 [info ] [MainThread]: Done. PASS=15 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=16
[0m20:56:40.415803 [debug] [MainThread]: Command `dbt run` failed at 20:56:40.415803 after 146.59 seconds
[0m20:56:40.417142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229AF0DEB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B13A1F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229B13A2860>]}
[0m20:56:40.418369 [debug] [MainThread]: Flushing usage events
[0m20:56:49.249637 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:58:48.261756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6C10DEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6C33F03A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6C33F01F0>]}


============================== 20:58:48.275419 | b6f1d76b-5e45-4860-9e03-e46b1d9d8473 ==============================
[0m20:58:48.275419 [info ] [MainThread]: Running with dbt=1.11.2
[0m20:58:48.277163 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'invocation_command': 'dbt run --select gold_mrr_trends.sql', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m20:58:51.193975 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:58:51.195974 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:58:51.198006 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:58:53.721129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b6f1d76b-5e45-4860-9e03-e46b1d9d8473', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6C33F0460>]}
[0m20:58:53.909221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b6f1d76b-5e45-4860-9e03-e46b1d9d8473', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6C2AFDCF0>]}
[0m20:58:53.911228 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m20:58:55.237904 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:58:55.239959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b6f1d76b-5e45-4860-9e03-e46b1d9d8473', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6C2F0C3D0>]}
[0m20:58:55.301750 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m20:58:56.071199 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:58:56.073263 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m20:58:56.074294 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:58:56.234409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b6f1d76b-5e45-4860-9e03-e46b1d9d8473', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6D6F13D60>]}
[0m20:58:56.564524 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:58:56.578523 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:58:56.619876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b6f1d76b-5e45-4860-9e03-e46b1d9d8473', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6D727EC20>]}
[0m20:58:56.621885 [info ] [MainThread]: Found 16 models, 32 data tests, 845 macros
[0m20:58:56.621885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b6f1d76b-5e45-4860-9e03-e46b1d9d8473', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6D727EBC0>]}
[0m20:58:56.627914 [info ] [MainThread]: 
[0m20:58:56.627914 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:58:56.634027 [info ] [MainThread]: 
[0m20:58:56.634027 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:58:56.637103 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:58:56.642151 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m20:58:56.643480 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m20:58:56.688315 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m20:58:56.690325 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m20:58:56.690325 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:59:00.592989 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da7-bb27-1834-9e8c-6ae8f6af6eae) - Created
[0m20:59:01.173385 [debug] [ThreadPool]: SQL status: OK in 4.480 seconds
[0m20:59:01.189488 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10da7-bb27-1834-9e8c-6ae8f6af6eae, command-id=01f10da7-bb58-1991-8c2b-9b0ef06b7d20) - Closing
[0m20:59:01.189488 [debug] [ThreadPool]: On list_workspace: Close
[0m20:59:01.189488 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da7-bb27-1834-9e8c-6ae8f6af6eae) - Closing
[0m20:59:01.569401 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m20:59:01.570589 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m20:59:01.589164 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m20:59:01.590540 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m20:59:01.590540 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:59:05.014781 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da7-bdce-12a9-a1d9-75dee9cb667c) - Created
[0m20:59:05.708307 [debug] [ThreadPool]: SQL status: OK in 4.120 seconds
[0m20:59:05.722022 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10da7-bdce-12a9-a1d9-75dee9cb667c, command-id=01f10da7-bdfc-195e-9c1d-7e59e06cb9cd) - Closing
[0m20:59:05.724029 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m20:59:05.724029 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da7-bdce-12a9-a1d9-75dee9cb667c) - Closing
[0m20:59:06.042611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b6f1d76b-5e45-4860-9e03-e46b1d9d8473', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6BFDA3A90>]}
[0m20:59:06.050855 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.gold_mrr_trends
[0m20:59:06.050855 [info ] [Thread-3 (]: 1 of 1 START sql table model analytics.gold_mrr_trends ......................... [RUN]
[0m20:59:06.055949 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.gold_mrr_trends) - Creating connection
[0m20:59:06.055949 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.gold_mrr_trends'
[0m20:59:06.055949 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.gold_mrr_trends
[0m20:59:06.076550 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.gold_mrr_trends"
[0m20:59:06.076550 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.gold_mrr_trends
[0m20:59:06.121317 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m20:59:06.122565 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m20:59:06.124956 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b6f1d76b-5e45-4860-9e03-e46b1d9d8473', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6D73FCF70>]}
[0m20:59:06.224937 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.gold_mrr_trends"
[0m20:59:06.227579 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.gold_mrr_trends"
[0m20:59:06.227579 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_mrr_trends: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.gold_mrr_trends"} */

  
    
        create or replace table `workspace`.`analytics`.`gold_mrr_trends`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      /*
    Grain: one row per month
    Core SaaS revenue metrics: new MRR, churned MRR, net MRR, active accounts.
    Used for executive revenue dashboards and board reporting.
*/

with subscriptions as (

    select
        account_id,
        subscription_id,
        start_date,
        end_date,
        plan_tier,
        mrr_amount,
        arr_amount,
        churn_flag,
        upgrade_flag,
        downgrade_flag,
        is_trial

    from `workspace`.`analytics`.`int_account_subscriptions`

    where is_trial = false  -- exclude trials from revenue metrics

),

-- Generate a month spine from the earliest start_date to today
months as (

    select
        date_trunc('month', add_months(
            (select min(start_date) from subscriptions),
            pos
        )) as month_start

    from (
        select explode(sequence(0,
            months_between(current_date(), (select min(start_date) from subscriptions))
        )) as pos
    )

),

-- Classify each subscription per month
monthly_subs as (

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        s.mrr_amount,
        'new'                                                   as mrr_type

    from subscriptions s

    union all

    select
        date_trunc('month', s.end_date)                         as month_start,
        s.plan_tier,
        -s.mrr_amount                                           as mrr_amount,
        'churned'                                               as mrr_type

    from subscriptions s
    where s.churn_flag = true and s.end_date is not null

    union all

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        s.mrr_amount,
        'expansion'                                             as mrr_type

    from subscriptions s
    where s.upgrade_flag = true

    union all

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        -s.mrr_amount                                           as mrr_amount,
        'contraction'                                           as mrr_type

    from subscriptions s
    where s.downgrade_flag = true

),

aggregated as (

    select
        month_start,
        plan_tier,
        mrr_type,
        sum(mrr_amount)                                         as mrr_movement,
        count(*)                                                as subscription_count

    from monthly_subs
    where month_start is not null
    group by month_start, plan_tier, mrr_type

),

final as (

    select
        month_start,
        plan_tier,

        -- MRR by type
        sum(case when mrr_type = 'new'         then mrr_movement else 0 end) as new_mrr,
        sum(case when mrr_type = 'churned'     then mrr_movement else 0 end) as churned_mrr,
        sum(case when mrr_type = 'expansion'   then mrr_movement else 0 end) as expansion_mrr,
        sum(case when mrr_type = 'contraction' then mrr_movement else 0 end) as contraction_mrr,
        sum(mrr_movement)                                                     as net_mrr_change,

        -- Subscription counts
        sum(case when mrr_type = 'new'     then subscription_count else 0 end) as new_subscriptions,
        sum(case when mrr_type = 'churned' then subscription_count else 0 end) as churned_subscriptions,

        current_timestamp()                                                   as updated_at

    from aggregated
    group by month_start, plan_tier

)

select * from final
order by month_start, plan_tier
  
[0m20:59:06.227579 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:59:09.453000 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-c078-1412-b4c9-dab6c26ec407) - Created
[0m20:59:10.926399 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.gold_mrr_trends"} */

  
    
        create or replace table `workspace`.`analytics`.`gold_mrr_trends`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      /*
    Grain: one row per month
    Core SaaS revenue metrics: new MRR, churned MRR, net MRR, active accounts.
    Used for executive revenue dashboards and board reporting.
*/

with subscriptions as (

    select
        account_id,
        subscription_id,
        start_date,
        end_date,
        plan_tier,
        mrr_amount,
        arr_amount,
        churn_flag,
        upgrade_flag,
        downgrade_flag,
        is_trial

    from `workspace`.`analytics`.`int_account_subscriptions`

    where is_trial = false  -- exclude trials from revenue metrics

),

-- Generate a month spine from the earliest start_date to today
months as (

    select
        date_trunc('month', add_months(
            (select min(start_date) from subscriptions),
            pos
        )) as month_start

    from (
        select explode(sequence(0,
            months_between(current_date(), (select min(start_date) from subscriptions))
        )) as pos
    )

),

-- Classify each subscription per month
monthly_subs as (

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        s.mrr_amount,
        'new'                                                   as mrr_type

    from subscriptions s

    union all

    select
        date_trunc('month', s.end_date)                         as month_start,
        s.plan_tier,
        -s.mrr_amount                                           as mrr_amount,
        'churned'                                               as mrr_type

    from subscriptions s
    where s.churn_flag = true and s.end_date is not null

    union all

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        s.mrr_amount,
        'expansion'                                             as mrr_type

    from subscriptions s
    where s.upgrade_flag = true

    union all

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        -s.mrr_amount                                           as mrr_amount,
        'contraction'                                           as mrr_type

    from subscriptions s
    where s.downgrade_flag = true

),

aggregated as (

    select
        month_start,
        plan_tier,
        mrr_type,
        sum(mrr_amount)                                         as mrr_movement,
        count(*)                                                as subscription_count

    from monthly_subs
    where month_start is not null
    group by month_start, plan_tier, mrr_type

),

final as (

    select
        month_start,
        plan_tier,

        -- MRR by type
        sum(case when mrr_type = 'new'         then mrr_movement else 0 end) as new_mrr,
        sum(case when mrr_type = 'churned'     then mrr_movement else 0 end) as churned_mrr,
        sum(case when mrr_type = 'expansion'   then mrr_movement else 0 end) as expansion_mrr,
        sum(case when mrr_type = 'contraction' then mrr_movement else 0 end) as contraction_mrr,
        sum(mrr_movement)                                                     as net_mrr_change,

        -- Subscription counts
        sum(case when mrr_type = 'new'     then subscription_count else 0 end) as new_subscriptions,
        sum(case when mrr_type = 'churned' then subscription_count else 0 end) as churned_subscriptions,

        current_timestamp()                                                   as updated_at

    from aggregated
    group by month_start, plan_tier

)

select * from final
order by month_start, plan_tier
  
: [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] Cannot resolve "sequence(0, months_between(current_date(), scalarsubquery(), true))" due to data type mismatch: `sequence` uses the wrong parameter type. The parameter type must conform to:
1. The start and stop expressions must resolve to the same type.
2. If start and stop expressions resolve to the ("TIMESTAMP" or "TIMESTAMP_NTZ" or "DATE") type, then the step expression must resolve to the ("INTERVAL" or "INTERVAL YEAR TO MONTH" or "INTERVAL DAY TO SECOND") type.
3. Otherwise, if start and stop expressions resolve to the "INTEGRAL" type, then the step expression must resolve to the same type. SQLSTATE: 42K09; line 55 pos 23
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] Cannot resolve "sequence(0, months_between(current_date(), scalarsubquery(), true))" due to data type mismatch: `sequence` uses the wrong parameter type. The parameter type must conform to:
1. The start and stop expressions must resolve to the same type.
2. If start and stop expressions resolve to the ("TIMESTAMP" or "TIMESTAMP_NTZ" or "DATE") type, then the step expression must resolve to the ("INTERVAL" or "INTERVAL YEAR TO MONTH" or "INTERVAL DAY TO SECOND") type.
3. Otherwise, if start and stop expressions resolve to the "INTEGRAL" type, then the step expression must resolve to the same type. SQLSTATE: 42K09; line 55 pos 23
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:50)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1067)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:804)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:595)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$17(ThriftLocalProperties.scala:343)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:215)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$14(ThriftLocalProperties.scala:338)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:334)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:164)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:545)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:608)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] Cannot resolve "sequence(0, months_between(current_date(), scalarsubquery(), true))" due to data type mismatch: `sequence` uses the wrong parameter type. The parameter type must conform to:
1. The start and stop expressions must resolve to the same type.
2. If start and stop expressions resolve to the ("TIMESTAMP" or "TIMESTAMP_NTZ" or "DATE") type, then the step expression must resolve to the ("INTERVAL" or "INTERVAL YEAR TO MONTH" or "INTERVAL DAY TO SECOND") type.
3. Otherwise, if start and stop expressions resolve to the "INTEGRAL" type, then the step expression must resolve to the same type. SQLSTATE: 42K09; line 55 pos 23
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1028)
	... 53 more
, operation-id=01f10da7-c0cb-1bae-96ac-9dfb2a9d7988
[0m20:59:10.926399 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_mrr_trends: Close
[0m20:59:10.926399 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da7-c078-1412-b4c9-dab6c26ec407) - Closing
[0m20:59:11.270656 [debug] [Thread-3 (]: Database Error in model gold_mrr_trends (models\gold\gold_mrr_trends.sql)
  [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] Cannot resolve "sequence(0, months_between(current_date(), scalarsubquery(), true))" due to data type mismatch: `sequence` uses the wrong parameter type. The parameter type must conform to:
  1. The start and stop expressions must resolve to the same type.
  2. If start and stop expressions resolve to the ("TIMESTAMP" or "TIMESTAMP_NTZ" or "DATE") type, then the step expression must resolve to the ("INTERVAL" or "INTERVAL YEAR TO MONTH" or "INTERVAL DAY TO SECOND") type.
  3. Otherwise, if start and stop expressions resolve to the "INTEGRAL" type, then the step expression must resolve to the same type. SQLSTATE: 42K09; line 55 pos 23
  compiled code at target\run\saas_dbt_analytics\models\gold\gold_mrr_trends.sql
[0m20:59:11.270656 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6f1d76b-5e45-4860-9e03-e46b1d9d8473', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6C0F062C0>]}
[0m20:59:11.270656 [error] [Thread-3 (]: 1 of 1 ERROR creating sql table model analytics.gold_mrr_trends ................ [[31mERROR[0m in 5.22s]
[0m20:59:11.270656 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.gold_mrr_trends
[0m20:59:11.270656 [debug] [Thread-6 (]: Marking all children of 'model.saas_dbt_analytics.gold_mrr_trends' to be skipped because of status 'error'.  Reason: Database Error in model gold_mrr_trends (models\gold\gold_mrr_trends.sql)
  [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] Cannot resolve "sequence(0, months_between(current_date(), scalarsubquery(), true))" due to data type mismatch: `sequence` uses the wrong parameter type. The parameter type must conform to:
  1. The start and stop expressions must resolve to the same type.
  2. If start and stop expressions resolve to the ("TIMESTAMP" or "TIMESTAMP_NTZ" or "DATE") type, then the step expression must resolve to the ("INTERVAL" or "INTERVAL YEAR TO MONTH" or "INTERVAL DAY TO SECOND") type.
  3. Otherwise, if start and stop expressions resolve to the "INTEGRAL" type, then the step expression must resolve to the same type. SQLSTATE: 42K09; line 55 pos 23
  compiled code at target\run\saas_dbt_analytics\models\gold\gold_mrr_trends.sql.
[0m20:59:11.270656 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m20:59:11.270656 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:59:11.286580 [info ] [MainThread]: 
[0m20:59:11.289622 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 14.65 seconds (14.65s).
[0m20:59:11.292187 [debug] [MainThread]: Command end result
[0m20:59:11.395702 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m20:59:11.402639 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m20:59:11.412178 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m20:59:11.412178 [info ] [MainThread]: 
[0m20:59:11.412178 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m20:59:11.412178 [info ] [MainThread]: 
[0m20:59:11.412178 [error] [MainThread]: [31mFailure in model gold_mrr_trends (models\gold\gold_mrr_trends.sql)[0m
[0m20:59:11.412178 [error] [MainThread]:   Database Error in model gold_mrr_trends (models\gold\gold_mrr_trends.sql)
  [DATATYPE_MISMATCH.SEQUENCE_WRONG_INPUT_TYPES] Cannot resolve "sequence(0, months_between(current_date(), scalarsubquery(), true))" due to data type mismatch: `sequence` uses the wrong parameter type. The parameter type must conform to:
  1. The start and stop expressions must resolve to the same type.
  2. If start and stop expressions resolve to the ("TIMESTAMP" or "TIMESTAMP_NTZ" or "DATE") type, then the step expression must resolve to the ("INTERVAL" or "INTERVAL YEAR TO MONTH" or "INTERVAL DAY TO SECOND") type.
  3. Otherwise, if start and stop expressions resolve to the "INTEGRAL" type, then the step expression must resolve to the same type. SQLSTATE: 42K09; line 55 pos 23
  compiled code at target\run\saas_dbt_analytics\models\gold\gold_mrr_trends.sql
[0m20:59:11.426636 [info ] [MainThread]: 
[0m20:59:11.428869 [info ] [MainThread]:   compiled code at target\compiled\saas_dbt_analytics\models\gold\gold_mrr_trends.sql
[0m20:59:11.430274 [info ] [MainThread]: 
[0m20:59:11.431669 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m20:59:11.434852 [debug] [MainThread]: Command `dbt run` failed at 20:59:11.433844 after 23.37 seconds
[0m20:59:11.435863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6C10DEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6C5D8F7C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C6C24036D0>]}
[0m20:59:11.436872 [debug] [MainThread]: Flushing usage events
[0m20:59:14.204737 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:01:27.682555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211DFC7EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211E1F90370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211E1F901C0>]}


============================== 21:01:27.682555 | 7315347f-4e19-44c3-b883-0dc2160aab03 ==============================
[0m21:01:27.682555 [info ] [MainThread]: Running with dbt=1.11.2
[0m21:01:27.682555 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'invocation_command': 'dbt run --select gold_mrr_trends.sql', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m21:01:29.855860 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:01:29.855860 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:01:29.855860 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:01:31.787997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7315347f-4e19-44c3-b883-0dc2160aab03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211E1F90190>]}
[0m21:01:31.914724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7315347f-4e19-44c3-b883-0dc2160aab03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211E1F93EB0>]}
[0m21:01:31.930762 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m21:01:32.927451 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m21:01:32.927451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7315347f-4e19-44c3-b883-0dc2160aab03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211E1A8C190>]}
[0m21:01:32.977754 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m21:01:33.584455 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:01:33.586463 [debug] [MainThread]: Partial parsing: updated file: saas_dbt_analytics://models\gold\gold_mrr_trends.sql
[0m21:01:34.188402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7315347f-4e19-44c3-b883-0dc2160aab03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211F60415D0>]}
[0m21:01:34.432849 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m21:01:34.437384 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m21:01:34.491280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7315347f-4e19-44c3-b883-0dc2160aab03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211F5E2A950>]}
[0m21:01:34.495302 [info ] [MainThread]: Found 16 models, 32 data tests, 845 macros
[0m21:01:34.497313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7315347f-4e19-44c3-b883-0dc2160aab03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211F5E2A920>]}
[0m21:01:34.508121 [info ] [MainThread]: 
[0m21:01:34.512371 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:01:34.515641 [info ] [MainThread]: 
[0m21:01:34.519058 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m21:01:34.522249 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:01:34.524243 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace) - Creating connection
[0m21:01:34.524243 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:01:34.559310 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:01:34.559310 [debug] [ThreadPool]: On list_workspace: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace"} */

    

  SHOW SCHEMAS IN `workspace`


  
[0m21:01:34.559310 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:01:38.736332 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da8-1965-1308-ba1f-53966288427d) - Created
[0m21:01:39.488088 [debug] [ThreadPool]: SQL status: OK in 4.930 seconds
[0m21:01:39.488088 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10da8-1965-1308-ba1f-53966288427d, command-id=01f10da8-199d-1a81-b45d-6bad85896b34) - Closing
[0m21:01:39.498403 [debug] [ThreadPool]: On list_workspace: Close
[0m21:01:39.498403 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da8-1965-1308-ba1f-53966288427d) - Closing
[0m21:01:39.959026 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m21:01:39.959026 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m21:01:39.974835 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m21:01:39.976473 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m21:01:39.977762 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:01:43.949819 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da8-1c1c-105f-b399-c47259a8def5) - Created
[0m21:01:45.008863 [debug] [ThreadPool]: SQL status: OK in 5.030 seconds
[0m21:01:45.008863 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10da8-1c1c-105f-b399-c47259a8def5, command-id=01f10da8-1cb5-1e47-8031-b41bcf9702f2) - Closing
[0m21:01:45.008863 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m21:01:45.020942 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da8-1c1c-105f-b399-c47259a8def5) - Closing
[0m21:01:45.348003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7315347f-4e19-44c3-b883-0dc2160aab03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211F6043790>]}
[0m21:01:45.354773 [debug] [Thread-3 (]: Began running node model.saas_dbt_analytics.gold_mrr_trends
[0m21:01:45.354773 [info ] [Thread-3 (]: 1 of 1 START sql table model analytics.gold_mrr_trends ......................... [RUN]
[0m21:01:45.354773 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.saas_dbt_analytics.gold_mrr_trends) - Creating connection
[0m21:01:45.354773 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.saas_dbt_analytics.gold_mrr_trends'
[0m21:01:45.367363 [debug] [Thread-3 (]: Began compiling node model.saas_dbt_analytics.gold_mrr_trends
[0m21:01:45.404065 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_dbt_analytics.gold_mrr_trends"
[0m21:01:45.404065 [debug] [Thread-3 (]: Began executing node model.saas_dbt_analytics.gold_mrr_trends
[0m21:01:45.460604 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m21:01:45.460604 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m21:01:45.470068 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '7315347f-4e19-44c3-b883-0dc2160aab03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211F7143A60>]}
[0m21:01:45.599510 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_dbt_analytics.gold_mrr_trends"
[0m21:01:45.602842 [debug] [Thread-3 (]: Using databricks connection "model.saas_dbt_analytics.gold_mrr_trends"
[0m21:01:45.604849 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_mrr_trends: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "model.saas_dbt_analytics.gold_mrr_trends"} */

  
    
        create or replace table `workspace`.`analytics`.`gold_mrr_trends`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      /*
    Grain: one row per month
    Core SaaS revenue metrics: new MRR, churned MRR, net MRR, active accounts.
    Used for executive revenue dashboards and board reporting.
*/

with subscriptions as (

    select
        account_id,
        subscription_id,
        start_date,
        end_date,
        plan_tier,
        mrr_amount,
        arr_amount,
        churn_flag,
        upgrade_flag,
        downgrade_flag,
        is_trial

    from `workspace`.`analytics`.`int_account_subscriptions`

    where is_trial = false  -- exclude trials from revenue metrics

),

-- Generate a month spine from the earliest start_date to today
months as (

    select
        date_trunc('month', add_months(
            (select min(start_date) from subscriptions),
            pos
        )) as month_start

    from (
        select explode(sequence(
            0,
            cast(floor(months_between(
                current_date(),
                (select min(start_date) from subscriptions)
            )) as int),
            1
        )) as pos
    )

),

-- Classify each subscription per month
monthly_subs as (

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        s.mrr_amount,
        'new'                                                   as mrr_type

    from subscriptions s

    union all

    select
        date_trunc('month', s.end_date)                         as month_start,
        s.plan_tier,
        -s.mrr_amount                                           as mrr_amount,
        'churned'                                               as mrr_type

    from subscriptions s
    where s.churn_flag = true and s.end_date is not null

    union all

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        s.mrr_amount,
        'expansion'                                             as mrr_type

    from subscriptions s
    where s.upgrade_flag = true

    union all

    select
        date_trunc('month', s.start_date)                       as month_start,
        s.plan_tier,
        -s.mrr_amount                                           as mrr_amount,
        'contraction'                                           as mrr_type

    from subscriptions s
    where s.downgrade_flag = true

),

aggregated as (

    select
        month_start,
        plan_tier,
        mrr_type,
        sum(mrr_amount)                                         as mrr_movement,
        count(*)                                                as subscription_count

    from monthly_subs
    where month_start is not null
    group by month_start, plan_tier, mrr_type

),

final as (

    select
        month_start,
        plan_tier,

        -- MRR by type
        sum(case when mrr_type = 'new'         then mrr_movement else 0 end) as new_mrr,
        sum(case when mrr_type = 'churned'     then mrr_movement else 0 end) as churned_mrr,
        sum(case when mrr_type = 'expansion'   then mrr_movement else 0 end) as expansion_mrr,
        sum(case when mrr_type = 'contraction' then mrr_movement else 0 end) as contraction_mrr,
        sum(mrr_movement)                                                     as net_mrr_change,

        -- Subscription counts
        sum(case when mrr_type = 'new'     then subscription_count else 0 end) as new_subscriptions,
        sum(case when mrr_type = 'churned' then subscription_count else 0 end) as churned_subscriptions,

        current_timestamp()                                                   as updated_at

    from aggregated
    group by month_start, plan_tier

)

select * from final
order by month_start, plan_tier
  
[0m21:01:45.606870 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m21:01:49.405473 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da8-1fca-1c98-bdc6-ab8ac9deb29a) - Created
[0m21:01:55.946575 [debug] [Thread-3 (]: SQL status: OK in 10.340 seconds
[0m21:01:55.946575 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f10da8-1fca-1c98-bdc6-ab8ac9deb29a, command-id=01f10da8-1ff9-107d-a154-c23aca50e6ec) - Closing
[0m21:01:56.308238 [debug] [Thread-3 (]: Applying tags to relation None
[0m21:01:56.359722 [debug] [Thread-3 (]: On model.saas_dbt_analytics.gold_mrr_trends: Close
[0m21:01:56.361730 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f10da8-1fca-1c98-bdc6-ab8ac9deb29a) - Closing
[0m21:01:56.677181 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7315347f-4e19-44c3-b883-0dc2160aab03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211DE08EEC0>]}
[0m21:01:56.681594 [info ] [Thread-3 (]: 1 of 1 OK created sql table model analytics.gold_mrr_trends .................... [[32mOK[0m in 11.32s]
[0m21:01:56.681594 [debug] [Thread-3 (]: Finished running node model.saas_dbt_analytics.gold_mrr_trends
[0m21:01:56.685734 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m21:01:56.685734 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:01:56.690165 [info ] [MainThread]: 
[0m21:01:56.692173 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 22.17 seconds (22.17s).
[0m21:01:56.694182 [debug] [MainThread]: Command end result
[0m21:01:56.983217 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m21:01:56.993266 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m21:01:57.013683 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m21:01:57.015692 [info ] [MainThread]: 
[0m21:01:57.017700 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:01:57.017700 [info ] [MainThread]: 
[0m21:01:57.021924 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m21:01:57.026046 [debug] [MainThread]: Command `dbt run` succeeded at 21:01:57.021924 after 29.53 seconds
[0m21:01:57.026046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211DFC7EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211F5A53D60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211F5A52F20>]}
[0m21:01:57.026046 [debug] [MainThread]: Flushing usage events
[0m21:01:59.168943 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:03:11.022889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001949707EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194983BC3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194983BC1F0>]}


============================== 21:03:11.022889 | 24339678-021d-4529-9751-c882739ec624 ==============================
[0m21:03:11.022889 [info ] [MainThread]: Running with dbt=1.11.2
[0m21:03:11.022889 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\HP\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt test', 'use_experimental_parser': 'False', 'log_path': 'D:\\DataScience\\saas-databricks-dbt-analytics\\logs'}
[0m21:03:13.251393 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:03:13.251393 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:03:13.257424 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:03:15.165870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '24339678-021d-4529-9751-c882739ec624', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194944BBDC0>]}
[0m21:03:15.327454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '24339678-021d-4529-9751-c882739ec624', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194AB64EA70>]}
[0m21:03:15.327454 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m21:03:16.289194 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m21:03:16.289194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '24339678-021d-4529-9751-c882739ec624', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019498392710>]}
[0m21:03:16.347961 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m21:03:16.953122 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m21:03:16.953122 [debug] [MainThread]: Partial parsing: added file: saas_dbt_analytics://models\gold\schema.yml
[0m21:03:17.372070 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on
'gold_account_health_summary' in package 'saas_dbt_analytics'
(models\gold\schema.yml). Arguments to generic tests should be nested under the
`arguments` property.
[0m21:03:17.372070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '24339678-021d-4529-9751-c882739ec624', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194AC3AC730>]}
[0m21:03:17.696192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '24339678-021d-4529-9751-c882739ec624', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194AC44E350>]}
[0m21:03:18.160029 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m21:03:18.171446 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m21:03:18.218813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '24339678-021d-4529-9751-c882739ec624', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194AC423F10>]}
[0m21:03:18.218813 [info ] [MainThread]: Found 16 models, 48 data tests, 845 macros
[0m21:03:18.228412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24339678-021d-4529-9751-c882739ec624', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194AC389C60>]}
[0m21:03:18.239301 [info ] [MainThread]: 
[0m21:03:18.240743 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:03:18.241762 [info ] [MainThread]: 
[0m21:03:18.241762 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m21:03:18.241762 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:03:18.265993 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_analytics) - Creating connection
[0m21:03:18.265993 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_analytics'
[0m21:03:18.293147 [debug] [ThreadPool]: Using databricks connection "list_workspace_analytics"
[0m21:03:18.293147 [debug] [ThreadPool]: On list_workspace_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "connection_name": "list_workspace_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'analytics'

  
[0m21:03:18.293147 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:03:21.556772 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da8-56ba-163e-a16a-95b1dae82464) - Created
[0m21:03:22.295621 [debug] [ThreadPool]: SQL status: OK in 4.000 seconds
[0m21:03:22.312971 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f10da8-56ba-163e-a16a-95b1dae82464, command-id=01f10da8-56e8-1282-a429-c8cf2354bd61) - Closing
[0m21:03:22.314980 [debug] [ThreadPool]: On list_workspace_analytics: Close
[0m21:03:22.316991 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f10da8-56ba-163e-a16a-95b1dae82464) - Closing
[0m21:03:23.373593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24339678-021d-4529-9751-c882739ec624', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194AC445810>]}
[0m21:03:23.389397 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned.80d50c9920
[0m21:03:23.389397 [info ] [Thread-2 (]: 1 of 48 START test accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned  [RUN]
[0m21:03:23.389397 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned.80d50c9920) - Creating connection
[0m21:03:23.389397 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned.80d50c9920'
[0m21:03:23.389397 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned.80d50c9920
[0m21:03:23.424255 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned.80d50c9920"
[0m21:03:23.424255 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned.80d50c9920
[0m21:03:23.470475 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned.80d50c9920"
[0m21:03:23.473889 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned.80d50c9920"
[0m21:03:23.473889 [debug] [Thread-2 (]: On test.saas_dbt_analytics.accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned.80d50c9920: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned.80d50c9920"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        health_status as value_field,
        count(*) as n_records

    from `workspace`.`analytics`.`gold_account_health_summary`
    group by health_status

)

select *
from all_values
where value_field not in (
    'Healthy','At Risk','Critical','Churned'
)



  
  
      
    ) dbt_internal_test
[0m21:03:23.473889 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:03:27.031544 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-59f2-1a5f-8182-88d8940d7cca) - Created
[0m21:03:28.065657 [debug] [Thread-2 (]: SQL status: OK in 4.590 seconds
[0m21:03:28.073872 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-59f2-1a5f-8182-88d8940d7cca, command-id=01f10da8-5a28-12d0-92ba-761c80c96a8e) - Closing
[0m21:03:28.081439 [debug] [Thread-2 (]: On test.saas_dbt_analytics.accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned.80d50c9920: Close
[0m21:03:28.083550 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-59f2-1a5f-8182-88d8940d7cca) - Closing
[0m21:03:28.462192 [info ] [Thread-2 (]: 1 of 48 PASS accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned  [[32mPASS[0m in 5.07s]
[0m21:03:28.463638 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.accepted_values_gold_account_health_summary_health_status__Healthy__At_Risk__Critical__Churned.80d50c9920
[0m21:03:28.465663 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention.f483ddc22f
[0m21:03:28.466901 [info ] [Thread-2 (]: 2 of 48 START test accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention  [RUN]
[0m21:03:28.468922 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention.f483ddc22f) - Creating connection
[0m21:03:28.470462 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention.f483ddc22f'
[0m21:03:28.472822 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention.f483ddc22f
[0m21:03:28.489046 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention.f483ddc22f"
[0m21:03:28.492871 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention.f483ddc22f
[0m21:03:28.497712 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention.f483ddc22f"
[0m21:03:28.497712 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention.f483ddc22f"
[0m21:03:28.505704 [debug] [Thread-2 (]: On test.saas_dbt_analytics.accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention.f483ddc22f: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention.f483ddc22f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        customer_segment as value_field,
        count(*) as n_records

    from `workspace`.`analytics`.`gold_customer_segments`
    group by customer_segment

)

select *
from all_values
where value_field not in (
    'Champion','Loyal','Promising','New Customer','At Risk – Downgraded','Hibernating','Critical – Likely to Churn','Needs Attention'
)



  
  
      
    ) dbt_internal_test
[0m21:03:28.507730 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:03:28.511732 [debug] [Thread-2 (]: Databricks adapter: Default tags: Query tag value for key '@@dbt_model_name' exceeds 128 characters (173 chars). Truncating to 128 characters.
[0m21:03:32.066286 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-5cf1-163c-b620-c2e0d90ebaae) - Created
[0m21:03:33.589725 [debug] [Thread-2 (]: SQL status: OK in 5.080 seconds
[0m21:03:33.601839 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-5cf1-163c-b620-c2e0d90ebaae, command-id=01f10da8-5d28-1121-9ccb-f2b21e8a9001) - Closing
[0m21:03:33.601839 [debug] [Thread-2 (]: On test.saas_dbt_analytics.accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention.f483ddc22f: Close
[0m21:03:33.606190 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-5cf1-163c-b620-c2e0d90ebaae) - Closing
[0m21:03:34.007454 [info ] [Thread-2 (]: 2 of 48 PASS accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention  [[32mPASS[0m in 5.54s]
[0m21:03:34.010104 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.accepted_values_gold_customer_segments_customer_segment__Champion__Loyal__Promising__New_Customer__At_Risk_Downgraded__Hibernating__Critical_Likely_to_Churn__Needs_Attention.f483ddc22f
[0m21:03:34.012234 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m21:03:34.013242 [info ] [Thread-2 (]: 3 of 48 START test dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [RUN]
[0m21:03:34.015901 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77) - Creating connection
[0m21:03:34.017165 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77'
[0m21:03:34.018521 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m21:03:34.045197 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m21:03:34.046194 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m21:03:34.056591 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m21:03:34.059184 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"
[0m21:03:34.059184 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  





with validation_errors as (

    select
        subscription_id, usage_date, feature_name
    from `workspace`.`analytics`.`stg_feature_usage`
    group by subscription_id, usage_date, feature_name
    having count(*) > 1

)

select *
from validation_errors



  
  
      
    ) dbt_internal_test
[0m21:03:34.059184 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:03:37.534930 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-6034-144d-bbf7-51e70ba7cf66) - Created
[0m21:03:38.173464 [debug] [Thread-2 (]: SQL status: OK in 4.110 seconds
[0m21:03:38.186459 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-6034-144d-bbf7-51e70ba7cf66, command-id=01f10da8-606b-1d9f-9aaa-50358e43cbeb) - Closing
[0m21:03:38.186459 [debug] [Thread-2 (]: On test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77: Close
[0m21:03:38.186459 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-6034-144d-bbf7-51e70ba7cf66) - Closing
[0m21:03:38.561471 [info ] [Thread-2 (]: 3 of 48 PASS dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name  [[32mPASS[0m in 4.55s]
[0m21:03:38.564507 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.dbt_utils_unique_combination_of_columns_stg_feature_usage_subscription_id__usage_date__feature_name.ae79df3b77
[0m21:03:38.566208 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_gold_account_health_summary_account_id.365a4216ab
[0m21:03:38.568209 [info ] [Thread-2 (]: 4 of 48 START test not_null_gold_account_health_summary_account_id ............. [RUN]
[0m21:03:38.569394 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_gold_account_health_summary_account_id.365a4216ab) - Creating connection
[0m21:03:38.571410 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_gold_account_health_summary_account_id.365a4216ab'
[0m21:03:38.572602 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_gold_account_health_summary_account_id.365a4216ab
[0m21:03:38.593702 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_gold_account_health_summary_account_id.365a4216ab"
[0m21:03:38.595561 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_gold_account_health_summary_account_id.365a4216ab
[0m21:03:38.607364 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_gold_account_health_summary_account_id.365a4216ab"
[0m21:03:38.607364 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_gold_account_health_summary_account_id.365a4216ab"
[0m21:03:38.607364 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_account_health_summary_account_id.365a4216ab: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_gold_account_health_summary_account_id.365a4216ab"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`gold_account_health_summary`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m21:03:38.607364 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:03:42.507027 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-632f-1d64-9a8f-b25f3d5d06b7) - Created
[0m21:03:43.321136 [debug] [Thread-2 (]: SQL status: OK in 4.710 seconds
[0m21:03:43.324845 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-632f-1d64-9a8f-b25f3d5d06b7, command-id=01f10da8-6363-157a-be23-c67e9a134e6f) - Closing
[0m21:03:43.326853 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_account_health_summary_account_id.365a4216ab: Close
[0m21:03:43.326853 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-632f-1d64-9a8f-b25f3d5d06b7) - Closing
[0m21:03:43.635018 [info ] [Thread-2 (]: 4 of 48 PASS not_null_gold_account_health_summary_account_id ................... [[32mPASS[0m in 5.07s]
[0m21:03:43.635018 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_gold_account_health_summary_account_id.365a4216ab
[0m21:03:43.639062 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_gold_account_health_summary_health_status.bf10b1f150
[0m21:03:43.639062 [info ] [Thread-2 (]: 5 of 48 START test not_null_gold_account_health_summary_health_status .......... [RUN]
[0m21:03:43.643630 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_gold_account_health_summary_health_status.bf10b1f150) - Creating connection
[0m21:03:43.647099 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_gold_account_health_summary_health_status.bf10b1f150'
[0m21:03:43.648326 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_gold_account_health_summary_health_status.bf10b1f150
[0m21:03:43.662172 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_gold_account_health_summary_health_status.bf10b1f150"
[0m21:03:43.671450 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_gold_account_health_summary_health_status.bf10b1f150
[0m21:03:43.676631 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_gold_account_health_summary_health_status.bf10b1f150"
[0m21:03:43.676631 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_gold_account_health_summary_health_status.bf10b1f150"
[0m21:03:43.676631 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_account_health_summary_health_status.bf10b1f150: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_gold_account_health_summary_health_status.bf10b1f150"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select health_status
from `workspace`.`analytics`.`gold_account_health_summary`
where health_status is null



  
  
      
    ) dbt_internal_test
[0m21:03:43.676631 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:03:47.035656 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-65ed-163a-84b0-03334f83c611) - Created
[0m21:03:47.741713 [debug] [Thread-2 (]: SQL status: OK in 4.070 seconds
[0m21:03:47.747531 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-65ed-163a-84b0-03334f83c611, command-id=01f10da8-6616-1817-9981-988fb1e24a56) - Closing
[0m21:03:47.747531 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_account_health_summary_health_status.bf10b1f150: Close
[0m21:03:47.747531 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-65ed-163a-84b0-03334f83c611) - Closing
[0m21:03:48.086349 [info ] [Thread-2 (]: 5 of 48 PASS not_null_gold_account_health_summary_health_status ................ [[32mPASS[0m in 4.45s]
[0m21:03:48.086349 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_gold_account_health_summary_health_status.bf10b1f150
[0m21:03:48.086349 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_gold_churn_analysis_churn_event_id.2b0234b22f
[0m21:03:48.086349 [info ] [Thread-2 (]: 6 of 48 START test not_null_gold_churn_analysis_churn_event_id ................. [RUN]
[0m21:03:48.101508 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_gold_churn_analysis_churn_event_id.2b0234b22f) - Creating connection
[0m21:03:48.103102 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_gold_churn_analysis_churn_event_id.2b0234b22f'
[0m21:03:48.105360 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_gold_churn_analysis_churn_event_id.2b0234b22f
[0m21:03:48.121426 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_gold_churn_analysis_churn_event_id.2b0234b22f"
[0m21:03:48.123732 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_gold_churn_analysis_churn_event_id.2b0234b22f
[0m21:03:48.127023 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_gold_churn_analysis_churn_event_id.2b0234b22f"
[0m21:03:48.137659 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_gold_churn_analysis_churn_event_id.2b0234b22f"
[0m21:03:48.138128 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_churn_analysis_churn_event_id.2b0234b22f: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_gold_churn_analysis_churn_event_id.2b0234b22f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select churn_event_id
from `workspace`.`analytics`.`gold_churn_analysis`
where churn_event_id is null



  
  
      
    ) dbt_internal_test
[0m21:03:48.138128 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:03:51.964628 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-68cf-11f7-93ae-9222287bcf51) - Created
[0m21:03:52.839786 [debug] [Thread-2 (]: SQL status: OK in 4.700 seconds
[0m21:03:52.843880 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-68cf-11f7-93ae-9222287bcf51, command-id=01f10da8-6905-1c1f-959b-bd94dae93d40) - Closing
[0m21:03:52.850379 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_churn_analysis_churn_event_id.2b0234b22f: Close
[0m21:03:52.851390 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-68cf-11f7-93ae-9222287bcf51) - Closing
[0m21:03:53.231153 [info ] [Thread-2 (]: 6 of 48 PASS not_null_gold_churn_analysis_churn_event_id ....................... [[32mPASS[0m in 5.14s]
[0m21:03:53.231153 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_gold_churn_analysis_churn_event_id.2b0234b22f
[0m21:03:53.231153 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_gold_churn_analysis_mrr_lost.8a675daa7b
[0m21:03:53.231153 [info ] [Thread-2 (]: 7 of 48 START test not_null_gold_churn_analysis_mrr_lost ....................... [RUN]
[0m21:03:53.241250 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_gold_churn_analysis_mrr_lost.8a675daa7b) - Creating connection
[0m21:03:53.243317 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_gold_churn_analysis_mrr_lost.8a675daa7b'
[0m21:03:53.244991 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_gold_churn_analysis_mrr_lost.8a675daa7b
[0m21:03:53.259430 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_gold_churn_analysis_mrr_lost.8a675daa7b"
[0m21:03:53.259430 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_gold_churn_analysis_mrr_lost.8a675daa7b
[0m21:03:53.270428 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_gold_churn_analysis_mrr_lost.8a675daa7b"
[0m21:03:53.270428 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_gold_churn_analysis_mrr_lost.8a675daa7b"
[0m21:03:53.270428 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_churn_analysis_mrr_lost.8a675daa7b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_gold_churn_analysis_mrr_lost.8a675daa7b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select mrr_lost
from `workspace`.`analytics`.`gold_churn_analysis`
where mrr_lost is null



  
  
      
    ) dbt_internal_test
[0m21:03:53.270428 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:03:56.706905 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-6ba6-1e8e-9aef-c24e4fab348c) - Created
[0m21:03:57.397771 [debug] [Thread-2 (]: SQL status: OK in 4.130 seconds
[0m21:03:57.402572 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-6ba6-1e8e-9aef-c24e4fab348c, command-id=01f10da8-6bd8-13fd-b5f3-6768dee73796) - Closing
[0m21:03:57.402572 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_churn_analysis_mrr_lost.8a675daa7b: Close
[0m21:03:57.402572 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-6ba6-1e8e-9aef-c24e4fab348c) - Closing
[0m21:03:57.703706 [info ] [Thread-2 (]: 7 of 48 PASS not_null_gold_churn_analysis_mrr_lost ............................. [[32mPASS[0m in 4.47s]
[0m21:03:57.707733 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_gold_churn_analysis_mrr_lost.8a675daa7b
[0m21:03:57.707733 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_gold_customer_segments_account_id.ff3ba25b2e
[0m21:03:57.710733 [info ] [Thread-2 (]: 8 of 48 START test not_null_gold_customer_segments_account_id .................. [RUN]
[0m21:03:57.713148 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_gold_customer_segments_account_id.ff3ba25b2e) - Creating connection
[0m21:03:57.715113 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_gold_customer_segments_account_id.ff3ba25b2e'
[0m21:03:57.716182 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_gold_customer_segments_account_id.ff3ba25b2e
[0m21:03:57.719496 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_gold_customer_segments_account_id.ff3ba25b2e"
[0m21:03:57.719496 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_gold_customer_segments_account_id.ff3ba25b2e
[0m21:03:57.740236 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_gold_customer_segments_account_id.ff3ba25b2e"
[0m21:03:57.741239 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_gold_customer_segments_account_id.ff3ba25b2e"
[0m21:03:57.741239 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_customer_segments_account_id.ff3ba25b2e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_gold_customer_segments_account_id.ff3ba25b2e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`gold_customer_segments`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m21:03:57.741239 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:04:01.260603 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-6e5c-1e85-807e-6acf3e6288f2) - Created
[0m21:04:01.945336 [debug] [Thread-2 (]: SQL status: OK in 4.200 seconds
[0m21:04:01.945336 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-6e5c-1e85-807e-6acf3e6288f2, command-id=01f10da8-6e94-15fb-947b-791dd3e451c3) - Closing
[0m21:04:01.945336 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_customer_segments_account_id.ff3ba25b2e: Close
[0m21:04:01.961449 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-6e5c-1e85-807e-6acf3e6288f2) - Closing
[0m21:04:02.308974 [info ] [Thread-2 (]: 8 of 48 PASS not_null_gold_customer_segments_account_id ........................ [[32mPASS[0m in 4.60s]
[0m21:04:02.315965 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_gold_customer_segments_account_id.ff3ba25b2e
[0m21:04:02.318992 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_gold_customer_segments_customer_segment.216c240b6c
[0m21:04:02.318992 [info ] [Thread-2 (]: 9 of 48 START test not_null_gold_customer_segments_customer_segment ............ [RUN]
[0m21:04:02.322922 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_gold_customer_segments_customer_segment.216c240b6c) - Creating connection
[0m21:04:02.325919 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_gold_customer_segments_customer_segment.216c240b6c'
[0m21:04:02.326917 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_gold_customer_segments_customer_segment.216c240b6c
[0m21:04:02.341708 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_gold_customer_segments_customer_segment.216c240b6c"
[0m21:04:02.341708 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_gold_customer_segments_customer_segment.216c240b6c
[0m21:04:02.352945 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_gold_customer_segments_customer_segment.216c240b6c"
[0m21:04:02.352945 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_gold_customer_segments_customer_segment.216c240b6c"
[0m21:04:02.352945 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_customer_segments_customer_segment.216c240b6c: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_gold_customer_segments_customer_segment.216c240b6c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_segment
from `workspace`.`analytics`.`gold_customer_segments`
where customer_segment is null



  
  
      
    ) dbt_internal_test
[0m21:04:02.352945 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:04:05.529513 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-70f1-1613-8a30-9e55a2a2d694) - Created
[0m21:04:06.131972 [debug] [Thread-2 (]: SQL status: OK in 3.780 seconds
[0m21:04:06.131972 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-70f1-1613-8a30-9e55a2a2d694, command-id=01f10da8-711a-14b7-a7ec-d8d9a44325c8) - Closing
[0m21:04:06.131972 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_customer_segments_customer_segment.216c240b6c: Close
[0m21:04:06.131972 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-70f1-1613-8a30-9e55a2a2d694) - Closing
[0m21:04:06.432617 [info ] [Thread-2 (]: 9 of 48 PASS not_null_gold_customer_segments_customer_segment .................. [[32mPASS[0m in 4.11s]
[0m21:04:06.432617 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_gold_customer_segments_customer_segment.216c240b6c
[0m21:04:06.432617 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_gold_feature_adoption_feature_name.49483c8562
[0m21:04:06.449348 [info ] [Thread-2 (]: 10 of 48 START test not_null_gold_feature_adoption_feature_name ................ [RUN]
[0m21:04:06.451555 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_gold_feature_adoption_feature_name.49483c8562) - Creating connection
[0m21:04:06.452809 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_gold_feature_adoption_feature_name.49483c8562'
[0m21:04:06.454824 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_gold_feature_adoption_feature_name.49483c8562
[0m21:04:06.463474 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_gold_feature_adoption_feature_name.49483c8562"
[0m21:04:06.469199 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_gold_feature_adoption_feature_name.49483c8562
[0m21:04:06.474663 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_gold_feature_adoption_feature_name.49483c8562"
[0m21:04:06.480369 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_gold_feature_adoption_feature_name.49483c8562"
[0m21:04:06.480369 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_feature_adoption_feature_name.49483c8562: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_gold_feature_adoption_feature_name.49483c8562"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select feature_name
from `workspace`.`analytics`.`gold_feature_adoption`
where feature_name is null



  
  
      
    ) dbt_internal_test
[0m21:04:06.483999 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:04:11.039997 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-73c5-1e99-b38c-e9d2831c93d1) - Created
[0m21:04:11.710094 [debug] [Thread-2 (]: SQL status: OK in 5.230 seconds
[0m21:04:11.716646 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-73c5-1e99-b38c-e9d2831c93d1, command-id=01f10da8-7464-15d1-9127-5f9c19efb2d9) - Closing
[0m21:04:11.716646 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_feature_adoption_feature_name.49483c8562: Close
[0m21:04:11.716646 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-73c5-1e99-b38c-e9d2831c93d1) - Closing
[0m21:04:12.054052 [info ] [Thread-2 (]: 10 of 48 PASS not_null_gold_feature_adoption_feature_name ...................... [[32mPASS[0m in 5.60s]
[0m21:04:12.060195 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_gold_feature_adoption_feature_name.49483c8562
[0m21:04:12.060195 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_gold_feature_adoption_usage_month.383ace688b
[0m21:04:12.060195 [info ] [Thread-2 (]: 11 of 48 START test not_null_gold_feature_adoption_usage_month ................. [RUN]
[0m21:04:12.066126 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_gold_feature_adoption_usage_month.383ace688b) - Creating connection
[0m21:04:12.067900 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_gold_feature_adoption_usage_month.383ace688b'
[0m21:04:12.068926 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_gold_feature_adoption_usage_month.383ace688b
[0m21:04:12.074550 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_gold_feature_adoption_usage_month.383ace688b"
[0m21:04:12.081305 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_gold_feature_adoption_usage_month.383ace688b
[0m21:04:12.087725 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_gold_feature_adoption_usage_month.383ace688b"
[0m21:04:12.087725 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_gold_feature_adoption_usage_month.383ace688b"
[0m21:04:12.087725 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_feature_adoption_usage_month.383ace688b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_gold_feature_adoption_usage_month.383ace688b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select usage_month
from `workspace`.`analytics`.`gold_feature_adoption`
where usage_month is null



  
  
      
    ) dbt_internal_test
[0m21:04:12.087725 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:04:15.353468 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-76c7-173c-b07e-75fb6eb4b589) - Created
[0m21:04:15.992899 [debug] [Thread-2 (]: SQL status: OK in 3.910 seconds
[0m21:04:15.997080 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-76c7-173c-b07e-75fb6eb4b589, command-id=01f10da8-76f6-136f-9fad-d731a803be7e) - Closing
[0m21:04:15.999840 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_feature_adoption_usage_month.383ace688b: Close
[0m21:04:16.001058 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-76c7-173c-b07e-75fb6eb4b589) - Closing
[0m21:04:16.935977 [info ] [Thread-2 (]: 11 of 48 PASS not_null_gold_feature_adoption_usage_month ....................... [[32mPASS[0m in 4.88s]
[0m21:04:16.944789 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_gold_feature_adoption_usage_month.383ace688b
[0m21:04:16.944789 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_gold_mrr_trends_month_start.3d9b7f7f5d
[0m21:04:16.944789 [info ] [Thread-2 (]: 12 of 48 START test not_null_gold_mrr_trends_month_start ....................... [RUN]
[0m21:04:16.950175 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_gold_mrr_trends_month_start.3d9b7f7f5d) - Creating connection
[0m21:04:16.952895 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_gold_mrr_trends_month_start.3d9b7f7f5d'
[0m21:04:16.954415 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_gold_mrr_trends_month_start.3d9b7f7f5d
[0m21:04:16.966692 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_gold_mrr_trends_month_start.3d9b7f7f5d"
[0m21:04:16.969820 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_gold_mrr_trends_month_start.3d9b7f7f5d
[0m21:04:16.973262 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_gold_mrr_trends_month_start.3d9b7f7f5d"
[0m21:04:16.973262 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_gold_mrr_trends_month_start.3d9b7f7f5d"
[0m21:04:16.973262 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_mrr_trends_month_start.3d9b7f7f5d: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_gold_mrr_trends_month_start.3d9b7f7f5d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select month_start
from `workspace`.`analytics`.`gold_mrr_trends`
where month_start is null



  
  
      
    ) dbt_internal_test
[0m21:04:16.983344 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:04:20.656424 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-79e4-15a0-b44d-1b1de4cd1f53) - Created
[0m21:04:21.511590 [debug] [Thread-2 (]: SQL status: OK in 4.530 seconds
[0m21:04:21.515844 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-79e4-15a0-b44d-1b1de4cd1f53, command-id=01f10da8-7a20-151f-b03d-85b1afec0007) - Closing
[0m21:04:21.518917 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_mrr_trends_month_start.3d9b7f7f5d: Close
[0m21:04:21.518917 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-79e4-15a0-b44d-1b1de4cd1f53) - Closing
[0m21:04:21.959090 [info ] [Thread-2 (]: 12 of 48 PASS not_null_gold_mrr_trends_month_start ............................. [[32mPASS[0m in 5.01s]
[0m21:04:21.962377 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_gold_mrr_trends_month_start.3d9b7f7f5d
[0m21:04:21.962377 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_gold_support_performance_priority.07e1b8449c
[0m21:04:21.962377 [info ] [Thread-2 (]: 13 of 48 START test not_null_gold_support_performance_priority ................. [RUN]
[0m21:04:21.962377 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_gold_support_performance_priority.07e1b8449c) - Creating connection
[0m21:04:21.962377 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_gold_support_performance_priority.07e1b8449c'
[0m21:04:21.970675 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_gold_support_performance_priority.07e1b8449c
[0m21:04:21.983364 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_gold_support_performance_priority.07e1b8449c"
[0m21:04:21.986530 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_gold_support_performance_priority.07e1b8449c
[0m21:04:21.994679 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_gold_support_performance_priority.07e1b8449c"
[0m21:04:21.994679 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_gold_support_performance_priority.07e1b8449c"
[0m21:04:21.998297 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_support_performance_priority.07e1b8449c: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_gold_support_performance_priority.07e1b8449c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select priority
from `workspace`.`analytics`.`gold_support_performance`
where priority is null



  
  
      
    ) dbt_internal_test
[0m21:04:22.000302 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:04:25.766669 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-7ceb-1b33-a028-1da6fdc70792) - Created
[0m21:04:26.581525 [debug] [Thread-2 (]: SQL status: OK in 4.580 seconds
[0m21:04:26.591903 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-7ceb-1b33-a028-1da6fdc70792, command-id=01f10da8-7d2c-1886-96fc-7be34385569f) - Closing
[0m21:04:26.591903 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_support_performance_priority.07e1b8449c: Close
[0m21:04:26.591903 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-7ceb-1b33-a028-1da6fdc70792) - Closing
[0m21:04:27.047858 [info ] [Thread-2 (]: 13 of 48 PASS not_null_gold_support_performance_priority ....................... [[32mPASS[0m in 5.08s]
[0m21:04:27.048866 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_gold_support_performance_priority.07e1b8449c
[0m21:04:27.050703 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_gold_support_performance_ticket_month.31a60876db
[0m21:04:27.050703 [info ] [Thread-2 (]: 14 of 48 START test not_null_gold_support_performance_ticket_month ............. [RUN]
[0m21:04:27.055767 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_gold_support_performance_ticket_month.31a60876db) - Creating connection
[0m21:04:27.058551 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_gold_support_performance_ticket_month.31a60876db'
[0m21:04:27.060636 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_gold_support_performance_ticket_month.31a60876db
[0m21:04:27.071265 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_gold_support_performance_ticket_month.31a60876db"
[0m21:04:27.071265 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_gold_support_performance_ticket_month.31a60876db
[0m21:04:27.082688 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_gold_support_performance_ticket_month.31a60876db"
[0m21:04:27.087350 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_gold_support_performance_ticket_month.31a60876db"
[0m21:04:27.087350 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_support_performance_ticket_month.31a60876db: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_gold_support_performance_ticket_month.31a60876db"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_month
from `workspace`.`analytics`.`gold_support_performance`
where ticket_month is null



  
  
      
    ) dbt_internal_test
[0m21:04:27.087350 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:04:34.810265 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-81d7-1553-9593-6883d279a644) - Created
[0m21:04:35.602242 [debug] [Thread-2 (]: SQL status: OK in 8.510 seconds
[0m21:04:35.604938 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-81d7-1553-9593-6883d279a644, command-id=01f10da8-8291-16f9-a9d0-3a2f21d31111) - Closing
[0m21:04:35.610980 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_gold_support_performance_ticket_month.31a60876db: Close
[0m21:04:35.610980 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-81d7-1553-9593-6883d279a644) - Closing
[0m21:04:36.836809 [info ] [Thread-2 (]: 14 of 48 PASS not_null_gold_support_performance_ticket_month ................... [[32mPASS[0m in 9.78s]
[0m21:04:36.836809 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_gold_support_performance_ticket_month.31a60876db
[0m21:04:36.836809 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73
[0m21:04:36.836809 [info ] [Thread-2 (]: 15 of 48 START test not_null_int_account_360_account_id ........................ [RUN]
[0m21:04:36.843933 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73) - Creating connection
[0m21:04:36.845204 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73'
[0m21:04:36.847327 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73
[0m21:04:36.857510 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73"
[0m21:04:36.857510 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73
[0m21:04:36.870367 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73"
[0m21:04:36.870367 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73"
[0m21:04:36.874416 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`int_account_360`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m21:04:36.874416 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:04:42.194925 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-862a-1617-8fa0-d36dab3b7e59) - Created
[0m21:04:42.888348 [debug] [Thread-2 (]: SQL status: OK in 6.010 seconds
[0m21:04:42.888348 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-862a-1617-8fa0-d36dab3b7e59, command-id=01f10da8-86f5-151c-a8ae-e76db730082c) - Closing
[0m21:04:42.888348 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73: Close
[0m21:04:42.888348 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-862a-1617-8fa0-d36dab3b7e59) - Closing
[0m21:04:43.305446 [info ] [Thread-2 (]: 15 of 48 PASS not_null_int_account_360_account_id .............................. [[32mPASS[0m in 6.47s]
[0m21:04:43.305446 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_360_account_id.96be6ebb73
[0m21:04:43.305446 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a
[0m21:04:43.311878 [info ] [Thread-2 (]: 16 of 48 START test not_null_int_account_360_health_score ...................... [RUN]
[0m21:04:43.315255 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a) - Creating connection
[0m21:04:43.317061 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a'
[0m21:04:43.318598 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a
[0m21:04:43.330826 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a"
[0m21:04:43.333348 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a
[0m21:04:43.338712 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a"
[0m21:04:43.338712 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a"
[0m21:04:43.345344 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select health_score
from `workspace`.`analytics`.`int_account_360`
where health_score is null



  
  
      
    ) dbt_internal_test
[0m21:04:43.346798 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:04:47.438517 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-89d7-12bf-beba-b73b5a12d8d6) - Created
[0m21:04:49.632363 [debug] [Thread-2 (]: SQL status: OK in 6.290 seconds
[0m21:04:49.639395 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-89d7-12bf-beba-b73b5a12d8d6, command-id=01f10da8-8a15-163d-b09a-deead2b26785) - Closing
[0m21:04:49.639395 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a: Close
[0m21:04:49.639395 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-89d7-12bf-beba-b73b5a12d8d6) - Closing
[0m21:04:50.072069 [info ] [Thread-2 (]: 16 of 48 PASS not_null_int_account_360_health_score ............................ [[32mPASS[0m in 6.76s]
[0m21:04:50.072069 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_360_health_score.0c834daf0a
[0m21:04:50.078845 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb
[0m21:04:50.079850 [info ] [Thread-2 (]: 17 of 48 START test not_null_int_account_churn_account_id ...................... [RUN]
[0m21:04:50.082020 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb) - Creating connection
[0m21:04:50.083992 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb'
[0m21:04:50.085635 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb
[0m21:04:50.099041 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb"
[0m21:04:50.100381 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb
[0m21:04:50.102812 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb"
[0m21:04:50.111557 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb"
[0m21:04:50.113288 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`int_account_churn`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m21:04:50.113288 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:04:53.520434 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-8d83-1a6a-b657-d06f1ac25697) - Created
[0m21:04:54.239229 [debug] [Thread-2 (]: SQL status: OK in 4.130 seconds
[0m21:04:54.239229 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-8d83-1a6a-b657-d06f1ac25697, command-id=01f10da8-8db6-19c0-8c22-770c88a4ad9a) - Closing
[0m21:04:54.239229 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb: Close
[0m21:04:54.239229 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-8d83-1a6a-b657-d06f1ac25697) - Closing
[0m21:04:54.614609 [info ] [Thread-2 (]: 17 of 48 PASS not_null_int_account_churn_account_id ............................ [[32mPASS[0m in 4.53s]
[0m21:04:54.627913 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_churn_account_id.fc70afe3bb
[0m21:04:54.629923 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c
[0m21:04:54.629923 [info ] [Thread-2 (]: 18 of 48 START test not_null_int_account_churn_churn_event_id .................. [RUN]
[0m21:04:54.634298 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c) - Creating connection
[0m21:04:54.635926 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c'
[0m21:04:54.637495 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c
[0m21:04:54.650111 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c"
[0m21:04:54.651112 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c
[0m21:04:54.661587 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c"
[0m21:04:54.662191 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c"
[0m21:04:54.662191 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select churn_event_id
from `workspace`.`analytics`.`int_account_churn`
where churn_event_id is null



  
  
      
    ) dbt_internal_test
[0m21:04:54.662191 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:04:57.972703 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-9031-1109-9511-5a096e20203c) - Created
[0m21:04:58.577691 [debug] [Thread-2 (]: SQL status: OK in 3.920 seconds
[0m21:04:58.580512 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-9031-1109-9511-5a096e20203c, command-id=01f10da8-905e-1213-9d13-7662b9ac3c24) - Closing
[0m21:04:58.580512 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c: Close
[0m21:04:58.580512 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-9031-1109-9511-5a096e20203c) - Closing
[0m21:04:58.870008 [info ] [Thread-2 (]: 18 of 48 PASS not_null_int_account_churn_churn_event_id ........................ [[32mPASS[0m in 4.24s]
[0m21:04:58.876289 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_churn_churn_event_id.dbfaf83a2c
[0m21:04:58.876289 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71
[0m21:04:58.876289 [info ] [Thread-2 (]: 19 of 48 START test not_null_int_account_feature_usage_account_id .............. [RUN]
[0m21:04:58.881397 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71) - Creating connection
[0m21:04:58.883397 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71'
[0m21:04:58.885097 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71
[0m21:04:58.898126 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71"
[0m21:04:58.900304 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71
[0m21:04:58.901327 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71"
[0m21:04:58.911937 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71"
[0m21:04:58.911937 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`int_account_feature_usage`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m21:04:58.911937 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:05:02.371130 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-92c8-1231-81e1-9b2439a51254) - Created
[0m21:05:03.032273 [debug] [Thread-2 (]: SQL status: OK in 4.120 seconds
[0m21:05:03.045476 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-92c8-1231-81e1-9b2439a51254, command-id=01f10da8-92fd-113e-a54d-93ee4302c56e) - Closing
[0m21:05:03.045476 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71: Close
[0m21:05:03.047998 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-92c8-1231-81e1-9b2439a51254) - Closing
[0m21:05:03.336609 [info ] [Thread-2 (]: 19 of 48 PASS not_null_int_account_feature_usage_account_id .................... [[32mPASS[0m in 4.46s]
[0m21:05:03.338736 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_feature_usage_account_id.f1aeaafc71
[0m21:05:03.341113 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5
[0m21:05:03.342541 [info ] [Thread-2 (]: 20 of 48 START test not_null_int_account_subscriptions_account_id .............. [RUN]
[0m21:05:03.344358 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5) - Creating connection
[0m21:05:03.345375 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5'
[0m21:05:03.347376 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5
[0m21:05:03.356140 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5"
[0m21:05:03.360596 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5
[0m21:05:03.370965 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5"
[0m21:05:03.372292 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5"
[0m21:05:03.372292 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`int_account_subscriptions`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m21:05:03.375904 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:05:07.136032 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-9595-159a-8c7b-8f396e1aec63) - Created
[0m21:05:07.770507 [debug] [Thread-2 (]: SQL status: OK in 4.400 seconds
[0m21:05:07.770507 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-9595-159a-8c7b-8f396e1aec63, command-id=01f10da8-95d5-1452-869a-62582faa550b) - Closing
[0m21:05:07.770507 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5: Close
[0m21:05:07.770507 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-9595-159a-8c7b-8f396e1aec63) - Closing
[0m21:05:08.204537 [info ] [Thread-2 (]: 20 of 48 PASS not_null_int_account_subscriptions_account_id .................... [[32mPASS[0m in 4.86s]
[0m21:05:08.220197 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_subscriptions_account_id.2d9109b4f5
[0m21:05:08.220197 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71
[0m21:05:08.225031 [info ] [Thread-2 (]: 21 of 48 START test not_null_int_account_subscriptions_subscription_id ......... [RUN]
[0m21:05:08.227102 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71) - Creating connection
[0m21:05:08.228896 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71'
[0m21:05:08.230428 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71
[0m21:05:08.232498 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71"
[0m21:05:08.244362 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71
[0m21:05:08.249477 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71"
[0m21:05:08.249477 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71"
[0m21:05:08.249477 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`int_account_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m21:05:08.249477 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:05:11.402398 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-9834-17ec-a6fd-689899c1c451) - Created
[0m21:05:12.011479 [debug] [Thread-2 (]: SQL status: OK in 3.750 seconds
[0m21:05:12.015170 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-9834-17ec-a6fd-689899c1c451, command-id=01f10da8-985f-1ad3-958e-58971f1355cd) - Closing
[0m21:05:12.015170 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71: Close
[0m21:05:12.015170 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-9834-17ec-a6fd-689899c1c451) - Closing
[0m21:05:13.682908 [info ] [Thread-2 (]: 21 of 48 PASS not_null_int_account_subscriptions_subscription_id ............... [[32mPASS[0m in 5.46s]
[0m21:05:13.696603 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_subscriptions_subscription_id.a243e8ee71
[0m21:05:13.696603 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59
[0m21:05:13.698612 [info ] [Thread-2 (]: 22 of 48 START test not_null_int_account_support_account_id .................... [RUN]
[0m21:05:13.700618 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59) - Creating connection
[0m21:05:13.700618 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59'
[0m21:05:13.700618 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59
[0m21:05:13.722626 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59"
[0m21:05:13.725035 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59
[0m21:05:13.737735 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59"
[0m21:05:13.741102 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59"
[0m21:05:13.743622 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`int_account_support`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m21:05:13.746105 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:05:17.512925 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-9bd6-1058-ad6f-5482cb0d86d3) - Created
[0m21:05:18.044963 [debug] [Thread-2 (]: SQL status: OK in 4.300 seconds
[0m21:05:18.050646 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-9bd6-1058-ad6f-5482cb0d86d3, command-id=01f10da8-9c09-142f-8f1a-32825a28391f) - Closing
[0m21:05:18.055413 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59: Close
[0m21:05:18.055413 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-9bd6-1058-ad6f-5482cb0d86d3) - Closing
[0m21:05:18.370895 [info ] [Thread-2 (]: 22 of 48 PASS not_null_int_account_support_account_id .......................... [[32mPASS[0m in 4.67s]
[0m21:05:18.370895 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_int_account_support_account_id.ece26cdb59
[0m21:05:18.376249 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m21:05:18.376249 [info ] [Thread-2 (]: 23 of 48 START test not_null_stg_accounts_account_id ........................... [RUN]
[0m21:05:18.376249 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108) - Creating connection
[0m21:05:18.382622 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108'
[0m21:05:18.384777 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m21:05:18.392844 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m21:05:18.392844 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m21:05:18.410011 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m21:05:18.410011 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"
[0m21:05:18.410011 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_accounts`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m21:05:18.410011 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:05:22.075384 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-9e8f-1637-a564-58886e1acb18) - Created
[0m21:05:22.573524 [debug] [Thread-2 (]: SQL status: OK in 4.160 seconds
[0m21:05:22.578383 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-9e8f-1637-a564-58886e1acb18, command-id=01f10da8-9eba-1b6d-87f4-106f3ca72258) - Closing
[0m21:05:22.581567 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108: Close
[0m21:05:22.581567 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-9e8f-1637-a564-58886e1acb18) - Closing
[0m21:05:24.224301 [info ] [Thread-2 (]: 23 of 48 PASS not_null_stg_accounts_account_id ................................. [[32mPASS[0m in 5.85s]
[0m21:05:24.228360 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_account_id.182dfbc108
[0m21:05:24.230367 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m21:05:24.232377 [info ] [Thread-2 (]: 24 of 48 START test not_null_stg_accounts_seats ................................ [RUN]
[0m21:05:24.232377 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39) - Creating connection
[0m21:05:24.236300 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39'
[0m21:05:24.236300 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m21:05:24.256064 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m21:05:24.260100 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m21:05:24.273168 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m21:05:24.276221 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"
[0m21:05:24.276221 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select seats
from `workspace`.`analytics`.`stg_accounts`
where seats is null



  
  
      
    ) dbt_internal_test
[0m21:05:24.276221 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:05:29.076298 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-a2b9-1ead-9037-f7a621c6fb27) - Created
[0m21:05:29.583438 [debug] [Thread-2 (]: SQL status: OK in 5.310 seconds
[0m21:05:29.583438 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-a2b9-1ead-9037-f7a621c6fb27, command-id=01f10da8-a2e7-1944-9cbd-bd8cf19ec1ed) - Closing
[0m21:05:29.583438 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39: Close
[0m21:05:29.583438 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-a2b9-1ead-9037-f7a621c6fb27) - Closing
[0m21:05:29.900199 [info ] [Thread-2 (]: 24 of 48 PASS not_null_stg_accounts_seats ...................................... [[32mPASS[0m in 5.67s]
[0m21:05:29.913642 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_seats.24d90a9e39
[0m21:05:29.913642 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m21:05:29.916170 [info ] [Thread-2 (]: 25 of 48 START test not_null_stg_accounts_signup_date .......................... [RUN]
[0m21:05:29.916170 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a) - Creating connection
[0m21:05:29.916170 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a'
[0m21:05:29.916170 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m21:05:29.939597 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m21:05:29.943122 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m21:05:29.955552 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m21:05:29.958521 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"
[0m21:05:29.961441 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signup_date
from `workspace`.`analytics`.`stg_accounts`
where signup_date is null



  
  
      
    ) dbt_internal_test
[0m21:05:29.963448 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:05:34.342949 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-a5ce-1199-ab4d-7f1f5b500595) - Created
[0m21:05:34.899627 [debug] [Thread-2 (]: SQL status: OK in 4.940 seconds
[0m21:05:34.904650 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-a5ce-1199-ab4d-7f1f5b500595, command-id=01f10da8-a60b-1443-994a-71824b597727) - Closing
[0m21:05:34.904650 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a: Close
[0m21:05:34.904650 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-a5ce-1199-ab4d-7f1f5b500595) - Closing
[0m21:05:35.337638 [info ] [Thread-2 (]: 25 of 48 PASS not_null_stg_accounts_signup_date ................................ [[32mPASS[0m in 5.42s]
[0m21:05:35.339650 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_accounts_signup_date.e63ee5944a
[0m21:05:35.342366 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m21:05:35.342366 [info ] [Thread-2 (]: 26 of 48 START test not_null_stg_churn_events_account_id ....................... [RUN]
[0m21:05:35.347128 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d) - Creating connection
[0m21:05:35.349539 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d'
[0m21:05:35.350629 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m21:05:35.362135 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m21:05:35.362135 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m21:05:35.372914 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m21:05:35.372914 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"
[0m21:05:35.372914 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_churn_events`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m21:05:35.383856 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:05:39.046479 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-a8ac-1eba-a490-dfdd71138702) - Created
[0m21:05:39.502230 [debug] [Thread-2 (]: SQL status: OK in 4.130 seconds
[0m21:05:39.508258 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-a8ac-1eba-a490-dfdd71138702, command-id=01f10da8-a8da-114b-a270-b820d025f1fa) - Closing
[0m21:05:39.509267 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d: Close
[0m21:05:39.511275 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-a8ac-1eba-a490-dfdd71138702) - Closing
[0m21:05:39.833666 [info ] [Thread-2 (]: 26 of 48 PASS not_null_stg_churn_events_account_id ............................. [[32mPASS[0m in 4.49s]
[0m21:05:39.835675 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_account_id.65a8c1652d
[0m21:05:39.840333 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m21:05:39.842421 [info ] [Thread-2 (]: 27 of 48 START test not_null_stg_churn_events_churn_event_id ................... [RUN]
[0m21:05:39.842421 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc) - Creating connection
[0m21:05:39.846531 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc'
[0m21:05:39.846531 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m21:05:39.865139 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m21:05:39.868753 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m21:05:39.880072 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m21:05:39.884090 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"
[0m21:05:39.884090 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select churn_event_id
from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is null



  
  
      
    ) dbt_internal_test
[0m21:05:39.887491 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:05:43.824484 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-ab81-1b6d-b0e9-758712b98173) - Created
[0m21:05:44.332871 [debug] [Thread-2 (]: SQL status: OK in 4.450 seconds
[0m21:05:44.342028 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-ab81-1b6d-b0e9-758712b98173, command-id=01f10da8-abb2-1513-b2cd-9dad9c263ef1) - Closing
[0m21:05:44.342028 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc: Close
[0m21:05:44.342028 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-ab81-1b6d-b0e9-758712b98173) - Closing
[0m21:05:44.696206 [info ] [Thread-2 (]: 27 of 48 PASS not_null_stg_churn_events_churn_event_id ......................... [[32mPASS[0m in 4.85s]
[0m21:05:44.698217 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_churn_events_churn_event_id.f5dff6bdbc
[0m21:05:44.700226 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m21:05:44.700226 [info ] [Thread-2 (]: 28 of 48 START test not_null_stg_feature_usage_subscription_id ................. [RUN]
[0m21:05:44.700226 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3) - Creating connection
[0m21:05:44.707150 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3'
[0m21:05:44.709378 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m21:05:44.723828 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m21:05:44.730247 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m21:05:44.739104 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m21:05:44.739104 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"
[0m21:05:44.739104 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_feature_usage`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m21:05:44.747089 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:05:49.704555 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-ae97-1181-8e83-02147a278999) - Created
[0m21:05:50.139621 [debug] [Thread-2 (]: SQL status: OK in 5.390 seconds
[0m21:05:50.142661 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-ae97-1181-8e83-02147a278999, command-id=01f10da8-af31-1fae-80a5-edc9fa0000d3) - Closing
[0m21:05:50.142661 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3: Close
[0m21:05:50.142661 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-ae97-1181-8e83-02147a278999) - Closing
[0m21:05:50.447815 [info ] [Thread-2 (]: 28 of 48 PASS not_null_stg_feature_usage_subscription_id ....................... [[32mPASS[0m in 5.75s]
[0m21:05:50.457193 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_feature_usage_subscription_id.d9a32d64d3
[0m21:05:50.457193 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m21:05:50.460833 [info ] [Thread-2 (]: 29 of 48 START test not_null_stg_subscriptions_account_id ...................... [RUN]
[0m21:05:50.462840 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9) - Creating connection
[0m21:05:50.466528 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9'
[0m21:05:50.466528 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m21:05:50.485989 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m21:05:50.488716 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m21:05:50.495224 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m21:05:50.503406 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"
[0m21:05:50.503406 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_subscriptions`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m21:05:50.503406 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:05:54.365942 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-b1cc-1dd3-b873-cd5a9fe2df0c) - Created
[0m21:05:55.039984 [debug] [Thread-2 (]: SQL status: OK in 4.540 seconds
[0m21:05:55.044058 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-b1cc-1dd3-b873-cd5a9fe2df0c, command-id=01f10da8-b1fa-12de-ac6b-1e8990c83b75) - Closing
[0m21:05:55.047618 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9: Close
[0m21:05:55.047618 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-b1cc-1dd3-b873-cd5a9fe2df0c) - Closing
[0m21:05:55.347409 [info ] [Thread-2 (]: 29 of 48 PASS not_null_stg_subscriptions_account_id ............................ [[32mPASS[0m in 4.88s]
[0m21:05:55.351214 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_account_id.a3159cfdf9
[0m21:05:55.351214 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m21:05:55.351214 [info ] [Thread-2 (]: 30 of 48 START test not_null_stg_subscriptions_subscription_id ................. [RUN]
[0m21:05:55.356520 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91) - Creating connection
[0m21:05:55.360201 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m21:05:55.360201 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m21:05:55.372401 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m21:05:55.379021 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m21:05:55.395853 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m21:05:55.398870 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m21:05:55.404003 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m21:05:55.408060 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:06:02.873420 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-b6d9-19ec-9534-dc0fca42e497) - Created
[0m21:06:03.382200 [debug] [Thread-2 (]: SQL status: OK in 7.980 seconds
[0m21:06:03.382200 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-b6d9-19ec-9534-dc0fca42e497, command-id=01f10da8-b70b-16bc-af9b-8be56c2d5aa2) - Closing
[0m21:06:03.389320 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m21:06:03.391329 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-b6d9-19ec-9534-dc0fca42e497) - Closing
[0m21:06:03.750231 [info ] [Thread-2 (]: 30 of 48 PASS not_null_stg_subscriptions_subscription_id ....................... [[32mPASS[0m in 8.39s]
[0m21:06:03.750231 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m21:06:03.750231 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m21:06:03.756800 [info ] [Thread-2 (]: 31 of 48 START test not_null_stg_support_tickets_account_id .................... [RUN]
[0m21:06:03.758810 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32) - Creating connection
[0m21:06:03.763249 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32'
[0m21:06:03.763249 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m21:06:03.782030 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m21:06:03.785184 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m21:06:03.795313 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m21:06:03.795313 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"
[0m21:06:03.795313 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select account_id
from `workspace`.`analytics`.`stg_support_tickets`
where account_id is null



  
  
      
    ) dbt_internal_test
[0m21:06:03.802856 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:06:07.766238 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-b9c7-1092-8fdc-c240363f1c77) - Created
[0m21:06:08.243226 [debug] [Thread-2 (]: SQL status: OK in 4.440 seconds
[0m21:06:08.243226 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-b9c7-1092-8fdc-c240363f1c77, command-id=01f10da8-b9f7-1e0c-b497-893bb12e0a67) - Closing
[0m21:06:08.252236 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32: Close
[0m21:06:08.252236 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-b9c7-1092-8fdc-c240363f1c77) - Closing
[0m21:06:09.871030 [info ] [Thread-2 (]: 31 of 48 PASS not_null_stg_support_tickets_account_id .......................... [[32mPASS[0m in 6.11s]
[0m21:06:09.871030 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_account_id.c7124b1b32
[0m21:06:09.871030 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m21:06:09.871030 [info ] [Thread-2 (]: 32 of 48 START test not_null_stg_support_tickets_ticket_id ..................... [RUN]
[0m21:06:09.880535 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2) - Creating connection
[0m21:06:09.880535 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2'
[0m21:06:09.884597 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m21:06:09.900620 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m21:06:09.900620 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m21:06:09.911106 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m21:06:09.917350 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"
[0m21:06:09.917350 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select ticket_id
from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is null



  
  
      
    ) dbt_internal_test
[0m21:06:09.917350 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:06:14.851747 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-bd76-15d9-85cc-3819605966d3) - Created
[0m21:06:15.380661 [debug] [Thread-2 (]: SQL status: OK in 5.460 seconds
[0m21:06:15.384966 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-bd76-15d9-85cc-3819605966d3, command-id=01f10da8-be2f-1a07-8ee4-549dd9f6b02e) - Closing
[0m21:06:15.388360 [debug] [Thread-2 (]: On test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2: Close
[0m21:06:15.389368 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-bd76-15d9-85cc-3819605966d3) - Closing
[0m21:06:15.763142 [info ] [Thread-2 (]: 32 of 48 PASS not_null_stg_support_tickets_ticket_id ........................... [[32mPASS[0m in 5.88s]
[0m21:06:15.765152 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.not_null_stg_support_tickets_ticket_id.7830ecbac2
[0m21:06:15.765152 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m21:06:15.765152 [info ] [Thread-2 (]: 33 of 48 START test relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [RUN]
[0m21:06:15.769550 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b) - Creating connection
[0m21:06:15.773954 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b'
[0m21:06:15.773954 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m21:06:15.801247 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m21:06:15.801247 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m21:06:15.814114 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m21:06:15.816858 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"
[0m21:06:15.817850 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_churn_events`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m21:06:15.820872 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:06:21.922350 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-c229-1451-b477-b96bbbc7f2f8) - Created
[0m21:06:23.702526 [debug] [Thread-2 (]: SQL status: OK in 7.880 seconds
[0m21:06:23.706930 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-c229-1451-b477-b96bbbc7f2f8, command-id=01f10da8-c318-1956-bb16-6c0cdbf68dcd) - Closing
[0m21:06:23.706930 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b: Close
[0m21:06:23.710949 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-c229-1451-b477-b96bbbc7f2f8) - Closing
[0m21:06:24.136627 [info ] [Thread-2 (]: 33 of 48 PASS relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 8.37s]
[0m21:06:24.136627 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_churn_events_account_id__account_id__ref_stg_accounts_.8935f7413b
[0m21:06:24.141059 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m21:06:24.141059 [info ] [Thread-2 (]: 34 of 48 START test relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [RUN]
[0m21:06:24.145299 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850) - Creating connection
[0m21:06:24.145299 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850'
[0m21:06:24.149316 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m21:06:24.170629 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m21:06:24.175014 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m21:06:24.186765 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m21:06:24.190785 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"
[0m21:06:24.191792 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select subscription_id as from_field
    from `workspace`.`analytics`.`stg_feature_usage`
    where subscription_id is not null
),

parent as (
    select subscription_id as to_field
    from `workspace`.`analytics`.`stg_subscriptions`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m21:06:24.191792 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:06:27.893858 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-c5c7-1e6d-a0fa-2664abdb581a) - Created
[0m21:06:28.416333 [debug] [Thread-2 (]: SQL status: OK in 4.220 seconds
[0m21:06:28.420355 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-c5c7-1e6d-a0fa-2664abdb581a, command-id=01f10da8-c5f6-13d7-be42-3ada2b08f972) - Closing
[0m21:06:28.422362 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850: Close
[0m21:06:28.424370 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-c5c7-1e6d-a0fa-2664abdb581a) - Closing
[0m21:06:28.757503 [info ] [Thread-2 (]: 34 of 48 PASS relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_  [[32mPASS[0m in 4.61s]
[0m21:06:28.757503 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_feature_usage_subscription_id__subscription_id__ref_stg_subscriptions_.a4644db850
[0m21:06:28.757503 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m21:06:28.761873 [info ] [Thread-2 (]: 35 of 48 START test relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [RUN]
[0m21:06:28.766062 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90) - Creating connection
[0m21:06:28.768071 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90'
[0m21:06:28.768071 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m21:06:28.790370 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m21:06:28.795238 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m21:06:28.805003 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m21:06:28.806749 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"
[0m21:06:28.810767 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_subscriptions`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m21:06:28.810767 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:06:34.430341 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-c99d-1b36-b5d3-11cd93659654) - Created
[0m21:06:35.054764 [debug] [Thread-2 (]: SQL status: OK in 6.240 seconds
[0m21:06:35.060792 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-c99d-1b36-b5d3-11cd93659654, command-id=01f10da8-c9db-1254-b3db-d7c9ba630ec9) - Closing
[0m21:06:35.067256 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90: Close
[0m21:06:35.067256 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-c99d-1b36-b5d3-11cd93659654) - Closing
[0m21:06:35.482921 [info ] [Thread-2 (]: 35 of 48 PASS relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 6.72s]
[0m21:06:35.489251 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_subscriptions_account_id__account_id__ref_stg_accounts_.8fed920c90
[0m21:06:35.489251 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m21:06:35.493164 [info ] [Thread-2 (]: 36 of 48 START test relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [RUN]
[0m21:06:35.495289 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e) - Creating connection
[0m21:06:35.495289 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e'
[0m21:06:35.500854 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m21:06:35.516958 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m21:06:35.518966 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m21:06:35.530265 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m21:06:35.530265 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"
[0m21:06:35.535543 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select account_id as from_field
    from `workspace`.`analytics`.`stg_support_tickets`
    where account_id is not null
),

parent as (
    select account_id as to_field
    from `workspace`.`analytics`.`stg_accounts`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m21:06:35.535543 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:06:40.655079 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-ccc7-122a-8088-f68478fd654c) - Created
[0m21:06:41.338427 [debug] [Thread-2 (]: SQL status: OK in 5.800 seconds
[0m21:06:41.342637 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-ccc7-122a-8088-f68478fd654c, command-id=01f10da8-cd90-1d7e-911a-e2a52b24c248) - Closing
[0m21:06:41.344644 [debug] [Thread-2 (]: On test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e: Close
[0m21:06:41.344644 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-ccc7-122a-8088-f68478fd654c) - Closing
[0m21:06:41.721194 [info ] [Thread-2 (]: 36 of 48 PASS relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_  [[32mPASS[0m in 6.22s]
[0m21:06:41.723205 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.relationships_stg_support_tickets_account_id__account_id__ref_stg_accounts_.67b64cb46e
[0m21:06:41.723205 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_gold_account_health_summary_account_id.b4a259da42
[0m21:06:41.727320 [info ] [Thread-2 (]: 37 of 48 START test unique_gold_account_health_summary_account_id .............. [RUN]
[0m21:06:41.728828 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_gold_account_health_summary_account_id.b4a259da42) - Creating connection
[0m21:06:41.728828 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_gold_account_health_summary_account_id.b4a259da42'
[0m21:06:41.733423 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_gold_account_health_summary_account_id.b4a259da42
[0m21:06:41.760207 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_gold_account_health_summary_account_id.b4a259da42"
[0m21:06:41.765151 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_gold_account_health_summary_account_id.b4a259da42
[0m21:06:41.770835 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_gold_account_health_summary_account_id.b4a259da42"
[0m21:06:41.779933 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_gold_account_health_summary_account_id.b4a259da42"
[0m21:06:41.780469 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_gold_account_health_summary_account_id.b4a259da42: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_gold_account_health_summary_account_id.b4a259da42"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`gold_account_health_summary`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m21:06:41.780469 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:06:45.600336 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-d051-1732-85cb-a8a36d029466) - Created
[0m21:06:46.411573 [debug] [Thread-2 (]: SQL status: OK in 4.630 seconds
[0m21:06:46.415588 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-d051-1732-85cb-a8a36d029466, command-id=01f10da8-d086-11ae-bbff-b33634bd3657) - Closing
[0m21:06:46.415588 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_gold_account_health_summary_account_id.b4a259da42: Close
[0m21:06:46.415588 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-d051-1732-85cb-a8a36d029466) - Closing
[0m21:06:46.780219 [info ] [Thread-2 (]: 37 of 48 PASS unique_gold_account_health_summary_account_id .................... [[32mPASS[0m in 5.05s]
[0m21:06:46.780219 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_gold_account_health_summary_account_id.b4a259da42
[0m21:06:46.780219 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_gold_churn_analysis_churn_event_id.d9171d4434
[0m21:06:46.780219 [info ] [Thread-2 (]: 38 of 48 START test unique_gold_churn_analysis_churn_event_id .................. [RUN]
[0m21:06:46.786280 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_gold_churn_analysis_churn_event_id.d9171d4434) - Creating connection
[0m21:06:46.789113 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_gold_churn_analysis_churn_event_id.d9171d4434'
[0m21:06:46.791122 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_gold_churn_analysis_churn_event_id.d9171d4434
[0m21:06:46.801984 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_gold_churn_analysis_churn_event_id.d9171d4434"
[0m21:06:46.810354 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_gold_churn_analysis_churn_event_id.d9171d4434
[0m21:06:46.819279 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_gold_churn_analysis_churn_event_id.d9171d4434"
[0m21:06:46.823296 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_gold_churn_analysis_churn_event_id.d9171d4434"
[0m21:06:46.825305 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_gold_churn_analysis_churn_event_id.d9171d4434: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_gold_churn_analysis_churn_event_id.d9171d4434"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    churn_event_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`gold_churn_analysis`
where churn_event_id is not null
group by churn_event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m21:06:46.828160 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:06:53.542706 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-d49f-185d-9516-78f2aeffc647) - Created
[0m21:06:54.370244 [debug] [Thread-2 (]: SQL status: OK in 7.540 seconds
[0m21:06:54.376272 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-d49f-185d-9516-78f2aeffc647, command-id=01f10da8-d541-1265-93ee-6ab39425f553) - Closing
[0m21:06:54.378279 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_gold_churn_analysis_churn_event_id.d9171d4434: Close
[0m21:06:54.380287 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-d49f-185d-9516-78f2aeffc647) - Closing
[0m21:06:54.720065 [info ] [Thread-2 (]: 38 of 48 PASS unique_gold_churn_analysis_churn_event_id ........................ [[32mPASS[0m in 7.93s]
[0m21:06:54.722076 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_gold_churn_analysis_churn_event_id.d9171d4434
[0m21:06:54.724088 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_gold_customer_segments_account_id.b84e794726
[0m21:06:54.726099 [info ] [Thread-2 (]: 39 of 48 START test unique_gold_customer_segments_account_id ................... [RUN]
[0m21:06:54.730122 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_gold_customer_segments_account_id.b84e794726) - Creating connection
[0m21:06:54.732131 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_gold_customer_segments_account_id.b84e794726'
[0m21:06:54.734141 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_gold_customer_segments_account_id.b84e794726
[0m21:06:54.755211 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_gold_customer_segments_account_id.b84e794726"
[0m21:06:54.757221 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_gold_customer_segments_account_id.b84e794726
[0m21:06:54.768683 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_gold_customer_segments_account_id.b84e794726"
[0m21:06:54.774757 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_gold_customer_segments_account_id.b84e794726"
[0m21:06:54.777050 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_gold_customer_segments_account_id.b84e794726: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_gold_customer_segments_account_id.b84e794726"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`gold_customer_segments`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m21:06:54.778382 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:07:00.646945 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-d94f-1d6e-926f-7286ad084606) - Created
[0m21:07:01.672444 [debug] [Thread-2 (]: SQL status: OK in 6.890 seconds
[0m21:07:01.676462 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-d94f-1d6e-926f-7286ad084606, command-id=01f10da8-d9a8-1744-9f1e-e1d9661c728a) - Closing
[0m21:07:01.680480 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_gold_customer_segments_account_id.b84e794726: Close
[0m21:07:01.682490 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-d94f-1d6e-926f-7286ad084606) - Closing
[0m21:07:02.402788 [info ] [Thread-2 (]: 39 of 48 PASS unique_gold_customer_segments_account_id ......................... [[32mPASS[0m in 7.67s]
[0m21:07:02.402788 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_gold_customer_segments_account_id.b84e794726
[0m21:07:02.402788 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376
[0m21:07:02.402788 [info ] [Thread-2 (]: 40 of 48 START test unique_int_account_360_account_id .......................... [RUN]
[0m21:07:02.412734 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376) - Creating connection
[0m21:07:02.412734 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376'
[0m21:07:02.416387 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376
[0m21:07:02.427546 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376"
[0m21:07:02.436855 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376
[0m21:07:02.448727 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376"
[0m21:07:02.448727 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376"
[0m21:07:02.448727 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`int_account_360`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m21:07:02.453576 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:07:06.769882 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-dcf2-183f-bdd3-fab187475612) - Created
[0m21:07:07.654963 [debug] [Thread-2 (]: SQL status: OK in 5.200 seconds
[0m21:07:07.656971 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-dcf2-183f-bdd3-fab187475612, command-id=01f10da8-dd23-1de5-a12a-b7271bb8d5fe) - Closing
[0m21:07:07.656971 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376: Close
[0m21:07:07.656971 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-dcf2-183f-bdd3-fab187475612) - Closing
[0m21:07:08.094861 [info ] [Thread-2 (]: 40 of 48 PASS unique_int_account_360_account_id ................................ [[32mPASS[0m in 5.69s]
[0m21:07:08.094861 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_int_account_360_account_id.e8eef92376
[0m21:07:08.094861 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a
[0m21:07:08.094861 [info ] [Thread-2 (]: 41 of 48 START test unique_int_account_churn_churn_event_id .................... [RUN]
[0m21:07:08.094861 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a) - Creating connection
[0m21:07:08.094861 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a'
[0m21:07:08.110576 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a
[0m21:07:08.125802 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a"
[0m21:07:08.129818 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a
[0m21:07:08.139858 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a"
[0m21:07:08.144989 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a"
[0m21:07:08.147341 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    churn_event_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`int_account_churn`
where churn_event_id is not null
group by churn_event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m21:07:08.147341 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:07:13.212442 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-e0c5-18da-a13a-527860701de4) - Created
[0m21:07:13.816624 [debug] [Thread-2 (]: SQL status: OK in 5.670 seconds
[0m21:07:13.816624 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-e0c5-18da-a13a-527860701de4, command-id=01f10da8-e0f7-1dd1-9e8a-ab8a68371196) - Closing
[0m21:07:13.832489 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a: Close
[0m21:07:13.832489 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-e0c5-18da-a13a-527860701de4) - Closing
[0m21:07:14.165316 [info ] [Thread-2 (]: 41 of 48 PASS unique_int_account_churn_churn_event_id .......................... [[32mPASS[0m in 6.06s]
[0m21:07:14.165316 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_int_account_churn_churn_event_id.311beabf0a
[0m21:07:14.165316 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe
[0m21:07:14.165316 [info ] [Thread-2 (]: 42 of 48 START test unique_int_account_feature_usage_account_id ................ [RUN]
[0m21:07:14.174240 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe) - Creating connection
[0m21:07:14.176280 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe'
[0m21:07:14.178281 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe
[0m21:07:14.190918 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe"
[0m21:07:14.192002 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe
[0m21:07:14.192002 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe"
[0m21:07:14.192002 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe"
[0m21:07:14.208669 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`int_account_feature_usage`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m21:07:14.208669 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:07:18.416477 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-e3d6-1564-8e4b-055a8ac9b6b5) - Created
[0m21:07:19.097484 [debug] [Thread-2 (]: SQL status: OK in 4.890 seconds
[0m21:07:19.101944 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-e3d6-1564-8e4b-055a8ac9b6b5, command-id=01f10da8-e414-1b9c-8173-fd3f4de26281) - Closing
[0m21:07:19.101944 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe: Close
[0m21:07:19.101944 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-e3d6-1564-8e4b-055a8ac9b6b5) - Closing
[0m21:07:20.395532 [info ] [Thread-2 (]: 42 of 48 PASS unique_int_account_feature_usage_account_id ...................... [[32mPASS[0m in 6.22s]
[0m21:07:20.396539 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_int_account_feature_usage_account_id.5aedca72fe
[0m21:07:20.398859 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b
[0m21:07:20.398859 [info ] [Thread-2 (]: 43 of 48 START test unique_int_account_subscriptions_subscription_id ........... [RUN]
[0m21:07:20.402735 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b) - Creating connection
[0m21:07:20.405514 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b'
[0m21:07:20.405514 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b
[0m21:07:20.423523 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b"
[0m21:07:20.424091 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b
[0m21:07:20.435801 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b"
[0m21:07:20.440789 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b"
[0m21:07:20.440789 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`int_account_subscriptions`
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m21:07:20.440789 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:07:24.349660 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-e76a-1b26-a37e-2ba78d5587e3) - Created
[0m21:07:24.988275 [debug] [Thread-2 (]: SQL status: OK in 4.550 seconds
[0m21:07:24.992626 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-e76a-1b26-a37e-2ba78d5587e3, command-id=01f10da8-e79d-1c15-8fc8-fdb02133c03f) - Closing
[0m21:07:24.992626 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b: Close
[0m21:07:24.998659 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-e76a-1b26-a37e-2ba78d5587e3) - Closing
[0m21:07:25.323873 [info ] [Thread-2 (]: 43 of 48 PASS unique_int_account_subscriptions_subscription_id ................. [[32mPASS[0m in 4.92s]
[0m21:07:25.332011 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_int_account_subscriptions_subscription_id.0c8ff3548b
[0m21:07:25.332011 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab
[0m21:07:25.336164 [info ] [Thread-2 (]: 44 of 48 START test unique_int_account_support_account_id ...................... [RUN]
[0m21:07:25.336164 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab) - Creating connection
[0m21:07:25.340031 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab'
[0m21:07:25.342173 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab
[0m21:07:25.356695 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab"
[0m21:07:25.359675 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab
[0m21:07:25.368309 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab"
[0m21:07:25.373856 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab"
[0m21:07:25.374958 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`int_account_support`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m21:07:25.374958 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:07:30.996735 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-eb5d-1c2b-aaa6-4b6df8b1f9ea) - Created
[0m21:07:31.904819 [debug] [Thread-2 (]: SQL status: OK in 6.530 seconds
[0m21:07:31.913840 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-eb5d-1c2b-aaa6-4b6df8b1f9ea, command-id=01f10da8-eb93-1074-9bfe-81442cba1e66) - Closing
[0m21:07:31.913840 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab: Close
[0m21:07:31.917176 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-eb5d-1c2b-aaa6-4b6df8b1f9ea) - Closing
[0m21:07:32.262198 [info ] [Thread-2 (]: 44 of 48 PASS unique_int_account_support_account_id ............................ [[32mPASS[0m in 6.93s]
[0m21:07:32.264658 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_int_account_support_account_id.3d85925cab
[0m21:07:32.264658 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m21:07:32.268499 [info ] [Thread-2 (]: 45 of 48 START test unique_stg_accounts_account_id ............................. [RUN]
[0m21:07:32.271755 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79) - Creating connection
[0m21:07:32.272779 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79'
[0m21:07:32.274780 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m21:07:32.289353 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m21:07:32.291884 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m21:07:32.301707 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m21:07:32.305775 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"
[0m21:07:32.305775 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    account_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_accounts`
where account_id is not null
group by account_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m21:07:32.305775 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:07:38.340201 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-efbc-168c-8b53-c943b57a5114) - Created
[0m21:07:38.863814 [debug] [Thread-2 (]: SQL status: OK in 6.560 seconds
[0m21:07:38.870899 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-efbc-168c-8b53-c943b57a5114, command-id=01f10da8-eff4-166a-9dfa-06734a83b683) - Closing
[0m21:07:38.870899 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79: Close
[0m21:07:38.870899 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-efbc-168c-8b53-c943b57a5114) - Closing
[0m21:07:39.257989 [info ] [Thread-2 (]: 45 of 48 PASS unique_stg_accounts_account_id ................................... [[32mPASS[0m in 6.98s]
[0m21:07:39.257989 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_accounts_account_id.cdf6252c79
[0m21:07:39.257989 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m21:07:39.257989 [info ] [Thread-2 (]: 46 of 48 START test unique_stg_churn_events_churn_event_id ..................... [RUN]
[0m21:07:39.265021 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947) - Creating connection
[0m21:07:39.265021 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947'
[0m21:07:39.269650 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m21:07:39.285721 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m21:07:39.286728 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m21:07:39.293900 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m21:07:39.300053 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"
[0m21:07:39.301060 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    churn_event_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_churn_events`
where churn_event_id is not null
group by churn_event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m21:07:39.301060 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:07:43.196050 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-f2ac-1d80-9e28-49c99d9e99bb) - Created
[0m21:07:43.642202 [debug] [Thread-2 (]: SQL status: OK in 4.340 seconds
[0m21:07:43.645440 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-f2ac-1d80-9e28-49c99d9e99bb, command-id=01f10da8-f2d9-16c7-86e9-0ec88f532d53) - Closing
[0m21:07:43.647454 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947: Close
[0m21:07:43.649454 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-f2ac-1d80-9e28-49c99d9e99bb) - Closing
[0m21:07:43.967083 [info ] [Thread-2 (]: 46 of 48 PASS unique_stg_churn_events_churn_event_id ........................... [[32mPASS[0m in 4.70s]
[0m21:07:43.969202 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_churn_events_churn_event_id.8047810947
[0m21:07:43.971716 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m21:07:43.972717 [info ] [Thread-2 (]: 47 of 48 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m21:07:43.975577 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e) - Creating connection
[0m21:07:43.977598 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m21:07:43.979726 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m21:07:43.994843 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m21:07:43.998160 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m21:07:44.009499 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m21:07:44.011530 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m21:07:44.012787 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_subscriptions`
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m21:07:44.015076 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:07:49.159224 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-f63d-144c-a168-d54b39307553) - Created
[0m21:07:49.692834 [debug] [Thread-2 (]: SQL status: OK in 5.680 seconds
[0m21:07:49.697272 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-f63d-144c-a168-d54b39307553, command-id=01f10da8-f667-1161-b534-d008b99149b4) - Closing
[0m21:07:49.699523 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m21:07:49.700533 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-f63d-144c-a168-d54b39307553) - Closing
[0m21:07:50.041214 [info ] [Thread-2 (]: 47 of 48 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 6.07s]
[0m21:07:50.043560 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m21:07:50.044565 [debug] [Thread-2 (]: Began running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m21:07:50.046565 [info ] [Thread-2 (]: 48 of 48 START test unique_stg_support_tickets_ticket_id ....................... [RUN]
[0m21:07:50.048910 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a) - Creating connection
[0m21:07:50.050989 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a'
[0m21:07:50.053228 [debug] [Thread-2 (]: Began compiling node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m21:07:50.068074 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m21:07:50.072197 [debug] [Thread-2 (]: Began executing node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m21:07:50.082377 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m21:07:50.084735 [debug] [Thread-2 (]: Using databricks connection "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"
[0m21:07:50.087170 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "saas_dbt_analytics", "target_name": "dev", "node_id": "test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    ticket_id as unique_field,
    count(*) as n_records

from `workspace`.`analytics`.`stg_support_tickets`
where ticket_id is not null
group by ticket_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m21:07:50.088540 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:07:53.957071 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-f90e-1a97-9f93-a6799c5334f6) - Created
[0m21:07:54.470743 [debug] [Thread-2 (]: SQL status: OK in 4.380 seconds
[0m21:07:54.476016 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f10da8-f90e-1a97-9f93-a6799c5334f6, command-id=01f10da8-f942-1c13-bc08-67451b952b3d) - Closing
[0m21:07:54.478488 [debug] [Thread-2 (]: On test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a: Close
[0m21:07:54.479960 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f10da8-f90e-1a97-9f93-a6799c5334f6) - Closing
[0m21:07:54.849504 [info ] [Thread-2 (]: 48 of 48 PASS unique_stg_support_tickets_ticket_id ............................. [[32mPASS[0m in 4.80s]
[0m21:07:54.851920 [debug] [Thread-2 (]: Finished running node test.saas_dbt_analytics.unique_stg_support_tickets_ticket_id.eed19dcb0a
[0m21:07:54.854972 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m21:07:54.857273 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:07:54.858277 [info ] [MainThread]: 
[0m21:07:54.860413 [info ] [MainThread]: Finished running 48 data tests in 0 hours 4 minutes and 36.62 seconds (276.62s).
[0m21:07:54.880848 [debug] [MainThread]: Command end result
[0m21:07:55.011806 [debug] [MainThread]: Wrote artifact WritableManifest to D:\DataScience\saas-databricks-dbt-analytics\target\manifest.json
[0m21:07:55.021196 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\DataScience\saas-databricks-dbt-analytics\target\semantic_manifest.json
[0m21:07:55.046479 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\DataScience\saas-databricks-dbt-analytics\target\run_results.json
[0m21:07:55.047539 [info ] [MainThread]: 
[0m21:07:55.049757 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:07:55.050773 [info ] [MainThread]: 
[0m21:07:55.052059 [info ] [MainThread]: Done. PASS=48 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=48
[0m21:07:55.055103 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 2 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m21:07:55.058393 [debug] [MainThread]: Command `dbt test` succeeded at 21:07:55.058393 after 284.22 seconds
[0m21:07:55.059825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001949707EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001949733A1A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001949741DB40>]}
[0m21:07:55.061838 [debug] [MainThread]: Flushing usage events
[0m21:07:58.198006 [debug] [MainThread]: An error was encountered while trying to flush usage events
